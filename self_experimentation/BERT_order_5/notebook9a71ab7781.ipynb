{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6991410,"sourceType":"datasetVersion","datasetId":4018477,"isSourceIdPinned":true},{"sourceId":7027043,"sourceType":"datasetVersion","datasetId":4024273,"isSourceIdPinned":false}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"mv /kaggle/input/bert-progressive-prompts/ProgressivePrompts /kaggle/working/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-11-22T13:26:44.823012Z","iopub.execute_input":"2023-11-22T13:26:44.823285Z","iopub.status.idle":"2023-11-22T13:26:48.115078Z","shell.execute_reply.started":"2023-11-22T13:26:44.823261Z","shell.execute_reply":"2023-11-22T13:26:48.113896Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"mv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/environment.yaml': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/LICENSE': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/datasets/README.md': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/datasets/src/data/amazon/train.csv': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/datasets/src/data/amazon/test.csv': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/BERT_codebase/train_soft_prompt.py': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/BERT_codebase/train_cl2.py': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/BERT_codebase/run_prog_prompts.sh': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/BERT_codebase/good_id_yahoo_test.npy': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/BERT_codebase/continual_learning_utils.py': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/BERT_codebase/dataset_utils.py': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/BERT_codebase/continual_learning_one_head.py': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/BERT_codebase/model_utils.py': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/BERT_codebase/train_cl.py': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/BERT_codebase/good_id_yahoo_train.npy': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/README.md': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/T5_codebase/train_prompt.py': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/T5_codebase/prompt_debug.ipynb': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/T5_codebase/train_t5_cl.py': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/T5_codebase/t5_dataset.py': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/T5_codebase/t5_continual.py': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/experimental_results/T5_order_3/prompts.npy': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/experimental_results/T5_order_3/ProgressivePrompts.ipynb': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/experimental_results/T5_order_3/results_dict.npy': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/experimental_results/T5_order_1_large_data/prompts.npy': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/experimental_results/T5_order_1_large_data/ProgressivePrompts.ipynb': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/experimental_results/T5_order_1_large_data/results_dict.npy': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/experimental_results/T5_order_1/prompts.npy': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/experimental_results/T5_order_1/ProgressivePrompts.ipynb': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/experimental_results/T5_order_1/results_dict.npy': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/experimental_results/T5_order_2/prompts.npy': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/experimental_results/T5_order_2/ProgressivePrompts.ipynb': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/experimental_results/T5_order_2/results_dict.npy': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/images/test.png': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/images/illustration.png': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/COMMIT_EDITMSG': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/config': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/info/exclude': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/refs/heads/main': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/refs/remotes/origin/HEAD': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/refs/remotes/origin/main': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/hooks/pre-merge-commit.sample': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/hooks/prepare-commit-msg.sample': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/hooks/update.sample': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/hooks/pre-push.sample': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/hooks/pre-rebase.sample': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/hooks/pre-applypatch.sample': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/hooks/push-to-checkout.sample': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/hooks/pre-commit.sample': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/hooks/commit-msg.sample': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/hooks/post-update.sample': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/hooks/pre-receive.sample': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/hooks/fsmonitor-watchman.sample': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/hooks/applypatch-msg.sample': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/packed-refs': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/HEAD': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/index': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/logs/refs/heads/main': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/logs/refs/remotes/origin/HEAD': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/logs/refs/remotes/origin/main': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/logs/HEAD': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/17/3073af254c62314fdc01c681efdcd97e84b4b1': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/81/a65f669bfe32428a16c002c013281766154547': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/19/da081be6da60776c1e22835e7ec851324bce63': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/a2/d0f9c9e8e27b4d05707a0e3c5c2e89a82c774e': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/cf/0bbe3e07373bf1036d6c251e89ed41882164ae': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/b8/f11ebedef504a56274bd8e394f477adce5f665': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/35/9420243d81df801369edf1b9c98bc47fdc9428': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/1b/711698907ee5dafbd93580c0c04762c2afcefe': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/9e/875fe63587e9a1b4163725d98d9218df19bdcc': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/32/e7ccdf9eb96252bbaa7ac4f7860e9ce01096a0': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/f0/1fd00c5f2937b543ea0068710baba945aa2852': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/d9/35124b6b2d9de57cf19c5400d026db6c345caa': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/e0/24ec3e7a4c5202e228d0acf2d1ebf291b7417c': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/b9/1a477b2ae70dddd3e48d37cc14c482e3d15284': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/31/fd669b4a8c707ce70f444c321c5a294528742a': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/f9/e1364cb8f4f2be90c2ad9553bf7908f518b517': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/b6/bfe75ccdc8106addce62b84fdd133ef51bc5b0': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/ec/fc9fd96f0f6cb09a677d2146bdd847f710f054': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/a5/bd66e751c6d8e67b1274d7e912086829d637f4': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/72/e613cdd187afa39da70a6c28a00aa6382f2854': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/6c/0ec92eeabff3e970ea3bf90f8f2e1cba2ad400': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/99/493a58cf13b9508d5b16cce3d83c6609c74ce4': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/b5/36d14aebe9d722e9ec4dd3ef16226ee5fb71ae': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/c6/12e23b5af8174b37362a2fc670f0866ab93fc5': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/15/a62685050dda0e06674ef779862c155ee4910e': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/ae/84b053f492814d7df8b2545c947499097505c8': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/ae/36bb87b2d76010cd5412abc4109b79145e931f': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/90/c2ab740759bfbeddd6af91127d307b7b68b128': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/90/016de1039ed219527ce7df13f178d57fe72e5d': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/95/8ea192f5c74ef5c1ec52ca0782e8d4fb342bb6': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/f3/74e755a13013977766d0fc27f6efae292bb76c': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/ed/b1eb648d6a47d9639f432bb272bfc5748c9780': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/09/a778db5cc6a2d76fb8a4381c23e3e53fc107c8': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/09/9ad7d0d7ff73928f2f5c835f0775eae216384c': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/pack/pack-2b0fcac691b2eb0f3ef8bd98f30bbfbe9aeb00a4.idx': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/pack/pack-2b0fcac691b2eb0f3ef8bd98f30bbfbe9aeb00a4.pack': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/4f/2d0858f89dae4cd6b05d4bc2219dfc25a3f862': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/30/3f870f1b0db5dbf5629f1957998ad118169aa5': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/7a/f726782eff88fb94ea66636e119586ebc963f5': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/03/615df04c03cdcb604660fc20493f8357d67102': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/76/0587e33f7dd96bb6567721f51d21bb8503ba51': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/84/84f343ff01da6d017ea1ffe521c9a2eb5ddb74': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/df/6dd1a450f30d0a415ff112e722492717573d71': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/1f/ef8898c9d495d89e62856d22a1716740f910c1': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/82/809ee59a913b718fa3a411d3bc114fc1199d6f': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/aa/6d2861208453a57c22794cf329876ca2709b9e': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/6b/880082ab29e0d47e018cb45c5b29df465ad93a': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/21/a15ef078c08b3da1fe2bd63a71dcfe8d45ede0': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/44/6d4509e4471808b7362a244f3c563beafafae1': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/d3/4a901f7047255683affba54dba83f1ba1cf019': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/8c/699a35e2df8ba0a5e9aebf490f1f9afe83fa66': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/fa/50a91b2d6ff41548d1cbe6e69d6acda920c1a7': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/c1/e7918e4f41cf3b4e577dc9351ba18acecc44c0': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/48/5a32f4519ff5d933f62c3afe1408c67c0dd6f6': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/3b/98e6e1b9de5d291e2b4a4295b1fa7d191e71c8': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/fd/0c4132a28b4dc39554b24d0c7624e09295aad3': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/24/6e61cc804fa9b700bf59882fce672a8d502c61': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/6a/a62969aba32285b7fa80648fe7d0e457a6e178': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/objects/6a/8d9b684a03866bbfdc58206d3693daaf04493d': Read-only file system\nmv: cannot remove '/kaggle/input/bert-progressive-prompts/ProgressivePrompts/.git/description': Read-only file system\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/ProgressivePrompts/BERT_codebase\nimport os\nos.mkdir(\"load_checkpoint\")\n%mv /kaggle/input/bert-order-2/current_checkpoint.pt /kaggle/working/ProgressivePrompts/BERT_codebase/load_checkpoint","metadata":{"execution":{"iopub.status.busy":"2023-11-22T13:26:48.117681Z","iopub.execute_input":"2023-11-22T13:26:48.118015Z","iopub.status.idle":"2023-11-22T13:26:57.512112Z","shell.execute_reply.started":"2023-11-22T13:26:48.117976Z","shell.execute_reply":"2023-11-22T13:26:57.511020Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/ProgressivePrompts/BERT_codebase\nmv: cannot remove '/kaggle/input/bert-order-2/current_checkpoint.pt': Read-only file system\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/ProgressivePrompts\n%conda env create -f environment.yaml","metadata":{"execution":{"iopub.status.busy":"2023-11-22T13:26:57.513932Z","iopub.execute_input":"2023-11-22T13:26:57.514215Z","iopub.status.idle":"2023-11-22T13:39:00.258985Z","shell.execute_reply.started":"2023-11-22T13:26:57.514189Z","shell.execute_reply":"2023-11-22T13:39:00.257819Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/ProgressivePrompts\nRetrieving notices: ...working... done\nCollecting package metadata (repodata.json): \\ WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.9.0.*, but conda is ignoring the .* and treating it as 1.9.0\nWARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.8.0.*, but conda is ignoring the .* and treating it as 1.8.0\nWARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\nWARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.6.0.*, but conda is ignoring the .* and treating it as 1.6.0\ndone\nSolving environment: done\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 23.7.4\n  latest version: 23.10.0\n\nPlease update conda by running\n\n    $ conda update -n base -c conda-forge conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=23.10.0\n\n\n\nDownloading and Extracting Packages\nqtconsole-5.3.0      | 90 KB     |                                       |   0% \nsend2trash-1.8.0     | 19 KB     |                                       |   0% \u001b[A\n\nmatplotlib-inline-0. | 12 KB     |                                       |   0% \u001b[A\u001b[A\n\n\nwidgetsnbextension-3 | 1.3 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n\n\n\nlibgcc-ng-12.1.0     | 940 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\nglib-2.56.2          | 5.0 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\nlibpng-1.6.37        | 306 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\njsonschema-4.4.0     | 120 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nprompt-toolkit-3.0.2 | 254 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\nlibdeflate-1.12      | 78 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\npcre-8.45            | 257 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\njupyter_core-4.9.2   | 84 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\nopenjpeg-2.4.0       | 444 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\npackaging-21.3       | 35 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\nlcms2-2.12           | 443 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsoupsieve-2.3.1      | 33 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-mutex-1.0    | 3 KB      |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprompt_toolkit-3.0.2 | 12 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzstd-1.5.2           | 452 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nargon2-cffi-20.1.0   | 49 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorchvision-0.11.2   | 9.1 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkupsafe-2.0.1     | 22 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibwebp-base-1.2.2   | 824 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nwidgetsnbextension-3 | 1.3 MB    | 4                                     |   1% \u001b[A\u001b[A\u001b[A\n\n\n\nlibgcc-ng-12.1.0     | 940 KB    | 6                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\n\nqtconsole-5.3.0      | 90 KB     | ######5                               |  18% \u001b[A\u001b[A\nsend2trash-1.8.0     | 19 KB     | ###############################2      |  84% \u001b[A\n\n\n\n\nglib-2.56.2          | 5.0 MB    | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\njsonschema-4.4.0     | 120 KB    | ####9                                 |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\nlibpng-1.6.37        | 306 KB    | #9                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\nmatplotlib-inline-0. | 12 KB     | ##################################### | 100% \u001b[A\u001b[A\n\n\n\n\n\n\n\nprompt-toolkit-3.0.2 | 254 KB    | ##3                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\nlibdeflate-1.12      | 78 KB     | #######5                              |  21% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\npcre-8.45            | 257 KB    | ##3                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\njupyter_core-4.9.2   | 84 KB     | #######                               |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\nglib-2.56.2          | 5.0 MB    | ##########9                           |  29% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nsend2trash-1.8.0     | 19 KB     | ##################################### | 100% \u001b[A\n\n\n\n\n\n\n\n\n\n\n\nopenjpeg-2.4.0       | 444 KB    | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\npackaging-21.3       | 35 KB     | ################7                     |  45% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\nlcms2-2.12           | 443 KB    | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsoupsieve-2.3.1      | 33 KB     | #################8                    |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-mutex-1.0    | 3 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprompt_toolkit-3.0.2 | 12 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqtconsole-5.3.0      | 90 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\nqtconsole-5.3.0      | 90 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nargon2-cffi-20.1.0   | 49 KB     | ############                          |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkupsafe-2.0.1     | 22 KB     | ##########################3           |  71% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibwebp-base-1.2.2   | 824 KB    | 7                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorchvision-0.11.2   | 9.1 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\njsonschema-4.4.0     | 120 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\njsonschema-4.4.0     | 120 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorchvision-0.11.2   | 9.1 MB    | #                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nlibgcc-ng-12.1.0     | 940 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nlibgcc-ng-12.1.0     | 940 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorchvision-0.11.2   | 9.1 MB    | ###8                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\nlibpng-1.6.37        | 306 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\nlibpng-1.6.37        | 306 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\nlibdeflate-1.12      | 78 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\nlibdeflate-1.12      | 78 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorchvision-0.11.2   | 9.1 MB    | #########                             |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorchvision-0.11.2   | 9.1 MB    | ###############7                      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nwidgetsnbextension-3 | 1.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\nwidgetsnbextension-3 | 1.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorchvision-0.11.2   | 9.1 MB    | ######################6               |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\npcre-8.45            | 257 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\npcre-8.45            | 257 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorchvision-0.11.2   | 9.1 MB    | ############################6         |  78% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\npackaging-21.3       | 35 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\npackaging-21.3       | 35 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorchvision-0.11.2   | 9.1 MB    | ###################################4  |  96% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nprompt-toolkit-3.0.2 | 254 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nprompt-toolkit-3.0.2 | 254 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\njupyter_core-4.9.2   | 84 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\njupyter_core-4.9.2   | 84 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsoupsieve-2.3.1      | 33 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsoupsieve-2.3.1      | 33 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-mutex-1.0    | 3 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\nopenjpeg-2.4.0       | 444 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\nopenjpeg-2.4.0       | 444 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\nlcms2-2.12           | 443 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\nlcms2-2.12           | 443 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprompt_toolkit-3.0.2 | 12 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nargon2-cffi-20.1.0   | 49 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nargon2-cffi-20.1.0   | 49 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkupsafe-2.0.1     | 22 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkupsafe-2.0.1     | 22 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzstd-1.5.2           | 452 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzstd-1.5.2           | 452 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibwebp-base-1.2.2   | 824 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibwebp-base-1.2.2   | 824 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\nglib-2.56.2          | 5.0 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorchvision-0.11.2   | 9.1 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \n                                                                                \u001b[A\n\n                                                                                \u001b[A\u001b[A\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: | By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n\ndone\nInstalling pip dependencies: | Ran pip subprocess with arguments:\n['/opt/conda/envs/nlp/bin/python', '-m', 'pip', 'install', '-U', '-r', '/kaggle/working/ProgressivePrompts/condaenv.k1rceg3a.requirements.txt', '--exists-action=b']\nPip subprocess output:\nCollecting adapter-transformers==3.0.1\n  Downloading adapter_transformers-3.0.1-py3-none-any.whl (3.9 MB)\n      3.9/3.9 MB 56.7 MB/s eta 0:00:00\nCollecting aiohttp==3.8.1\n  Downloading aiohttp-3.8.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n      1.2/1.2 MB 60.1 MB/s eta 0:00:00\nCollecting aiosignal==1.2.0\n  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\nCollecting async-timeout==4.0.2\n  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\nCollecting certifi==2022.6.15\n  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n      160.2/160.2 kB 16.7 MB/s eta 0:00:00\nCollecting charset-normalizer==2.0.12\n  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\nCollecting click==8.1.3\n  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n      96.6/96.6 kB 10.0 MB/s eta 0:00:00\nCollecting cycler==0.11.0\n  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\nCollecting data==0.4\n  Downloading data-0.4.tar.gz (7.0 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting datasets==2.3.2\n  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n      362.3/362.3 kB 29.4 MB/s eta 0:00:00\nCollecting dill==0.3.5.1\n  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n      95.8/95.8 kB 10.7 MB/s eta 0:00:00\nCollecting filelock==3.7.1\n  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\nCollecting fonttools==4.33.3\n  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n      930.9/930.9 kB 53.8 MB/s eta 0:00:00\nCollecting frozenlist==1.3.0\n  Downloading frozenlist-1.3.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n      156.2/156.2 kB 15.1 MB/s eta 0:00:00\nCollecting fsspec==2022.5.0\n  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n      140.6/140.6 kB 13.6 MB/s eta 0:00:00\nCollecting funcsigs==1.0.2\n  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\nCollecting future==0.18.2\n  Downloading future-0.18.2.tar.gz (829 kB)\n      829.2/829.2 kB 52.4 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting huggingface-hub==0.7.0\n  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n      86.2/86.2 kB 9.5 MB/s eta 0:00:00\nCollecting idna==3.3\n  Downloading idna-3.3-py3-none-any.whl (61 kB)\n      61.2/61.2 kB 6.8 MB/s eta 0:00:00\nCollecting joblib==1.1.0\n  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n      307.0/307.0 kB 30.1 MB/s eta 0:00:00\nCollecting kiwisolver==1.4.3\n  Downloading kiwisolver-1.4.3-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n      1.6/1.6 MB 66.6 MB/s eta 0:00:00\nCollecting latex==0.7.0\n  Downloading latex-0.7.0.tar.gz (6.5 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting matplotlib==3.5.2\n  Downloading matplotlib-3.5.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n      11.2/11.2 MB 68.0 MB/s eta 0:00:00\nCollecting multidict==6.0.2\n  Downloading multidict-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n      114.2/114.2 kB 11.9 MB/s eta 0:00:00\nCollecting multiprocess==0.70.13\n  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n      132.3/132.3 kB 13.2 MB/s eta 0:00:00\nCollecting nltk==3.7\n  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n      1.5/1.5 MB 69.3 MB/s eta 0:00:00\nCollecting pandas==1.4.2\n  Downloading pandas-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n      11.7/11.7 MB 74.1 MB/s eta 0:00:00\nCollecting pyarrow==8.0.0\n  Downloading pyarrow-8.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n      29.4/29.4 MB 47.8 MB/s eta 0:00:00\nRequirement already satisfied: pyparsing==3.0.9 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from -r /kaggle/working/ProgressivePrompts/condaenv.k1rceg3a.requirements.txt (line 29)) (3.0.9)\nCollecting pytorch-ranger==0.1.1\n  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\nCollecting pytz==2022.1\n  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n      503.5/503.5 kB 40.0 MB/s eta 0:00:00\nCollecting pyyaml==6.0\n  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n      661.8/661.8 kB 48.7 MB/s eta 0:00:00\nCollecting regex==2022.6.2\n  Downloading regex-2022.6.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)\n      763.2/763.2 kB 49.6 MB/s eta 0:00:00\nCollecting requests==2.28.0\n  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n      62.8/62.8 kB 6.7 MB/s eta 0:00:00\nCollecting responses==0.18.0\n  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\nCollecting sacremoses==0.0.53\n  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n      880.6/880.6 kB 55.7 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting scikit-learn==1.1.1\n  Downloading scikit_learn-1.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.8 MB)\n      30.8/30.8 MB 43.8 MB/s eta 0:00:00\nCollecting scipy==1.8.1\n  Downloading scipy-1.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n      42.2/42.2 MB 31.9 MB/s eta 0:00:00\nCollecting seaborn==0.12.0\n  Downloading seaborn-0.12.0-py3-none-any.whl (285 kB)\n      285.1/285.1 kB 29.9 MB/s eta 0:00:00\nCollecting sentencepiece==0.1.96\n  Downloading sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n      1.2/1.2 MB 24.2 MB/s eta 0:00:00\nCollecting shutilwhich==1.1.0\n  Downloading shutilwhich-1.1.0.tar.gz (2.3 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting sklearn==0.0\n  Downloading sklearn-0.0.tar.gz (1.1 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting tempdir==0.7.1\n  Downloading tempdir-0.7.1.tar.gz (5.9 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting threadpoolctl==3.1.0\n  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\nCollecting tokenizers==0.12.1\n  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n      6.6/6.6 MB 88.0 MB/s eta 0:00:00\nCollecting torch-optimizer==0.3.0\n  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n      61.9/61.9 kB 6.0 MB/s eta 0:00:00\nCollecting tqdm==4.64.0\n  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n      78.4/78.4 kB 8.7 MB/s eta 0:00:00\nCollecting transformers==4.20.0\n  Downloading transformers-4.20.0-py3-none-any.whl (4.4 MB)\n      4.4/4.4 MB 80.6 MB/s eta 0:00:00\nCollecting urllib3==1.26.9\n  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n      139.0/139.0 kB 14.5 MB/s eta 0:00:00\nCollecting xxhash==3.0.0\n  Downloading xxhash-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n      211.2/211.2 kB 21.3 MB/s eta 0:00:00\nCollecting yarl==1.7.2\n  Downloading yarl-1.7.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (304 kB)\n      304.5/304.5 kB 27.5 MB/s eta 0:00:00\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from adapter-transformers==3.0.1->-r /kaggle/working/ProgressivePrompts/condaenv.k1rceg3a.requirements.txt (line 1)) (21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from adapter-transformers==3.0.1->-r /kaggle/working/ProgressivePrompts/condaenv.k1rceg3a.requirements.txt (line 1)) (1.22.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from aiohttp==3.8.1->-r /kaggle/working/ProgressivePrompts/condaenv.k1rceg3a.requirements.txt (line 2)) (21.4.0)\nRequirement already satisfied: six in /opt/conda/envs/nlp/lib/python3.9/site-packages (from data==0.4->-r /kaggle/working/ProgressivePrompts/condaenv.k1rceg3a.requirements.txt (line 9)) (1.16.0)\nRequirement already satisfied: decorator in /opt/conda/envs/nlp/lib/python3.9/site-packages (from data==0.4->-r /kaggle/working/ProgressivePrompts/condaenv.k1rceg3a.requirements.txt (line 9)) (5.1.1)\nCollecting fsspec[http]>=2021.05.0\n  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n      166.4/166.4 kB 16.0 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from huggingface-hub==0.7.0->-r /kaggle/working/ProgressivePrompts/condaenv.k1rceg3a.requirements.txt (line 18)) (4.2.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from matplotlib==3.5.2->-r /kaggle/working/ProgressivePrompts/condaenv.k1rceg3a.requirements.txt (line 23)) (9.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from matplotlib==3.5.2->-r /kaggle/working/ProgressivePrompts/condaenv.k1rceg3a.requirements.txt (line 23)) (2.8.2)\nRequirement already satisfied: torch in /opt/conda/envs/nlp/lib/python3.9/site-packages (from pytorch-ranger==0.1.1->-r /kaggle/working/ProgressivePrompts/condaenv.k1rceg3a.requirements.txt (line 30)) (1.10.1)\n  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n      173.4/173.4 kB 15.8 MB/s eta 0:00:00\n  Downloading fsspec-2023.9.1-py3-none-any.whl (173 kB)\n      173.4/173.4 kB 17.1 MB/s eta 0:00:00\n  Downloading fsspec-2023.9.0-py3-none-any.whl (173 kB)\n      173.2/173.2 kB 17.0 MB/s eta 0:00:00\n  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n      163.8/163.8 kB 16.1 MB/s eta 0:00:00\n  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n      160.1/160.1 kB 17.8 MB/s eta 0:00:00\n  Downloading fsspec-2023.4.0-py3-none-any.whl (153 kB)\n      154.0/154.0 kB 3.5 MB/s eta 0:00:00\n  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n      145.4/145.4 kB 14.5 MB/s eta 0:00:00\n  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n      143.0/143.0 kB 15.3 MB/s eta 0:00:00\n  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n      139.5/139.5 kB 13.6 MB/s eta 0:00:00\n  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n      138.8/138.8 kB 12.4 MB/s eta 0:00:00\n  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n      140.8/140.8 kB 14.2 MB/s eta 0:00:00\n  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n      141.2/141.2 kB 14.7 MB/s eta 0:00:00\n  Downloading fsspec-2022.7.0-py3-none-any.whl (141 kB)\n      141.2/141.2 kB 13.9 MB/s eta 0:00:00\nBuilding wheels for collected packages: data, future, latex, sacremoses, shutilwhich, sklearn, tempdir\n  Building wheel for data (setup.py): started\n  Building wheel for data (setup.py): finished with status 'done'\n  Created wheel for data: filename=data-0.4-py3-none-any.whl size=7227 sha256=2af8f1d1e33245be10b90c9b5e42b00837a3233d5991f104c9fd3ecdf313bede\n  Stored in directory: /root/.cache/pip/wheels/8a/0b/a3/37ca07d5a2838bba2e475e8090455e40b94631bd57a99a35f4\n  Building wheel for future (setup.py): started\n  Building wheel for future (setup.py): finished with status 'done'\n  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=633086f722d2ad006707461bf9b7bae70f7ac497179f36b616cb7e7cd9347fde\n  Stored in directory: /root/.cache/pip/wheels/2f/a0/d3/4030d9f80e6b3be787f19fc911b8e7aa462986a40ab1e4bb94\n  Building wheel for latex (setup.py): started\n  Building wheel for latex (setup.py): finished with status 'done'\n  Created wheel for latex: filename=latex-0.7.0-py3-none-any.whl size=7588 sha256=bae7814076b62208461ea64681c4c159779daac538c533ad17d8774b3734de7e\n  Stored in directory: /root/.cache/pip/wheels/94/84/e5/5ce582523fd479d00356867953085a67c47fbbc86506aa92f8\n  Building wheel for sacremoses (setup.py): started\n  Building wheel for sacremoses (setup.py): finished with status 'done'\n  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=96ddeff1867f443c1a57eca7f149fd6c1c05b0292bb03c04f932dd537e84edd8\n  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n  Building wheel for shutilwhich (setup.py): started\n  Building wheel for shutilwhich (setup.py): finished with status 'done'\n  Created wheel for shutilwhich: filename=shutilwhich-1.1.0-py3-none-any.whl size=2767 sha256=f7a0505a7be6f6498945200a70815af28493caf49d4e5271c78a2dbd4327b0a9\n  Stored in directory: /root/.cache/pip/wheels/84/c7/f5/fed66dce1ed897b44e0da776b6a592dfad0a70f7dd61f73a9d\n  Building wheel for sklearn (setup.py): started\n  Building wheel for sklearn (setup.py): finished with status 'done'\n  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1304 sha256=8db1456858bc791b72b2e51f076636a281f04122af41a8e336da32f52f77793c\n  Stored in directory: /root/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n  Building wheel for tempdir (setup.py): started\n  Building wheel for tempdir (setup.py): finished with status 'done'\n  Created wheel for tempdir: filename=tempdir-0.7.1-py3-none-any.whl size=2197 sha256=5a1559f26a4b061bd87bb6a3162cb5add6d830405d57bc5d2d86da892036e477\n  Stored in directory: /root/.cache/pip/wheels/31/7b/e3/af441c2f71a48c30809aada978c1433b163a0747e73b5805ca\nSuccessfully built data future latex sacremoses shutilwhich sklearn tempdir\nInstalling collected packages: tokenizers, tempdir, shutilwhich, sentencepiece, pytz, funcsigs, xxhash, urllib3, tqdm, threadpoolctl, scipy, regex, pyyaml, pyarrow, multidict, kiwisolver, joblib, idna, future, fsspec, frozenlist, fonttools, filelock, dill, data, cycler, click, charset-normalizer, certifi, async-timeout, yarl, scikit-learn, sacremoses, requests, pytorch-ranger, pandas, nltk, multiprocess, matplotlib, latex, aiosignal, torch-optimizer, sklearn, seaborn, responses, huggingface-hub, aiohttp, transformers, adapter-transformers, datasets\nSuccessfully installed adapter-transformers-3.0.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 certifi-2022.6.15 charset-normalizer-2.0.12 click-8.1.3 cycler-0.11.0 data-0.4 datasets-2.3.2 dill-0.3.5.1 filelock-3.7.1 fonttools-4.33.3 frozenlist-1.3.0 fsspec-2022.5.0 funcsigs-1.0.2 future-0.18.2 huggingface-hub-0.7.0 idna-3.3 joblib-1.1.0 kiwisolver-1.4.3 latex-0.7.0 matplotlib-3.5.2 multidict-6.0.2 multiprocess-0.70.13 nltk-3.7 pandas-1.4.2 pyarrow-8.0.0 pytorch-ranger-0.1.1 pytz-2022.1 pyyaml-6.0 regex-2022.6.2 requests-2.28.0 responses-0.18.0 sacremoses-0.0.53 scikit-learn-1.1.1 scipy-1.8.1 seaborn-0.12.0 sentencepiece-0.1.96 shutilwhich-1.1.0 sklearn-0.0 tempdir-0.7.1 threadpoolctl-3.1.0 tokenizers-0.12.1 torch-optimizer-0.3.0 tqdm-4.64.0 transformers-4.20.0 urllib3-1.26.9 xxhash-3.0.0 yarl-1.7.2\n\ndone\n#\n# To activate this environment, use\n#\n#     $ conda activate nlp\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd BERT_codebase\nimport os\nos.mkdir(\"results\")\nos.mkdir(\"save_checkpoint\")","metadata":{"execution":{"iopub.status.busy":"2023-11-22T13:39:00.261217Z","iopub.execute_input":"2023-11-22T13:39:00.261566Z","iopub.status.idle":"2023-11-22T13:39:00.268439Z","shell.execute_reply.started":"2023-11-22T13:39:00.261534Z","shell.execute_reply":"2023-11-22T13:39:00.267648Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working/ProgressivePrompts/BERT_codebase\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_cl2.py --task_list yelp_review_full yahoo_answers_topics amazon dbpedia_14 ag_news \\\n--prefix_MLP residual_MLP2 --lr 1e-4 --num_epochs 40 --freeze_weights 1 --prefix_len 20 \\\n--early_stopping 1 --seq_len 380 --prompt_tuning 1 \\\n--save_name BERT_order_2 --save_dir results","metadata":{"execution":{"iopub.status.busy":"2023-11-18T19:08:11.363446Z","iopub.execute_input":"2023-11-18T19:08:11.363712Z","iopub.status.idle":"2023-11-19T06:20:54.609904Z","shell.execute_reply.started":"2023-11-18T19:08:11.363689Z","shell.execute_reply":"2023-11-19T06:20:54.608721Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Max repeats =  0\nUsing MLP reparametrization with bottleneck =  800\nDownloading: 100%|| 28.0/28.0 [00:00<00:00, 24.0kB/s]\nDownloading: 100%|| 570/570 [00:00<00:00, 318kB/s]\nDownloading: 100%|| 226k/226k [00:00<00:00, 1.37MB/s]\nDownloading: 100%|| 455k/455k [00:00<00:00, 2.77MB/s]\nDownloading: 100%|| 420M/420M [00:07<00:00, 57.5MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertAdapterModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nyelp_review_full\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]']\nDownloading builder script: 4.41kB [00:00, 3.49MB/s]                            \nDownloading metadata: 2.04kB [00:00, 1.36MB/s]                                  \nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\nDownloading data: 100%|| 196M/196M [00:08<00:00, 22.6MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\nParameter 'function'=<function Dataset.prepare_dataset.<locals>.<lambda> at 0x791854b50f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n100%|| 25000/25000 [00:51<00:00, 487.66ex/s]\n100%|| 25/25 [00:23<00:00,  1.08ba/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 25000/25000 [00:50<00:00, 490.71ex/s]\n100%|| 25/25 [00:22<00:00,  1.10ba/s]\n100%|| 25000/25000 [00:51<00:00, 488.47ex/s]\n100%|| 25/25 [00:23<00:00,  1.09ba/s]\nyahoo_answers_topics\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]']\nDownloading builder script: 3.60kB [00:00, 2.77MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.33MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:09<00:00, 32.0MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:17<00:00, 323.95ex/s]\n100%|| 25/25 [00:24<00:00,  1.04ba/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 10239/10239 [00:31<00:00, 322.30ex/s]\n100%|| 11/11 [00:09<00:00,  1.12ba/s]\n100%|| 10239/10239 [00:31<00:00, 322.09ex/s]\n100%|| 11/11 [00:09<00:00,  1.13ba/s]\namazon\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]']\n100%|| 25000/25000 [01:33<00:00, 267.13ex/s]\n100%|| 25/25 [00:25<00:00,  1.00s/ba]\n100%|| 3800/3800 [00:13<00:00, 272.09ex/s]\n100%|| 4/4 [00:03<00:00,  1.07ba/s]\n100%|| 3800/3800 [00:14<00:00, 269.58ex/s]\n100%|| 4/4 [00:03<00:00,  1.08ba/s]\ndbpedia_14\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]']\nDownloading builder script: 5.22kB [00:00, 4.35MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.89MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:02<00:00, 23.3MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:53<00:00, 219.38ex/s]\n100%|| 25/25 [00:25<00:00,  1.03s/ba]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 35000/35000 [02:39<00:00, 219.28ex/s]\n100%|| 35/35 [00:36<00:00,  1.04s/ba]\n100%|| 35000/35000 [02:39<00:00, 218.85ex/s]\n100%|| 35/35 [00:36<00:00,  1.04s/ba]\nag_news\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]', '[PRE4_1]', '[PRE4_2]', '[PRE4_3]', '[PRE4_4]', '[PRE4_5]', '[PRE4_6]', '[PRE4_7]', '[PRE4_8]', '[PRE4_9]', '[PRE4_10]', '[PRE4_11]', '[PRE4_12]', '[PRE4_13]', '[PRE4_14]', '[PRE4_15]', '[PRE4_16]', '[PRE4_17]', '[PRE4_18]', '[PRE4_19]', '[PRE4_20]']\nDownloading builder script: 4.06kB [00:00, 3.14MB/s]                            \nDownloading metadata: 2.65kB [00:00, 2.00MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 62.1MB/s]                                      \nDownloading data: 1.86MB [00:00, 46.8MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|| 25000/25000 [02:15<00:00, 183.87ex/s]\n100%|| 25/25 [00:26<00:00,  1.08s/ba]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 3800/3800 [00:20<00:00, 186.05ex/s]\n100%|| 4/4 [00:04<00:00,  1.06s/ba]\n100%|| 3800/3800 [00:20<00:00, 186.89ex/s]\n100%|| 4/4 [00:04<00:00,  1.03s/ba]\nContinual training\n\n\nTASK  yelp_review_full\n0\n100%|| 3125/3125 [14:24<00:00,  3.61it/s]\nUsing cls_idx 380\nDownloading builder script: 4.21kB [00:00, 3.58MB/s]                            \n100%|| 3125/3125 [06:19<00:00,  8.24it/s]\nyelp_review_full  result =  0.56852\nNEW BEST MODEL acc= 0.56852\n1\n100%|| 3125/3125 [14:23<00:00,  3.62it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:19<00:00,  8.23it/s]\nyelp_review_full  result =  0.57696\nNEW BEST MODEL acc= 0.57696\n2\n100%|| 3125/3125 [14:24<00:00,  3.62it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:19<00:00,  8.23it/s]\nyelp_review_full  result =  0.58428\nNEW BEST MODEL acc= 0.58428\n3\n100%|| 3125/3125 [14:24<00:00,  3.62it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.22it/s]\nyelp_review_full  result =  0.58576\nNEW BEST MODEL acc= 0.58576\n4\n100%|| 3125/3125 [14:24<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.21it/s]\nyelp_review_full  result =  0.58708\nNEW BEST MODEL acc= 0.58708\n5\n100%|| 3125/3125 [14:24<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.21it/s]\nyelp_review_full  result =  0.58952\nNEW BEST MODEL acc= 0.58952\n6\n100%|| 3125/3125 [14:25<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [14:26<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:21<00:00,  8.19it/s]\nyelp_review_full  result =  0.594\nNEW BEST MODEL acc= 0.594\n8\n100%|| 3125/3125 [14:26<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.21it/s]\nyelp_review_full  result =  0.5932\n9\n100%|| 3125/3125 [14:27<00:00,  3.60it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.22it/s]\nyelp_review_full  result =  0.5968\nNEW BEST MODEL acc= 0.5968\n10\n100%|| 3125/3125 [14:25<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:19<00:00,  8.25it/s]\nyelp_review_full  result =  0.59772\nNEW BEST MODEL acc= 0.59772\n11\n100%|| 3125/3125 [14:24<00:00,  3.62it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:18<00:00,  8.25it/s]\nyelp_review_full  result =  0.596\n12\n100%|| 3125/3125 [14:25<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.21it/s]\nyelp_review_full  result =  0.59764\n13\n100%|| 3125/3125 [14:25<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:17<00:00,  8.28it/s]\nyelp_review_full  result =  0.60144\nNEW BEST MODEL acc= 0.60144\n14\n100%|| 3125/3125 [14:22<00:00,  3.62it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:17<00:00,  8.27it/s]\nyelp_review_full  result =  0.6008\n15\n100%|| 3125/3125 [14:21<00:00,  3.63it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:17<00:00,  8.27it/s]\nyelp_review_full  result =  0.60232\nNEW BEST MODEL acc= 0.60232\n16\n100%|| 3125/3125 [14:22<00:00,  3.62it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:17<00:00,  8.28it/s]\nyelp_review_full  result =  0.60364\nNEW BEST MODEL acc= 0.60364\n17\n100%|| 3125/3125 [14:21<00:00,  3.63it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:17<00:00,  8.28it/s]\nyelp_review_full  result =  0.60312\n18\n100%|| 3125/3125 [14:22<00:00,  3.62it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:17<00:00,  8.28it/s]\nyelp_review_full  result =  0.6048\nNEW BEST MODEL acc= 0.6048\n19\n100%|| 3125/3125 [14:22<00:00,  3.62it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:17<00:00,  8.28it/s]\nyelp_review_full  result =  0.60304\n20\n100%|| 3125/3125 [14:23<00:00,  3.62it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.22it/s]\nyelp_review_full  result =  0.6046\n21\n100%|| 3125/3125 [14:25<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:18<00:00,  8.27it/s]\nyelp_review_full  result =  0.60412\n22\n100%|| 3125/3125 [14:25<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:21<00:00,  8.19it/s]\nyelp_review_full  result =  0.60524\nNEW BEST MODEL acc= 0.60524\n23\n100%|| 3125/3125 [14:26<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:21<00:00,  8.20it/s]\nyelp_review_full  result =  0.60096\n24\n100%|| 3125/3125 [14:26<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:21<00:00,  8.19it/s]\nyelp_review_full  result =  0.60588\nNEW BEST MODEL acc= 0.60588\n25\n100%|| 3125/3125 [14:25<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.21it/s]\nyelp_review_full  result =  0.60572\n26\n100%|| 3125/3125 [14:25<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.21it/s]\nyelp_review_full  result =  0.60696\nNEW BEST MODEL acc= 0.60696\n27\n100%|| 3125/3125 [14:25<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.22it/s]\nyelp_review_full  result =  0.60624\n28\n100%|| 3125/3125 [14:24<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:19<00:00,  8.23it/s]\nyelp_review_full  result =  0.60708\nNEW BEST MODEL acc= 0.60708\n29\n100%|| 3125/3125 [14:25<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.22it/s]\nyelp_review_full  result =  0.60508\n30\n100%|| 3125/3125 [14:26<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:21<00:00,  8.19it/s]\nyelp_review_full  result =  0.60916\nNEW BEST MODEL acc= 0.60916\n31\n  5%|                                      | 155/3125 [00:43<13:41,  3.61it/s]^C\n  5%|                                      | 156/3125 [00:43<13:51,  3.57it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/train_cl2.py\", line 330, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/train_cl2.py\", line 46, in main\n    results_dict = CL_class.continual_training(num_epochs=args.num_epochs,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/continual_learning_utils.py\", line 485, in continual_training\n    val_scores = self.train_on_one_task(i, task,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/continual_learning_utils.py\", line 412, in train_on_one_task\n    self.trainer.pass_batch_with_prompt(batch, task, self.device,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/model_utils.py\", line 407, in pass_batch_with_prompt\n    loss.backward()\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/_tensor.py\", line 307, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 154, in backward\n    Variable._execution_engine.run_backward(\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_cl2.py --task_list yelp_review_full yahoo_answers_topics amazon dbpedia_14 ag_news \\\n--prefix_MLP residual_MLP2 --lr 1e-4 --num_epochs 40 --freeze_weights 1 --prefix_len 20 \\\n--early_stopping 1 --seq_len 380 --prompt_tuning 1 \\\n--save_name BERT_order_2 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-19T06:52:48.466918Z","iopub.execute_input":"2023-11-19T06:52:48.467565Z","iopub.status.idle":"2023-11-19T14:11:09.682977Z","shell.execute_reply.started":"2023-11-19T06:52:48.467530Z","shell.execute_reply":"2023-11-19T14:11:09.681802Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Max repeats =  0\nUsing MLP reparametrization with bottleneck =  800\nDownloading: 100%|| 28.0/28.0 [00:00<00:00, 27.1kB/s]\nDownloading: 100%|| 570/570 [00:00<00:00, 550kB/s]\nDownloading: 100%|| 226k/226k [00:00<00:00, 1.82MB/s]\nDownloading: 100%|| 455k/455k [00:00<00:00, 3.90MB/s]\nDownloading: 100%|| 420M/420M [00:07<00:00, 57.3MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertAdapterModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nyelp_review_full\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]']\nDownloading builder script: 4.41kB [00:00, 3.38MB/s]                            \nDownloading metadata: 2.04kB [00:00, 1.81MB/s]                                  \nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\nDownloading data: 100%|| 196M/196M [00:05<00:00, 32.8MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\nParameter 'function'=<function Dataset.prepare_dataset.<locals>.<lambda> at 0x78cbe7b51f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n100%|| 25000/25000 [00:49<00:00, 504.90ex/s]\n100%|| 25/25 [00:22<00:00,  1.12ba/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 25000/25000 [00:49<00:00, 508.36ex/s]\n100%|| 25/25 [00:22<00:00,  1.11ba/s]\n100%|| 25000/25000 [00:49<00:00, 504.62ex/s]\n100%|| 25/25 [00:22<00:00,  1.13ba/s]\nyahoo_answers_topics\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]']\nDownloading builder script: 3.60kB [00:00, 2.01MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.31MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:09<00:00, 33.9MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:14<00:00, 336.41ex/s]\n100%|| 25/25 [00:23<00:00,  1.08ba/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 10239/10239 [00:30<00:00, 336.39ex/s]\n100%|| 11/11 [00:09<00:00,  1.14ba/s]\n100%|| 10239/10239 [00:30<00:00, 338.41ex/s]\n100%|| 11/11 [00:09<00:00,  1.16ba/s]\namazon\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]']\n100%|| 25000/25000 [01:31<00:00, 273.28ex/s]\n100%|| 25/25 [00:23<00:00,  1.04ba/s]\n100%|| 3800/3800 [00:13<00:00, 273.55ex/s]\n100%|| 4/4 [00:03<00:00,  1.07ba/s]\n100%|| 3800/3800 [00:13<00:00, 279.17ex/s]\n100%|| 4/4 [00:03<00:00,  1.10ba/s]\ndbpedia_14\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]']\nDownloading builder script: 5.22kB [00:00, 3.92MB/s]                            \nDownloading metadata: 2.47kB [00:00, 2.15MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:02<00:00, 32.5MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:51<00:00, 224.60ex/s]\n100%|| 25/25 [00:25<00:00,  1.01s/ba]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 35000/35000 [02:34<00:00, 225.90ex/s]\n100%|| 35/35 [00:35<00:00,  1.00s/ba]\n100%|| 35000/35000 [02:33<00:00, 227.46ex/s]\n100%|| 35/35 [00:35<00:00,  1.01s/ba]\nag_news\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]', '[PRE4_1]', '[PRE4_2]', '[PRE4_3]', '[PRE4_4]', '[PRE4_5]', '[PRE4_6]', '[PRE4_7]', '[PRE4_8]', '[PRE4_9]', '[PRE4_10]', '[PRE4_11]', '[PRE4_12]', '[PRE4_13]', '[PRE4_14]', '[PRE4_15]', '[PRE4_16]', '[PRE4_17]', '[PRE4_18]', '[PRE4_19]', '[PRE4_20]']\nDownloading builder script: 4.06kB [00:00, 2.56MB/s]                            \nDownloading metadata: 2.65kB [00:00, 1.99MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 62.1MB/s]                                      \nDownloading data: 1.86MB [00:00, 42.6MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|| 25000/25000 [02:10<00:00, 191.94ex/s]\n100%|| 25/25 [00:26<00:00,  1.04s/ba]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 3800/3800 [00:19<00:00, 194.73ex/s]\n100%|| 4/4 [00:03<00:00,  1.02ba/s]\n100%|| 3800/3800 [00:19<00:00, 193.18ex/s]\n100%|| 4/4 [00:03<00:00,  1.01ba/s]\nContinual training\n\n\nTASK  yelp_review_full\n31\n100%|| 3125/3125 [14:24<00:00,  3.62it/s]\nUsing cls_idx 380\nDownloading builder script: 4.21kB [00:00, 3.45MB/s]                            \n100%|| 3125/3125 [06:19<00:00,  8.24it/s]\nyelp_review_full  result =  0.608\n32\n100%|| 3125/3125 [14:23<00:00,  3.62it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:19<00:00,  8.24it/s]\nyelp_review_full  result =  0.60836\n33\n100%|| 3125/3125 [14:25<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.21it/s]\nyelp_review_full  result =  0.60604\n34\n100%|| 3125/3125 [14:25<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:21<00:00,  8.20it/s]\nyelp_review_full  result =  0.60784\n35\n100%|| 3125/3125 [14:26<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.20it/s]\nyelp_review_full  result =  0.60852\n36\n100%|| 3125/3125 [14:26<00:00,  3.61it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:21<00:00,  8.20it/s]\nyelp_review_full  result =  0.6088\n37\n100%|| 3125/3125 [14:27<00:00,  3.60it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:21<00:00,  8.19it/s]\nyelp_review_full  result =  0.60952\nNEW BEST MODEL acc= 0.60952\n38\n100%|| 3125/3125 [14:27<00:00,  3.60it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:22<00:00,  8.18it/s]\nyelp_review_full  result =  0.6068\n39\n100%|| 3125/3125 [14:27<00:00,  3.60it/s]\nUsing cls_idx 380\n100%|| 3125/3125 [06:21<00:00,  8.19it/s]\nyelp_review_full  result =  0.6072\nUsing cls_idx 380\n100%|| 3125/3125 [06:22<00:00,  8.18it/s]\nyelp_review_full  result =  0.60316\nUsing cls_idx 400\n100%|| 1280/1280 [02:45<00:00,  7.74it/s]\nyahoo_answers_topics  result =  0.08330891688641469\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.23it/s]\namazon  result =  0.20289473684210527\nUsing cls_idx 440\n100%|| 4375/4375 [10:45<00:00,  6.78it/s]\ndbpedia_14  result =  0.10574285714285714\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.66it/s]\nag_news  result =  0.2505263157894737\n\n\nTASK  yahoo_answers_topics\n0\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.76it/s]\nyahoo_answers_topics  result =  0.7687274147866002\nNEW BEST MODEL acc= 0.7687274147866002\n1\n100%|| 3125/3125 [15:19<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:45<00:00,  7.74it/s]\nyahoo_answers_topics  result =  0.7781033304033597\nNEW BEST MODEL acc= 0.7781033304033597\n2\n100%|| 3125/3125 [15:19<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.77it/s]\nyahoo_answers_topics  result =  0.7843539408145327\nNEW BEST MODEL acc= 0.7843539408145327\n3\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.76it/s]\nyahoo_answers_topics  result =  0.7864049223556988\nNEW BEST MODEL acc= 0.7864049223556988\n4\n100%|| 3125/3125 [15:19<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:45<00:00,  7.75it/s]\nyahoo_answers_topics  result =  0.7882605723215158\nNEW BEST MODEL acc= 0.7882605723215158\n5\n100%|| 3125/3125 [15:19<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:45<00:00,  7.75it/s]\nyahoo_answers_topics  result =  0.7873815802324445\n6\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:45<00:00,  7.75it/s]\nyahoo_answers_topics  result =  0.78728391444477\n7\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.77it/s]\nyahoo_answers_topics  result =  0.7868932512940717\n8\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:45<00:00,  7.76it/s]\nyahoo_answers_topics  result =  0.7889442328352378\nNEW BEST MODEL acc= 0.7889442328352378\n9\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.77it/s]\nyahoo_answers_topics  result =  0.7911905459517531\nNEW BEST MODEL acc= 0.7911905459517531\n10\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.77it/s]\nyahoo_answers_topics  result =  0.7896278933489599\n11\n 20%|                                | 618/3125 [03:01<12:16,  3.41it/s]^C\n 20%|                                | 618/3125 [03:01<12:17,  3.40it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/train_cl2.py\", line 330, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/train_cl2.py\", line 46, in main\n    results_dict = CL_class.continual_training(num_epochs=args.num_epochs,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/continual_learning_utils.py\", line 485, in continual_training\n    val_scores = self.train_on_one_task(i, task,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/continual_learning_utils.py\", line 412, in train_on_one_task\n    self.trainer.pass_batch_with_prompt(batch, task, self.device,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/model_utils.py\", line 407, in pass_batch_with_prompt\n    loss.backward()\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/_tensor.py\", line 307, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 154, in backward\n    Variable._execution_engine.run_backward(\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_cl2.py --task_list yelp_review_full yahoo_answers_topics amazon dbpedia_14 ag_news \\\n--prefix_MLP residual_MLP2 --lr 1e-4 --num_epochs 40 --freeze_weights 1 --prefix_len 20 \\\n--early_stopping 1 --seq_len 380 --prompt_tuning 1 \\\n--save_name BERT_order_2 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-20T01:08:06.994184Z","iopub.execute_input":"2023-11-20T01:08:06.994543Z","iopub.status.idle":"2023-11-20T12:10:31.502596Z","shell.execute_reply.started":"2023-11-20T01:08:06.994509Z","shell.execute_reply":"2023-11-20T12:10:31.501528Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Max repeats =  0\nUsing MLP reparametrization with bottleneck =  800\nDownloading: 100%|| 28.0/28.0 [00:00<00:00, 24.9kB/s]\nDownloading: 100%|| 570/570 [00:00<00:00, 493kB/s]\nDownloading: 100%|| 226k/226k [00:00<00:00, 22.8MB/s]\nDownloading: 100%|| 455k/455k [00:00<00:00, 16.9MB/s]\nDownloading: 100%|| 420M/420M [00:11<00:00, 37.2MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertAdapterModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nyelp_review_full\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]']\nDownloading builder script: 4.41kB [00:00, 3.19MB/s]                            \nDownloading metadata: 2.04kB [00:00, 1.65MB/s]                                  \nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\nDownloading data: 100%|| 196M/196M [00:13<00:00, 14.5MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\nParameter 'function'=<function Dataset.prepare_dataset.<locals>.<lambda> at 0x7d8e7cbd4e50> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n100%|| 25000/25000 [00:49<00:00, 500.54ex/s]\n100%|| 25/25 [00:22<00:00,  1.12ba/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 25000/25000 [00:48<00:00, 513.04ex/s]\n100%|| 25/25 [00:22<00:00,  1.12ba/s]\n100%|| 25000/25000 [00:48<00:00, 513.52ex/s]\n100%|| 25/25 [00:22<00:00,  1.13ba/s]\nyahoo_answers_topics\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]']\nDownloading builder script: 3.60kB [00:00, 2.81MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.63MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:22<00:00, 13.9MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:13<00:00, 339.75ex/s]\n100%|| 25/25 [00:23<00:00,  1.08ba/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 10239/10239 [00:30<00:00, 340.82ex/s]\n100%|| 11/11 [00:09<00:00,  1.17ba/s]\n100%|| 10239/10239 [00:29<00:00, 343.90ex/s]\n100%|| 11/11 [00:09<00:00,  1.16ba/s]\namazon\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]']\n100%|| 25000/25000 [01:28<00:00, 282.29ex/s]\n100%|| 25/25 [00:23<00:00,  1.04ba/s]\n100%|| 3800/3800 [00:13<00:00, 290.01ex/s]\n100%|| 4/4 [00:03<00:00,  1.11ba/s]\n100%|| 3800/3800 [00:13<00:00, 284.53ex/s]\n100%|| 4/4 [00:03<00:00,  1.10ba/s]\ndbpedia_14\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]']\nDownloading builder script: 5.22kB [00:00, 4.29MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.94MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:09<00:00, 7.13MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:49<00:00, 228.10ex/s]\n100%|| 25/25 [00:24<00:00,  1.01ba/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 35000/35000 [02:31<00:00, 230.83ex/s]\n100%|| 35/35 [00:35<00:00,  1.00s/ba]\n100%|| 35000/35000 [02:31<00:00, 231.76ex/s]\n100%|| 35/35 [00:35<00:00,  1.00s/ba]\nag_news\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]', '[PRE4_1]', '[PRE4_2]', '[PRE4_3]', '[PRE4_4]', '[PRE4_5]', '[PRE4_6]', '[PRE4_7]', '[PRE4_8]', '[PRE4_9]', '[PRE4_10]', '[PRE4_11]', '[PRE4_12]', '[PRE4_13]', '[PRE4_14]', '[PRE4_15]', '[PRE4_16]', '[PRE4_17]', '[PRE4_18]', '[PRE4_19]', '[PRE4_20]']\nDownloading builder script: 4.06kB [00:00, 2.83MB/s]                            \nDownloading metadata: 2.65kB [00:00, 2.34MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 61.0MB/s]                                      \nDownloading data: 1.86MB [00:00, 52.9MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|| 25000/25000 [02:08<00:00, 193.93ex/s]\n100%|| 25/25 [00:25<00:00,  1.03s/ba]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 3800/3800 [00:19<00:00, 197.50ex/s]\n100%|| 4/4 [00:03<00:00,  1.01ba/s]\n100%|| 3800/3800 [00:19<00:00, 197.61ex/s]\n100%|| 4/4 [00:03<00:00,  1.03ba/s]\nContinual training\n\n\nTASK  yahoo_answers_topics\n11\n100%|| 3125/3125 [15:13<00:00,  3.42it/s]\nUsing cls_idx 400\nDownloading builder script: 4.21kB [00:00, 3.84MB/s]                            \n100%|| 1280/1280 [02:43<00:00,  7.81it/s]\nyahoo_answers_topics  result =  0.7936321906436176\nNEW BEST MODEL acc= 0.7936321906436176\n12\n100%|| 3125/3125 [15:16<00:00,  3.41it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.80it/s]\nyahoo_answers_topics  result =  0.7917765406778006\n13\n100%|| 3125/3125 [15:17<00:00,  3.41it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.79it/s]\nyahoo_answers_topics  result =  0.7894325617736107\n14\n100%|| 3125/3125 [15:17<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.77it/s]\nyahoo_answers_topics  result =  0.7930461959175701\n15\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.76it/s]\nyahoo_answers_topics  result =  0.789823224924309\n16\n100%|| 3125/3125 [15:19<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.78it/s]\nyahoo_answers_topics  result =  0.7950971774587362\nNEW BEST MODEL acc= 0.7950971774587362\n17\n100%|| 3125/3125 [15:19<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.78it/s]\nyahoo_answers_topics  result =  0.7946088485203633\n18\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:45<00:00,  7.75it/s]\nyahoo_answers_topics  result =  0.7944135169450142\n19\n100%|| 3125/3125 [15:17<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.79it/s]\nyahoo_answers_topics  result =  0.793534524855943\n20\n100%|| 3125/3125 [15:15<00:00,  3.41it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:43<00:00,  7.81it/s]\nyahoo_answers_topics  result =  0.7934368590682684\n21\n100%|| 3125/3125 [15:17<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.79it/s]\nyahoo_answers_topics  result =  0.7938275222189667\n22\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.80it/s]\nyahoo_answers_topics  result =  0.7919718722531497\n23\n100%|| 3125/3125 [15:16<00:00,  3.41it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.80it/s]\nyahoo_answers_topics  result =  0.7918742064654751\n24\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.79it/s]\nyahoo_answers_topics  result =  0.7903115538626819\n25\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.78it/s]\nyahoo_answers_topics  result =  0.7922648696161735\n26\n100%|| 3125/3125 [15:17<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:43<00:00,  7.83it/s]\nyahoo_answers_topics  result =  0.7932415274929192\n27\n100%|| 3125/3125 [15:17<00:00,  3.41it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:43<00:00,  7.82it/s]\nyahoo_answers_topics  result =  0.7914835433147769\n28\n100%|| 3125/3125 [15:17<00:00,  3.41it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:43<00:00,  7.81it/s]\nyahoo_answers_topics  result =  0.7927531985545463\n29\n100%|| 3125/3125 [15:16<00:00,  3.41it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:43<00:00,  7.81it/s]\nyahoo_answers_topics  result =  0.7918742064654751\n30\n100%|| 3125/3125 [15:17<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.77it/s]\nyahoo_answers_topics  result =  0.792362535403848\n31\n100%|| 3125/3125 [15:19<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.76it/s]\nyahoo_answers_topics  result =  0.7907022170133802\n32\n100%|| 3125/3125 [15:19<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.76it/s]\nyahoo_answers_topics  result =  0.791678874890126\n33\n100%|| 3125/3125 [15:19<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:45<00:00,  7.76it/s]\nyahoo_answers_topics  result =  0.7918742064654751\n34\n100%|| 3125/3125 [15:19<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.76it/s]\nyahoo_answers_topics  result =  0.7897255591366344\n35\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.77it/s]\nyahoo_answers_topics  result =  0.792850864342221\n36\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.76it/s]\nyahoo_answers_topics  result =  0.7887489012598886\n37\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.77it/s]\nyahoo_answers_topics  result =  0.7897255591366344\n38\n100%|| 3125/3125 [15:18<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.77it/s]\nyahoo_answers_topics  result =  0.7880652407461666\n39\n100%|| 3125/3125 [15:19<00:00,  3.40it/s]\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.77it/s]\nyahoo_answers_topics  result =  0.7893348959859361\nUsing cls_idx 380\n100%|| 3125/3125 [06:20<00:00,  8.20it/s]\nyelp_review_full  result =  0.60316\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.76it/s]\nyahoo_answers_topics  result =  0.792850864342221\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.22it/s]\namazon  result =  0.19526315789473683\nUsing cls_idx 440\n100%|| 4375/4375 [10:43<00:00,  6.80it/s]\ndbpedia_14  result =  0.09962857142857143\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.67it/s]\nag_news  result =  0.2505263157894737\n\n\nTASK  amazon\n0\n100%|| 3125/3125 [16:15<00:00,  3.20it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.23it/s]\namazon  result =  0.5363157894736842\nNEW BEST MODEL acc= 0.5363157894736842\n1\n100%|| 3125/3125 [16:15<00:00,  3.20it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.23it/s]\namazon  result =  0.5452631578947369\nNEW BEST MODEL acc= 0.5452631578947369\n2\n100%|| 3125/3125 [16:15<00:00,  3.20it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.22it/s]\namazon  result =  0.5565789473684211\nNEW BEST MODEL acc= 0.5565789473684211\n3\n100%|| 3125/3125 [16:15<00:00,  3.20it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.23it/s]\namazon  result =  0.5610526315789474\nNEW BEST MODEL acc= 0.5610526315789474\n4\n100%|| 3125/3125 [16:14<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.24it/s]\namazon  result =  0.5726315789473684\nNEW BEST MODEL acc= 0.5726315789473684\n5\n  5%|                                      | 166/3125 [00:51<15:23,  3.21it/s]^C\n  5%|                                      | 166/3125 [00:52<15:28,  3.19it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/train_cl2.py\", line 330, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/train_cl2.py\", line 46, in main\n    results_dict = CL_class.continual_training(num_epochs=args.num_epochs,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/continual_learning_utils.py\", line 485, in continual_training\n    val_scores = self.train_on_one_task(i, task,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/continual_learning_utils.py\", line 412, in train_on_one_task\n    self.trainer.pass_batch_with_prompt(batch, task, self.device,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/model_utils.py\", line 407, in pass_batch_with_prompt\n    loss.backward()\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/_tensor.py\", line 307, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 154, in backward\n    Variable._execution_engine.run_backward(\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_cl2.py --task_list yelp_review_full yahoo_answers_topics amazon dbpedia_14 ag_news \\\n--prefix_MLP residual_MLP2 --lr 1e-4 --num_epochs 40 --freeze_weights 1 --prefix_len 20 \\\n--early_stopping 1 --seq_len 380 --prompt_tuning 1 \\\n--save_name BERT_order_2 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-20T17:14:19.472709Z","iopub.execute_input":"2023-11-20T17:14:19.473041Z","iopub.status.idle":"2023-11-21T01:34:35.899838Z","shell.execute_reply.started":"2023-11-20T17:14:19.473009Z","shell.execute_reply":"2023-11-21T01:34:35.898707Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Max repeats =  0\nUsing MLP reparametrization with bottleneck =  800\nDownloading: 100%|| 28.0/28.0 [00:00<00:00, 24.3kB/s]\nDownloading: 100%|| 570/570 [00:00<00:00, 476kB/s]\nDownloading: 100%|| 226k/226k [00:00<00:00, 2.96MB/s]\nDownloading: 100%|| 455k/455k [00:00<00:00, 11.6MB/s]\nDownloading: 100%|| 420M/420M [00:07<00:00, 60.2MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertAdapterModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nyelp_review_full\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]']\nDownloading builder script: 4.41kB [00:00, 3.39MB/s]                            \nDownloading metadata: 2.04kB [00:00, 1.65MB/s]                                  \nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\nDownloading data: 100%|| 196M/196M [00:05<00:00, 36.5MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\nParameter 'function'=<function Dataset.prepare_dataset.<locals>.<lambda> at 0x7d079c1d7f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n100%|| 25000/25000 [00:47<00:00, 521.94ex/s]\n100%|| 25/25 [00:21<00:00,  1.16ba/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 25000/25000 [00:47<00:00, 524.64ex/s]\n100%|| 25/25 [00:21<00:00,  1.15ba/s]\n100%|| 25000/25000 [00:47<00:00, 523.11ex/s]\n100%|| 25/25 [00:21<00:00,  1.15ba/s]\nyahoo_answers_topics\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]']\nDownloading builder script: 3.60kB [00:00, 3.49MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.74MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:10<00:00, 29.6MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:13<00:00, 339.89ex/s]\n100%|| 25/25 [00:23<00:00,  1.08ba/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 10239/10239 [00:30<00:00, 338.31ex/s]\n100%|| 11/11 [00:09<00:00,  1.16ba/s]\n100%|| 10239/10239 [00:30<00:00, 338.65ex/s]\n100%|| 11/11 [00:09<00:00,  1.17ba/s]\namazon\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]']\n100%|| 25000/25000 [01:29<00:00, 278.86ex/s]\n100%|| 25/25 [00:24<00:00,  1.03ba/s]\n100%|| 3800/3800 [00:13<00:00, 286.04ex/s]\n100%|| 4/4 [00:03<00:00,  1.11ba/s]\n100%|| 3800/3800 [00:13<00:00, 282.06ex/s]\n100%|| 4/4 [00:03<00:00,  1.11ba/s]\ndbpedia_14\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]']\nDownloading builder script: 5.22kB [00:00, 4.01MB/s]                            \nDownloading metadata: 2.47kB [00:00, 2.08MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:02<00:00, 30.7MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:49<00:00, 227.36ex/s]\n100%|| 25/25 [00:25<00:00,  1.01s/ba]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 35000/35000 [02:32<00:00, 230.11ex/s]\n100%|| 35/35 [00:34<00:00,  1.01ba/s]\n100%|| 35000/35000 [02:31<00:00, 231.43ex/s]\n100%|| 35/35 [00:34<00:00,  1.01ba/s]\nag_news\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]', '[PRE4_1]', '[PRE4_2]', '[PRE4_3]', '[PRE4_4]', '[PRE4_5]', '[PRE4_6]', '[PRE4_7]', '[PRE4_8]', '[PRE4_9]', '[PRE4_10]', '[PRE4_11]', '[PRE4_12]', '[PRE4_13]', '[PRE4_14]', '[PRE4_15]', '[PRE4_16]', '[PRE4_17]', '[PRE4_18]', '[PRE4_19]', '[PRE4_20]']\nDownloading builder script: 4.06kB [00:00, 2.92MB/s]                            \nDownloading metadata: 2.65kB [00:00, 1.77MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 61.7MB/s]                                      \nDownloading data: 1.86MB [00:00, 39.4MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|| 25000/25000 [02:08<00:00, 195.30ex/s]\n100%|| 25/25 [00:25<00:00,  1.03s/ba]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 3800/3800 [00:19<00:00, 196.54ex/s]\n100%|| 4/4 [00:03<00:00,  1.03ba/s]\n100%|| 3800/3800 [00:19<00:00, 196.58ex/s]\n100%|| 4/4 [00:03<00:00,  1.03ba/s]\nContinual training\n\n\nTASK  amazon\n5\n100%|| 3125/3125 [15:59<00:00,  3.26it/s]\nUsing cls_idx 420\nDownloading builder script: 4.21kB [00:00, 3.48MB/s]                            \n100%|| 475/475 [01:04<00:00,  7.35it/s]\namazon  result =  0.5723684210526315\n6\n100%|| 3125/3125 [16:02<00:00,  3.25it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.31it/s]\namazon  result =  0.5689473684210526\n7\n100%|| 3125/3125 [16:07<00:00,  3.23it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.27it/s]\namazon  result =  0.5760526315789474\nNEW BEST MODEL acc= 0.5760526315789474\n8\n100%|| 3125/3125 [16:10<00:00,  3.22it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.30it/s]\namazon  result =  0.5747368421052632\n9\n100%|| 3125/3125 [16:10<00:00,  3.22it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.28it/s]\namazon  result =  0.5794736842105264\nNEW BEST MODEL acc= 0.5794736842105264\n10\n100%|| 3125/3125 [16:10<00:00,  3.22it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.29it/s]\namazon  result =  0.5752631578947368\n11\n100%|| 3125/3125 [16:11<00:00,  3.22it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.28it/s]\namazon  result =  0.5765789473684211\n12\n100%|| 3125/3125 [16:12<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.27it/s]\namazon  result =  0.5760526315789474\n13\n100%|| 3125/3125 [16:12<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.27it/s]\namazon  result =  0.5831578947368421\nNEW BEST MODEL acc= 0.5831578947368421\n14\n100%|| 3125/3125 [16:13<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.26it/s]\namazon  result =  0.5768421052631579\n15\n100%|| 3125/3125 [16:13<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.26it/s]\namazon  result =  0.5807894736842105\n16\n100%|| 3125/3125 [16:14<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.26it/s]\namazon  result =  0.5821052631578948\n17\n100%|| 3125/3125 [16:14<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.26it/s]\namazon  result =  0.5847368421052631\nNEW BEST MODEL acc= 0.5847368421052631\n18\n100%|| 3125/3125 [16:14<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.27it/s]\namazon  result =  0.5805263157894737\n19\n100%|| 3125/3125 [16:14<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.25it/s]\namazon  result =  0.5797368421052631\n20\n100%|| 3125/3125 [16:14<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.26it/s]\namazon  result =  0.5760526315789474\n21\n100%|| 3125/3125 [16:14<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.27it/s]\namazon  result =  0.5857894736842105\nNEW BEST MODEL acc= 0.5857894736842105\n22\n100%|| 3125/3125 [16:13<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.28it/s]\namazon  result =  0.5797368421052631\n23\n100%|| 3125/3125 [16:12<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.27it/s]\namazon  result =  0.58\n24\n100%|| 3125/3125 [16:12<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.28it/s]\namazon  result =  0.583421052631579\n25\n100%|| 3125/3125 [16:13<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.27it/s]\namazon  result =  0.5776315789473684\n26\n100%|| 3125/3125 [16:13<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.28it/s]\namazon  result =  0.5813157894736842\n27\n100%|| 3125/3125 [16:13<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.28it/s]\namazon  result =  0.5802631578947368\n28\n100%|| 3125/3125 [16:13<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.27it/s]\namazon  result =  0.5794736842105264\n29\n100%|| 3125/3125 [16:13<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.28it/s]\namazon  result =  0.5815789473684211\n30\n100%|| 3125/3125 [16:13<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.27it/s]\namazon  result =  0.5786842105263158\n31\n100%|| 3125/3125 [16:13<00:00,  3.21it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.28it/s]\namazon  result =  0.5823684210526315\n32\n 37%|                        | 1158/3125 [06:00<10:09,  3.23it/s]^C\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_cl2.py --task_list yelp_review_full yahoo_answers_topics amazon dbpedia_14 ag_news \\\n--prefix_MLP residual_MLP2 --lr 1e-4 --num_epochs 40 --freeze_weights 1 --prefix_len 20 \\\n--early_stopping 1 --seq_len 380 --prompt_tuning 1 \\\n--save_name BERT_order_2 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-21T05:05:26.596381Z","iopub.execute_input":"2023-11-21T05:05:26.596927Z","iopub.status.idle":"2023-11-21T06:42:06.109400Z","shell.execute_reply.started":"2023-11-21T05:05:26.596892Z","shell.execute_reply":"2023-11-21T06:42:06.108121Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Max repeats =  0\nUsing MLP reparametrization with bottleneck =  800\nDownloading: 100%|| 28.0/28.0 [00:00<00:00, 24.3kB/s]\nDownloading: 100%|| 570/570 [00:00<00:00, 554kB/s]\nDownloading: 100%|| 226k/226k [00:00<00:00, 3.64MB/s]\nDownloading: 100%|| 455k/455k [00:00<00:00, 3.54MB/s]\nDownloading: 100%|| 420M/420M [00:07<00:00, 58.6MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertAdapterModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nyelp_review_full\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]']\nDownloading builder script: 4.41kB [00:00, 2.69MB/s]                            \nDownloading metadata: 2.04kB [00:00, 1.85MB/s]                                  \nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\nDownloading data: 100%|| 196M/196M [00:05<00:00, 33.4MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\nParameter 'function'=<function Dataset.prepare_dataset.<locals>.<lambda> at 0x7a00c7fd3f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n100%|| 25000/25000 [00:48<00:00, 511.21ex/s]\n100%|| 25/25 [00:22<00:00,  1.11ba/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 25000/25000 [00:48<00:00, 513.50ex/s]\n100%|| 25/25 [00:22<00:00,  1.10ba/s]\n100%|| 25000/25000 [00:48<00:00, 511.46ex/s]\n100%|| 25/25 [00:23<00:00,  1.07ba/s]\nyahoo_answers_topics\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]']\nDownloading builder script: 3.60kB [00:00, 2.65MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.56MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:11<00:00, 27.4MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:14<00:00, 335.98ex/s]\n100%|| 25/25 [00:24<00:00,  1.04ba/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 10239/10239 [00:30<00:00, 333.72ex/s]\n100%|| 11/11 [00:10<00:00,  1.10ba/s]\n100%|| 10239/10239 [00:30<00:00, 334.20ex/s]\n100%|| 11/11 [00:09<00:00,  1.13ba/s]\namazon\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]']\n100%|| 25000/25000 [01:30<00:00, 275.16ex/s]\n100%|| 25/25 [00:24<00:00,  1.00ba/s]\n100%|| 3800/3800 [00:13<00:00, 281.63ex/s]\n100%|| 4/4 [00:03<00:00,  1.04ba/s]\n100%|| 3800/3800 [00:13<00:00, 280.84ex/s]\n100%|| 4/4 [00:03<00:00,  1.08ba/s]\ndbpedia_14\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]']\nDownloading builder script: 5.22kB [00:00, 3.38MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.83MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:02<00:00, 33.0MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:50<00:00, 225.32ex/s]\n100%|| 25/25 [00:26<00:00,  1.05s/ba]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 35000/35000 [02:35<00:00, 224.96ex/s]\n100%|| 35/35 [00:37<00:00,  1.06s/ba]\n100%|| 35000/35000 [02:35<00:00, 225.07ex/s]\n100%|| 35/35 [00:36<00:00,  1.05s/ba]\nag_news\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]', '[PRE4_1]', '[PRE4_2]', '[PRE4_3]', '[PRE4_4]', '[PRE4_5]', '[PRE4_6]', '[PRE4_7]', '[PRE4_8]', '[PRE4_9]', '[PRE4_10]', '[PRE4_11]', '[PRE4_12]', '[PRE4_13]', '[PRE4_14]', '[PRE4_15]', '[PRE4_16]', '[PRE4_17]', '[PRE4_18]', '[PRE4_19]', '[PRE4_20]']\nDownloading builder script: 4.06kB [00:00, 2.33MB/s]                            \nDownloading metadata: 2.65kB [00:00, 1.72MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 58.1MB/s]                                      \nDownloading data: 1.86MB [00:00, 33.6MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|| 25000/25000 [02:11<00:00, 189.50ex/s]\n100%|| 25/25 [00:27<00:00,  1.11s/ba]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 3800/3800 [00:19<00:00, 191.24ex/s]\n100%|| 4/4 [00:04<00:00,  1.04s/ba]\n100%|| 3800/3800 [00:19<00:00, 191.52ex/s]\n100%|| 4/4 [00:04<00:00,  1.03s/ba]\nContinual training\n\n\nTASK  amazon\n32\n100%|| 3125/3125 [15:55<00:00,  3.27it/s]\nUsing cls_idx 420\nDownloading builder script: 4.21kB [00:00, 3.27MB/s]                            \n100%|| 475/475 [01:04<00:00,  7.39it/s]\namazon  result =  0.5773684210526315\n33\n100%|| 3125/3125 [15:59<00:00,  3.26it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:04<00:00,  7.34it/s]\namazon  result =  0.5768421052631579\n34\n100%|| 3125/3125 [16:04<00:00,  3.24it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:04<00:00,  7.31it/s]\namazon  result =  0.5739473684210527\n35\n100%|| 3125/3125 [16:07<00:00,  3.23it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.29it/s]\namazon  result =  0.5773684210526315\n36\n 12%|                                   | 389/3125 [02:00<14:09,  3.22it/s]^C\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_cl2.py --task_list yelp_review_full yahoo_answers_topics amazon dbpedia_14 ag_news \\\n--prefix_MLP residual_MLP2 --lr 1e-4 --num_epochs 40 --freeze_weights 1 --prefix_len 20 \\\n--early_stopping 1 --seq_len 380 --prompt_tuning 1 \\\n--save_name BERT_order_2 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-21T07:13:49.311298Z","iopub.execute_input":"2023-11-21T07:13:49.311628Z","iopub.status.idle":"2023-11-21T18:53:22.016866Z","shell.execute_reply.started":"2023-11-21T07:13:49.311596Z","shell.execute_reply":"2023-11-21T18:53:22.015719Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Max repeats =  0\nUsing MLP reparametrization with bottleneck =  800\nDownloading: 100%|| 28.0/28.0 [00:00<00:00, 23.4kB/s]\nDownloading: 100%|| 570/570 [00:00<00:00, 648kB/s]\nDownloading: 100%|| 226k/226k [00:00<00:00, 25.6MB/s]\nDownloading: 100%|| 455k/455k [00:00<00:00, 1.14MB/s]\nDownloading: 100%|| 420M/420M [00:11<00:00, 38.9MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertAdapterModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nyelp_review_full\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]']\nDownloading builder script: 4.41kB [00:00, 3.41MB/s]                            \nDownloading metadata: 2.04kB [00:00, 2.08MB/s]                                  \nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\nDownloading data: 100%|| 196M/196M [00:13<00:00, 14.9MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\nParameter 'function'=<function Dataset.prepare_dataset.<locals>.<lambda> at 0x78e15fec0e50> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n100%|| 25000/25000 [00:49<00:00, 500.25ex/s]\n100%|| 25/25 [00:22<00:00,  1.14ba/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 25000/25000 [00:49<00:00, 501.54ex/s]\n100%|| 25/25 [00:22<00:00,  1.14ba/s]\n100%|| 25000/25000 [00:50<00:00, 498.48ex/s]\n100%|| 25/25 [00:22<00:00,  1.13ba/s]\nyahoo_answers_topics\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]']\nDownloading builder script: 3.60kB [00:00, 2.38MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.59MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:21<00:00, 14.8MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:15<00:00, 329.18ex/s]\n100%|| 25/25 [00:23<00:00,  1.08ba/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 10239/10239 [00:31<00:00, 328.08ex/s]\n100%|| 11/11 [00:09<00:00,  1.14ba/s]\n100%|| 10239/10239 [00:32<00:00, 319.86ex/s]\n100%|| 11/11 [00:09<00:00,  1.14ba/s]\namazon\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]']\n100%|| 25000/25000 [01:34<00:00, 264.42ex/s]\n100%|| 25/25 [00:24<00:00,  1.03ba/s]\n100%|| 3800/3800 [00:13<00:00, 276.77ex/s]\n100%|| 4/4 [00:03<00:00,  1.10ba/s]\n100%|| 3800/3800 [00:13<00:00, 274.66ex/s]\n100%|| 4/4 [00:03<00:00,  1.11ba/s]\ndbpedia_14\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]']\nDownloading builder script: 5.22kB [00:00, 3.64MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.92MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:06<00:00, 11.1MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:52<00:00, 222.77ex/s]\n100%|| 25/25 [00:25<00:00,  1.00s/ba]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 35000/35000 [02:37<00:00, 222.50ex/s]\n100%|| 35/35 [00:35<00:00,  1.02s/ba]\n100%|| 35000/35000 [02:37<00:00, 222.71ex/s]\n100%|| 35/35 [00:35<00:00,  1.01s/ba]\nag_news\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]', '[PRE4_1]', '[PRE4_2]', '[PRE4_3]', '[PRE4_4]', '[PRE4_5]', '[PRE4_6]', '[PRE4_7]', '[PRE4_8]', '[PRE4_9]', '[PRE4_10]', '[PRE4_11]', '[PRE4_12]', '[PRE4_13]', '[PRE4_14]', '[PRE4_15]', '[PRE4_16]', '[PRE4_17]', '[PRE4_18]', '[PRE4_19]', '[PRE4_20]']\nDownloading builder script: 4.06kB [00:00, 2.73MB/s]                            \nDownloading metadata: 2.65kB [00:00, 2.23MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 62.9MB/s]                                      \nDownloading data: 1.86MB [00:00, 52.2MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|| 25000/25000 [02:13<00:00, 186.84ex/s]\n100%|| 25/25 [00:26<00:00,  1.05s/ba]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 3800/3800 [00:20<00:00, 187.33ex/s]\n100%|| 4/4 [00:03<00:00,  1.00ba/s]\n100%|| 3800/3800 [00:20<00:00, 187.32ex/s]\n100%|| 4/4 [00:03<00:00,  1.01ba/s]\nContinual training\n\n\nTASK  amazon\n36\n100%|| 3125/3125 [15:57<00:00,  3.26it/s]\nUsing cls_idx 420\nDownloading builder script: 4.21kB [00:00, 2.04MB/s]                            \n100%|| 475/475 [01:04<00:00,  7.37it/s]\namazon  result =  0.5815789473684211\n37\n100%|| 3125/3125 [16:02<00:00,  3.25it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:04<00:00,  7.34it/s]\namazon  result =  0.5826315789473684\n38\n100%|| 3125/3125 [16:05<00:00,  3.24it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:04<00:00,  7.31it/s]\namazon  result =  0.583421052631579\n39\n100%|| 3125/3125 [16:08<00:00,  3.23it/s]\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.29it/s]\namazon  result =  0.583421052631579\nUsing cls_idx 380\n100%|| 3125/3125 [06:18<00:00,  8.25it/s]\nyelp_review_full  result =  0.60316\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.79it/s]\nyahoo_answers_topics  result =  0.792850864342221\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.29it/s]\namazon  result =  0.5776315789473684\nUsing cls_idx 440\n100%|| 4375/4375 [10:43<00:00,  6.80it/s]\ndbpedia_14  result =  0.10237142857142857\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.69it/s]\nag_news  result =  0.25026315789473685\n\n\nTASK  dbpedia_14\n0\n100%|| 3125/3125 [17:15<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:42<00:00,  6.81it/s]\ndbpedia_14  result =  0.9869428571428571\nNEW BEST MODEL acc= 0.9869428571428571\n1\n100%|| 3125/3125 [17:16<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:42<00:00,  6.81it/s]\ndbpedia_14  result =  0.9895714285714285\nNEW BEST MODEL acc= 0.9895714285714285\n2\n100%|| 3125/3125 [17:17<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:44<00:00,  6.78it/s]\ndbpedia_14  result =  0.9900857142857142\nNEW BEST MODEL acc= 0.9900857142857142\n3\n100%|| 3125/3125 [17:18<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:45<00:00,  6.78it/s]\ndbpedia_14  result =  0.9897428571428571\n4\n100%|| 3125/3125 [17:18<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:45<00:00,  6.78it/s]\ndbpedia_14  result =  0.9900857142857142\n5\n100%|| 3125/3125 [17:18<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:44<00:00,  6.79it/s]\ndbpedia_14  result =  0.9902285714285715\nNEW BEST MODEL acc= 0.9902285714285715\n6\n100%|| 3125/3125 [17:19<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:44<00:00,  6.79it/s]\ndbpedia_14  result =  0.9898857142857143\n7\n100%|| 3125/3125 [17:18<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:43<00:00,  6.79it/s]\ndbpedia_14  result =  0.9898857142857143\n8\n100%|| 3125/3125 [17:17<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:43<00:00,  6.79it/s]\ndbpedia_14  result =  0.9903142857142857\nNEW BEST MODEL acc= 0.9903142857142857\n9\n100%|| 3125/3125 [17:17<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:43<00:00,  6.80it/s]\ndbpedia_14  result =  0.9902857142857143\n10\n100%|| 3125/3125 [17:16<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:43<00:00,  6.80it/s]\ndbpedia_14  result =  0.9899142857142857\n11\n100%|| 3125/3125 [17:17<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:43<00:00,  6.80it/s]\ndbpedia_14  result =  0.9891428571428571\n12\n100%|| 3125/3125 [17:15<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:40<00:00,  6.84it/s]\ndbpedia_14  result =  0.9897428571428571\n13\n100%|| 3125/3125 [17:14<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:40<00:00,  6.83it/s]\ndbpedia_14  result =  0.9896857142857143\n14\n100%|| 3125/3125 [17:15<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:41<00:00,  6.82it/s]\ndbpedia_14  result =  0.9901428571428571\n15\n100%|| 3125/3125 [17:14<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:39<00:00,  6.84it/s]\ndbpedia_14  result =  0.9901714285714286\n16\n100%|| 3125/3125 [17:12<00:00,  3.03it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:39<00:00,  6.84it/s]\ndbpedia_14  result =  0.99\n17\n100%|| 3125/3125 [17:12<00:00,  3.03it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:39<00:00,  6.84it/s]\ndbpedia_14  result =  0.9900571428571429\n18\n100%|| 3125/3125 [17:12<00:00,  3.03it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:39<00:00,  6.84it/s]\ndbpedia_14  result =  0.9898571428571429\n19\n100%|| 3125/3125 [17:12<00:00,  3.03it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:39<00:00,  6.84it/s]\ndbpedia_14  result =  0.9898\n20\n100%|| 3125/3125 [17:12<00:00,  3.03it/s]\nUsing cls_idx 440\n 30%|                           | 1308/4375 [03:10<07:27,  6.85it/s]^C\n 30%|                           | 1308/4375 [03:11<07:28,  6.84it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/train_cl2.py\", line 330, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/train_cl2.py\", line 46, in main\n    results_dict = CL_class.continual_training(num_epochs=args.num_epochs,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/continual_learning_utils.py\", line 485, in continual_training\n    val_scores = self.train_on_one_task(i, task,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/continual_learning_utils.py\", line 425, in train_on_one_task\n    val_scores = self.eval_on_tasks(val_scores, split='val', prompt_tuning=prompt_tuning, original_task_id=task_id, tasks_to_eval=[task])\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/continual_learning_utils.py\", line 273, in eval_on_tasks\n    result = self.trainer.eval_with_prompt(dataloader_val, task, None,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/model_utils.py\", line 595, in eval_with_prompt\n    metric.add_batch(predictions=predictions, references=batch['labels'])\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/datasets/metric.py\", line 479, in add_batch\n    batch = self.info.features.encode_batch(batch)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/datasets/features/features.py\", line 1595, in encode_batch\n    column = cast_to_python_objects(column)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/datasets/features/features.py\", line 398, in cast_to_python_objects\n    return _cast_to_python_objects(\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/datasets/features/features.py\", line 308, in _cast_to_python_objects\n    return obj.detach().cpu().numpy(), True\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_cl2.py --task_list yelp_review_full yahoo_answers_topics amazon dbpedia_14 ag_news \\\n--prefix_MLP residual_MLP2 --lr 1e-4 --num_epochs 40 --freeze_weights 1 --prefix_len 20 \\\n--early_stopping 1 --seq_len 380 --prompt_tuning 1 \\\n--save_name BERT_order_2 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-21T19:24:59.852142Z","iopub.execute_input":"2023-11-21T19:24:59.852390Z","iopub.status.idle":"2023-11-22T06:31:50.338390Z","shell.execute_reply.started":"2023-11-21T19:24:59.852368Z","shell.execute_reply":"2023-11-22T06:31:50.337365Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Max repeats =  0\nUsing MLP reparametrization with bottleneck =  800\nDownloading: 100%|| 28.0/28.0 [00:00<00:00, 31.8kB/s]\nDownloading: 100%|| 570/570 [00:00<00:00, 630kB/s]\nDownloading: 100%|| 226k/226k [00:00<00:00, 2.77MB/s]\nDownloading: 100%|| 455k/455k [00:00<00:00, 1.88MB/s]\nDownloading: 100%|| 420M/420M [00:07<00:00, 56.2MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertAdapterModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nyelp_review_full\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]']\nDownloading builder script: 4.41kB [00:00, 2.83MB/s]                            \nDownloading metadata: 2.04kB [00:00, 2.16MB/s]                                  \nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\nDownloading data: 100%|| 196M/196M [00:07<00:00, 26.2MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\nParameter 'function'=<function Dataset.prepare_dataset.<locals>.<lambda> at 0x7f4471cc1f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n100%|| 25000/25000 [00:50<00:00, 491.70ex/s]\n100%|| 25/25 [00:22<00:00,  1.12ba/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 25000/25000 [00:51<00:00, 488.81ex/s]\n100%|| 25/25 [00:22<00:00,  1.11ba/s]\n100%|| 25000/25000 [00:51<00:00, 488.68ex/s]\n100%|| 25/25 [00:22<00:00,  1.11ba/s]\nyahoo_answers_topics\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]']\nDownloading builder script: 3.60kB [00:00, 3.07MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.42MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:11<00:00, 28.8MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:18<00:00, 318.76ex/s]\n100%|| 25/25 [00:23<00:00,  1.06ba/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 10239/10239 [00:32<00:00, 318.35ex/s]\n100%|| 11/11 [00:09<00:00,  1.13ba/s]\n100%|| 10239/10239 [00:31<00:00, 320.07ex/s]\n100%|| 11/11 [00:09<00:00,  1.14ba/s]\namazon\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]']\n100%|| 25000/25000 [01:33<00:00, 267.65ex/s]\n100%|| 25/25 [00:24<00:00,  1.01ba/s]\n100%|| 3800/3800 [00:13<00:00, 274.63ex/s]\n100%|| 4/4 [00:03<00:00,  1.06ba/s]\n100%|| 3800/3800 [00:13<00:00, 273.82ex/s]\n100%|| 4/4 [00:03<00:00,  1.08ba/s]\ndbpedia_14\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]']\nDownloading builder script: 5.22kB [00:00, 4.35MB/s]                            \nDownloading metadata: 2.47kB [00:00, 2.43MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:02<00:00, 25.3MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:53<00:00, 220.12ex/s]\n100%|| 25/25 [00:25<00:00,  1.01s/ba]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 35000/35000 [02:38<00:00, 220.61ex/s]\n100%|| 35/35 [00:35<00:00,  1.02s/ba]\n100%|| 35000/35000 [02:39<00:00, 219.34ex/s]\n100%|| 35/35 [00:36<00:00,  1.03s/ba]\nag_news\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]', '[PRE4_1]', '[PRE4_2]', '[PRE4_3]', '[PRE4_4]', '[PRE4_5]', '[PRE4_6]', '[PRE4_7]', '[PRE4_8]', '[PRE4_9]', '[PRE4_10]', '[PRE4_11]', '[PRE4_12]', '[PRE4_13]', '[PRE4_14]', '[PRE4_15]', '[PRE4_16]', '[PRE4_17]', '[PRE4_18]', '[PRE4_19]', '[PRE4_20]']\nDownloading builder script: 4.06kB [00:00, 2.72MB/s]                            \nDownloading metadata: 2.65kB [00:00, 2.03MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 58.0MB/s]                                      \nDownloading data: 1.86MB [00:00, 50.0MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|| 25000/25000 [02:17<00:00, 182.18ex/s]\n100%|| 25/25 [00:26<00:00,  1.06s/ba]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 3800/3800 [00:20<00:00, 186.52ex/s]\n100%|| 4/4 [00:04<00:00,  1.03s/ba]\n100%|| 3800/3800 [00:20<00:00, 187.87ex/s]\n100%|| 4/4 [00:03<00:00,  1.01ba/s]\nContinual training\n\n\nTASK  dbpedia_14\n20\n100%|| 3125/3125 [17:14<00:00,  3.02it/s]\nUsing cls_idx 440\nDownloading builder script: 4.21kB [00:00, 4.11MB/s]                            \n100%|| 4375/4375 [10:42<00:00,  6.81it/s]\ndbpedia_14  result =  0.9899714285714286\n21\n100%|| 3125/3125 [17:13<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:42<00:00,  6.81it/s]\ndbpedia_14  result =  0.9896\n22\n100%|| 3125/3125 [17:14<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:43<00:00,  6.80it/s]\ndbpedia_14  result =  0.9897428571428571\n23\n100%|| 3125/3125 [17:15<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:44<00:00,  6.78it/s]\ndbpedia_14  result =  0.9899714285714286\n24\n100%|| 3125/3125 [17:16<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:43<00:00,  6.80it/s]\ndbpedia_14  result =  0.9895428571428572\n25\n100%|| 3125/3125 [17:17<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:43<00:00,  6.80it/s]\ndbpedia_14  result =  0.9894\n26\n100%|| 3125/3125 [17:17<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:43<00:00,  6.80it/s]\ndbpedia_14  result =  0.989\n27\n100%|| 3125/3125 [17:17<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:44<00:00,  6.79it/s]\ndbpedia_14  result =  0.9896285714285714\n28\n100%|| 3125/3125 [17:15<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:41<00:00,  6.82it/s]\ndbpedia_14  result =  0.9894285714285714\n29\n100%|| 3125/3125 [17:15<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:41<00:00,  6.82it/s]\ndbpedia_14  result =  0.9899714285714286\n30\n100%|| 3125/3125 [17:15<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:40<00:00,  6.83it/s]\ndbpedia_14  result =  0.9901714285714286\n31\n100%|| 3125/3125 [17:15<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:41<00:00,  6.82it/s]\ndbpedia_14  result =  0.9897428571428571\n32\n100%|| 3125/3125 [17:14<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:41<00:00,  6.83it/s]\ndbpedia_14  result =  0.9895428571428572\n33\n100%|| 3125/3125 [17:14<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:40<00:00,  6.83it/s]\ndbpedia_14  result =  0.9898\n34\n100%|| 3125/3125 [17:14<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:42<00:00,  6.81it/s]\ndbpedia_14  result =  0.9903142857142857\n35\n100%|| 3125/3125 [17:14<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:41<00:00,  6.82it/s]\ndbpedia_14  result =  0.9891428571428571\n36\n100%|| 3125/3125 [17:15<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:41<00:00,  6.82it/s]\ndbpedia_14  result =  0.9896\n37\n100%|| 3125/3125 [17:16<00:00,  3.01it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:42<00:00,  6.81it/s]\ndbpedia_14  result =  0.9897714285714285\n38\n100%|| 3125/3125 [17:16<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:42<00:00,  6.81it/s]\ndbpedia_14  result =  0.9892571428571428\n39\n100%|| 3125/3125 [17:14<00:00,  3.02it/s]\nUsing cls_idx 440\n100%|| 4375/4375 [10:39<00:00,  6.84it/s]\ndbpedia_14  result =  0.9892285714285715\nUsing cls_idx 380\n100%|| 3125/3125 [06:18<00:00,  8.26it/s]\nyelp_review_full  result =  0.60316\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.78it/s]\nyahoo_answers_topics  result =  0.792850864342221\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.26it/s]\namazon  result =  0.5776315789473684\nUsing cls_idx 440\n100%|| 4375/4375 [10:41<00:00,  6.82it/s]\ndbpedia_14  result =  0.9896857142857143\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.69it/s]\nag_news  result =  0.25026315789473685\n\n\nTASK  ag_news\n0\n100%|| 3125/3125 [17:36<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.68it/s]\nag_news  result =  0.8986842105263158\nNEW BEST MODEL acc= 0.8986842105263158\n1\n100%|| 3125/3125 [17:35<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:10<00:00,  6.70it/s]\nag_news  result =  0.9010526315789473\nNEW BEST MODEL acc= 0.9010526315789473\n2\n100%|| 3125/3125 [17:34<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:10<00:00,  6.70it/s]\nag_news  result =  0.8968421052631579\n3\n  4%|                                      | 133/3125 [00:45<16:49,  2.96it/s]^C\n  4%|                                      | 133/3125 [00:45<17:00,  2.93it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/train_cl2.py\", line 330, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/train_cl2.py\", line 46, in main\n    results_dict = CL_class.continual_training(num_epochs=args.num_epochs,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/continual_learning_utils.py\", line 485, in continual_training\n    val_scores = self.train_on_one_task(i, task,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/continual_learning_utils.py\", line 412, in train_on_one_task\n    self.trainer.pass_batch_with_prompt(batch, task, self.device,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/model_utils.py\", line 424, in pass_batch_with_prompt\n    model.bert.embeddings.word_embeddings.weight[:k1] = torch.from_numpy(self.saved_embs[:k1]) # restore all emb except curr task\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_cl2.py --task_list yelp_review_full yahoo_answers_topics amazon dbpedia_14 ag_news \\\n--prefix_MLP residual_MLP2 --lr 1e-4 --num_epochs 40 --freeze_weights 1 --prefix_len 20 \\\n--early_stopping 1 --seq_len 380 --prompt_tuning 1 \\\n--save_name BERT_order_2 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-22T06:58:29.510681Z","iopub.execute_input":"2023-11-22T06:58:29.511056Z","iopub.status.idle":"2023-11-22T12:48:47.222729Z","shell.execute_reply.started":"2023-11-22T06:58:29.511022Z","shell.execute_reply":"2023-11-22T12:48:47.221605Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Max repeats =  0\nUsing MLP reparametrization with bottleneck =  800\nDownloading: 100%|| 28.0/28.0 [00:00<00:00, 20.8kB/s]\nDownloading: 100%|| 570/570 [00:00<00:00, 417kB/s]\nDownloading: 100%|| 226k/226k [00:00<00:00, 7.11MB/s]\nDownloading: 100%|| 455k/455k [00:00<00:00, 2.30MB/s]\nDownloading: 100%|| 420M/420M [00:07<00:00, 60.0MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertAdapterModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nyelp_review_full\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]']\nDownloading builder script: 4.41kB [00:00, 2.50MB/s]                            \nDownloading metadata: 2.04kB [00:00, 1.29MB/s]                                  \nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\nDownloading data: 100%|| 196M/196M [00:05<00:00, 33.2MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\nParameter 'function'=<function Dataset.prepare_dataset.<locals>.<lambda> at 0x7e815f750f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n100%|| 25000/25000 [00:53<00:00, 466.39ex/s]\n100%|| 25/25 [00:23<00:00,  1.05ba/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 25000/25000 [00:53<00:00, 469.38ex/s]\n100%|| 25/25 [00:23<00:00,  1.05ba/s]\n100%|| 25000/25000 [00:53<00:00, 466.67ex/s]\n100%|| 25/25 [00:23<00:00,  1.05ba/s]\nyahoo_answers_topics\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]']\nDownloading builder script: 3.60kB [00:00, 1.74MB/s]                            \nDownloading metadata: 1.88kB [00:00, 932kB/s]                                   \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:10<00:00, 29.3MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:22<00:00, 304.73ex/s]\n100%|| 25/25 [00:25<00:00,  1.02s/ba]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 10239/10239 [00:33<00:00, 309.92ex/s]\n100%|| 11/11 [00:10<00:00,  1.07ba/s]\n100%|| 10239/10239 [00:33<00:00, 309.31ex/s]\n100%|| 11/11 [00:10<00:00,  1.08ba/s]\namazon\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]']\n100%|| 25000/25000 [01:36<00:00, 258.91ex/s]\n100%|| 25/25 [00:25<00:00,  1.03s/ba]\n100%|| 3800/3800 [00:14<00:00, 263.58ex/s]\n100%|| 4/4 [00:03<00:00,  1.02ba/s]\n100%|| 3800/3800 [00:14<00:00, 259.98ex/s]\n100%|| 4/4 [00:03<00:00,  1.02ba/s]\ndbpedia_14\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]']\nDownloading builder script: 5.22kB [00:00, 2.52MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.21MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:01<00:00, 37.9MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:58<00:00, 211.57ex/s]\n100%|| 25/25 [00:26<00:00,  1.08s/ba]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 35000/35000 [02:43<00:00, 214.32ex/s]\n100%|| 35/35 [00:38<00:00,  1.10s/ba]\n100%|| 35000/35000 [02:42<00:00, 215.55ex/s]\n100%|| 35/35 [00:38<00:00,  1.09s/ba]\nag_news\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]', '[PRE4_1]', '[PRE4_2]', '[PRE4_3]', '[PRE4_4]', '[PRE4_5]', '[PRE4_6]', '[PRE4_7]', '[PRE4_8]', '[PRE4_9]', '[PRE4_10]', '[PRE4_11]', '[PRE4_12]', '[PRE4_13]', '[PRE4_14]', '[PRE4_15]', '[PRE4_16]', '[PRE4_17]', '[PRE4_18]', '[PRE4_19]', '[PRE4_20]']\nDownloading builder script: 4.06kB [00:00, 2.15MB/s]                            \nDownloading metadata: 2.65kB [00:00, 1.29MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 61.0MB/s]                                      \nDownloading data: 1.86MB [00:00, 40.7MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|| 25000/25000 [02:17<00:00, 182.01ex/s]\n100%|| 25/25 [00:27<00:00,  1.11s/ba]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 3800/3800 [00:20<00:00, 181.24ex/s]\n100%|| 4/4 [00:04<00:00,  1.06s/ba]\n100%|| 3800/3800 [00:20<00:00, 182.94ex/s]\n100%|| 4/4 [00:04<00:00,  1.06s/ba]\nContinual training\n\n\nTASK  ag_news\n3\n100%|| 3125/3125 [17:39<00:00,  2.95it/s]\nUsing cls_idx 460\nDownloading builder script: 4.21kB [00:00, 1.88MB/s]                            \n100%|| 475/475 [01:11<00:00,  6.68it/s]\nag_news  result =  0.9055263157894737\nNEW BEST MODEL acc= 0.9055263157894737\n4\n100%|| 3125/3125 [17:38<00:00,  2.95it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.68it/s]\nag_news  result =  0.9063157894736842\nNEW BEST MODEL acc= 0.9063157894736842\n5\n100%|| 3125/3125 [17:39<00:00,  2.95it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.68it/s]\nag_news  result =  0.91\nNEW BEST MODEL acc= 0.91\n6\n100%|| 3125/3125 [17:40<00:00,  2.95it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.67it/s]\nag_news  result =  0.9092105263157895\n7\n100%|| 3125/3125 [17:40<00:00,  2.95it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.67it/s]\nag_news  result =  0.9094736842105263\n8\n100%|| 3125/3125 [17:40<00:00,  2.95it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.66it/s]\nag_news  result =  0.9073684210526316\n9\n100%|| 3125/3125 [17:41<00:00,  2.94it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.63it/s]\nag_news  result =  0.9118421052631579\nNEW BEST MODEL acc= 0.9118421052631579\n10\n100%|| 3125/3125 [17:41<00:00,  2.94it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.66it/s]\nag_news  result =  0.9097368421052632\n11\n100%|| 3125/3125 [17:41<00:00,  2.94it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.65it/s]\nag_news  result =  0.9092105263157895\n12\n100%|| 3125/3125 [17:42<00:00,  2.94it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.65it/s]\nag_news  result =  0.9110526315789473\n13\n100%|| 3125/3125 [17:42<00:00,  2.94it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.65it/s]\nag_news  result =  0.9128947368421053\nNEW BEST MODEL acc= 0.9128947368421053\n14\n100%|| 3125/3125 [17:42<00:00,  2.94it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.64it/s]\nag_news  result =  0.9142105263157895\nNEW BEST MODEL acc= 0.9142105263157895\n15\n100%|| 3125/3125 [17:41<00:00,  2.94it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.68it/s]\nag_news  result =  0.9128947368421053\n16\n100%|| 3125/3125 [17:41<00:00,  2.94it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.65it/s]\nag_news  result =  0.9121052631578948\n17\n100%|| 3125/3125 [17:42<00:00,  2.94it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.65it/s]\nag_news  result =  0.9131578947368421\n18\n100%|| 3125/3125 [17:43<00:00,  2.94it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.65it/s]\nag_news  result =  0.9128947368421053\n19\n100%|| 3125/3125 [17:42<00:00,  2.94it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.65it/s]\nag_news  result =  0.9147368421052632\nNEW BEST MODEL acc= 0.9147368421052632\n20\n  5%|                                      | 143/3125 [00:48<16:57,  2.93it/s]^C\n  5%|                                      | 143/3125 [00:48<16:59,  2.92it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/train_cl2.py\", line 330, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/train_cl2.py\", line 46, in main\n    results_dict = CL_class.continual_training(num_epochs=args.num_epochs,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/continual_learning_utils.py\", line 485, in continual_training\n    val_scores = self.train_on_one_task(i, task,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/continual_learning_utils.py\", line 412, in train_on_one_task\n    self.trainer.pass_batch_with_prompt(batch, task, self.device,\n  File \"/kaggle/working/ProgressivePrompts/BERT_codebase/model_utils.py\", line 407, in pass_batch_with_prompt\n    loss.backward()\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/_tensor.py\", line 307, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 154, in backward\n    Variable._execution_engine.run_backward(\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_cl2.py --task_list yelp_review_full yahoo_answers_topics amazon dbpedia_14 ag_news \\\n--prefix_MLP residual_MLP2 --lr 1e-4 --num_epochs 40 --freeze_weights 1 --prefix_len 20 \\\n--early_stopping 1 --seq_len 380 --prompt_tuning 1 \\\n--save_name BERT_order_2 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-22T13:39:00.269468Z","iopub.execute_input":"2023-11-22T13:39:00.269716Z","iopub.status.idle":"2023-11-22T21:06:32.728765Z","shell.execute_reply.started":"2023-11-22T13:39:00.269694Z","shell.execute_reply":"2023-11-22T21:06:32.727614Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Max repeats =  0\nUsing MLP reparametrization with bottleneck =  800\nDownloading: 100%|| 28.0/28.0 [00:00<00:00, 32.5kB/s]\nDownloading: 100%|| 570/570 [00:00<00:00, 559kB/s]\nDownloading: 100%|| 226k/226k [00:00<00:00, 2.77MB/s]\nDownloading: 100%|| 455k/455k [00:00<00:00, 5.64MB/s]\nDownloading: 100%|| 420M/420M [00:07<00:00, 57.6MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertAdapterModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nyelp_review_full\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]']\nDownloading builder script: 4.41kB [00:00, 3.30MB/s]                            \nDownloading metadata: 2.04kB [00:00, 1.27MB/s]                                  \nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\nDownloading data: 100%|| 196M/196M [00:07<00:00, 25.1MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\nParameter 'function'=<function Dataset.prepare_dataset.<locals>.<lambda> at 0x7e7992f94e50> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n100%|| 25000/25000 [00:48<00:00, 510.93ex/s]\n100%|| 25/25 [00:22<00:00,  1.10ba/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 25000/25000 [00:48<00:00, 513.60ex/s]\n100%|| 25/25 [00:22<00:00,  1.11ba/s]\n100%|| 25000/25000 [00:48<00:00, 513.70ex/s]\n100%|| 25/25 [00:22<00:00,  1.11ba/s]\nyahoo_answers_topics\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]']\nDownloading builder script: 3.60kB [00:00, 1.61MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.54MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:14<00:00, 22.7MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:13<00:00, 340.19ex/s]\n100%|| 25/25 [00:23<00:00,  1.06ba/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 10239/10239 [00:30<00:00, 336.93ex/s]\n100%|| 11/11 [00:09<00:00,  1.12ba/s]\n100%|| 10239/10239 [00:30<00:00, 338.14ex/s]\n100%|| 11/11 [00:09<00:00,  1.15ba/s]\namazon\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]']\n100%|| 25000/25000 [01:29<00:00, 279.78ex/s]\n100%|| 25/25 [00:24<00:00,  1.02ba/s]\n100%|| 3800/3800 [00:13<00:00, 286.91ex/s]\n100%|| 4/4 [00:03<00:00,  1.08ba/s]\n100%|| 3800/3800 [00:13<00:00, 284.27ex/s]\n100%|| 4/4 [00:03<00:00,  1.11ba/s]\ndbpedia_14\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]']\nDownloading builder script: 5.22kB [00:00, 3.68MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.84MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:02<00:00, 23.4MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n100%|| 25000/25000 [01:49<00:00, 228.50ex/s]\n100%|| 25/25 [00:25<00:00,  1.02s/ba]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 35000/35000 [02:32<00:00, 229.01ex/s]\n100%|| 35/35 [00:35<00:00,  1.03s/ba]\n100%|| 35000/35000 [02:32<00:00, 229.18ex/s]\n100%|| 35/35 [00:35<00:00,  1.03s/ba]\nag_news\n['[PRE0_1]', '[PRE0_2]', '[PRE0_3]', '[PRE0_4]', '[PRE0_5]', '[PRE0_6]', '[PRE0_7]', '[PRE0_8]', '[PRE0_9]', '[PRE0_10]', '[PRE0_11]', '[PRE0_12]', '[PRE0_13]', '[PRE0_14]', '[PRE0_15]', '[PRE0_16]', '[PRE0_17]', '[PRE0_18]', '[PRE0_19]', '[PRE0_20]', '[PRE1_1]', '[PRE1_2]', '[PRE1_3]', '[PRE1_4]', '[PRE1_5]', '[PRE1_6]', '[PRE1_7]', '[PRE1_8]', '[PRE1_9]', '[PRE1_10]', '[PRE1_11]', '[PRE1_12]', '[PRE1_13]', '[PRE1_14]', '[PRE1_15]', '[PRE1_16]', '[PRE1_17]', '[PRE1_18]', '[PRE1_19]', '[PRE1_20]', '[PRE2_1]', '[PRE2_2]', '[PRE2_3]', '[PRE2_4]', '[PRE2_5]', '[PRE2_6]', '[PRE2_7]', '[PRE2_8]', '[PRE2_9]', '[PRE2_10]', '[PRE2_11]', '[PRE2_12]', '[PRE2_13]', '[PRE2_14]', '[PRE2_15]', '[PRE2_16]', '[PRE2_17]', '[PRE2_18]', '[PRE2_19]', '[PRE2_20]', '[PRE3_1]', '[PRE3_2]', '[PRE3_3]', '[PRE3_4]', '[PRE3_5]', '[PRE3_6]', '[PRE3_7]', '[PRE3_8]', '[PRE3_9]', '[PRE3_10]', '[PRE3_11]', '[PRE3_12]', '[PRE3_13]', '[PRE3_14]', '[PRE3_15]', '[PRE3_16]', '[PRE3_17]', '[PRE3_18]', '[PRE3_19]', '[PRE3_20]', '[PRE4_1]', '[PRE4_2]', '[PRE4_3]', '[PRE4_4]', '[PRE4_5]', '[PRE4_6]', '[PRE4_7]', '[PRE4_8]', '[PRE4_9]', '[PRE4_10]', '[PRE4_11]', '[PRE4_12]', '[PRE4_13]', '[PRE4_14]', '[PRE4_15]', '[PRE4_16]', '[PRE4_17]', '[PRE4_18]', '[PRE4_19]', '[PRE4_20]']\nDownloading builder script: 4.06kB [00:00, 3.24MB/s]                            \nDownloading metadata: 2.65kB [00:00, 1.61MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 60.5MB/s]                                      \nDownloading data: 1.86MB [00:00, 49.2MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|| 25000/25000 [02:10<00:00, 191.90ex/s]\n100%|| 25/25 [00:26<00:00,  1.06s/ba]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 3800/3800 [00:19<00:00, 195.69ex/s]\n100%|| 4/4 [00:04<00:00,  1.00s/ba]\n100%|| 3800/3800 [00:19<00:00, 194.81ex/s]\n100%|| 4/4 [00:03<00:00,  1.00ba/s]\nContinual training\n\n\nTASK  ag_news\n20\n100%|| 3125/3125 [17:35<00:00,  2.96it/s]\nUsing cls_idx 460\nDownloading builder script: 4.21kB [00:00, 3.66MB/s]                            \n100%|| 475/475 [01:10<00:00,  6.71it/s]\nag_news  result =  0.9134210526315789\n21\n100%|| 3125/3125 [17:34<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:10<00:00,  6.71it/s]\nag_news  result =  0.9121052631578948\n22\n100%|| 3125/3125 [17:35<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.68it/s]\nag_news  result =  0.9163157894736842\nNEW BEST MODEL acc= 0.9163157894736842\n23\n100%|| 3125/3125 [17:34<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:10<00:00,  6.69it/s]\nag_news  result =  0.911578947368421\n24\n100%|| 3125/3125 [17:35<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.67it/s]\nag_news  result =  0.9139473684210526\n25\n100%|| 3125/3125 [17:35<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.67it/s]\nag_news  result =  0.916578947368421\nNEW BEST MODEL acc= 0.916578947368421\n26\n100%|| 3125/3125 [17:37<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.66it/s]\nag_news  result =  0.9194736842105263\nNEW BEST MODEL acc= 0.9194736842105263\n27\n100%|| 3125/3125 [17:37<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.68it/s]\nag_news  result =  0.9171052631578948\n28\n100%|| 3125/3125 [17:37<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.67it/s]\nag_news  result =  0.9197368421052632\nNEW BEST MODEL acc= 0.9197368421052632\n29\n100%|| 3125/3125 [17:36<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.67it/s]\nag_news  result =  0.9155263157894736\n30\n100%|| 3125/3125 [17:36<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:10<00:00,  6.70it/s]\nag_news  result =  0.9202631578947369\nNEW BEST MODEL acc= 0.9202631578947369\n31\n100%|| 3125/3125 [17:37<00:00,  2.95it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.65it/s]\nag_news  result =  0.9163157894736842\n32\n100%|| 3125/3125 [17:38<00:00,  2.95it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.65it/s]\nag_news  result =  0.9160526315789473\n33\n100%|| 3125/3125 [17:38<00:00,  2.95it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.67it/s]\nag_news  result =  0.9186842105263158\n34\n100%|| 3125/3125 [17:37<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:10<00:00,  6.70it/s]\nag_news  result =  0.9160526315789473\n35\n100%|| 3125/3125 [17:36<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:10<00:00,  6.70it/s]\nag_news  result =  0.9155263157894736\n36\n100%|| 3125/3125 [17:36<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.68it/s]\nag_news  result =  0.9213157894736842\nNEW BEST MODEL acc= 0.9213157894736842\n37\n100%|| 3125/3125 [17:36<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:10<00:00,  6.70it/s]\nag_news  result =  0.9210526315789473\n38\n100%|| 3125/3125 [17:35<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.68it/s]\nag_news  result =  0.9189473684210526\n39\n100%|| 3125/3125 [17:36<00:00,  2.96it/s]\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.69it/s]\nag_news  result =  0.9186842105263158\nUsing cls_idx 380\n100%|| 3125/3125 [06:19<00:00,  8.24it/s]\nyelp_review_full  result =  0.60316\nUsing cls_idx 400\n100%|| 1280/1280 [02:43<00:00,  7.81it/s]\nyahoo_answers_topics  result =  0.792850864342221\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.25it/s]\namazon  result =  0.5776315789473684\nUsing cls_idx 440\n100%|| 4375/4375 [10:41<00:00,  6.82it/s]\ndbpedia_14  result =  0.9896857142857143\nUsing cls_idx 460\n100%|| 475/475 [01:11<00:00,  6.69it/s]\nag_news  result =  0.9213157894736842\nUsing cls_idx 380\n100%|| 3125/3125 [06:18<00:00,  8.26it/s]\nyelp_review_full  result =  0.60316\nUsing cls_idx 400\n100%|| 1280/1280 [02:44<00:00,  7.77it/s]\nyahoo_answers_topics  result =  0.792850864342221\nUsing cls_idx 420\n100%|| 475/475 [01:05<00:00,  7.27it/s]\namazon  result =  0.5776315789473684\nUsing cls_idx 440\n100%|| 4375/4375 [10:40<00:00,  6.83it/s]\ndbpedia_14  result =  0.9896857142857143\nUsing cls_idx 460\n100%|| 475/475 [01:10<00:00,  6.71it/s]\nag_news  result =  0.9213157894736842\n","output_type":"stream"}]},{"cell_type":"code","source":"%mv /kaggle/working/ProgressivePrompts/BERT_codebase/save_checkpoint/current_checkpoint.pt /kaggle/working/\n%cd /kaggle/working\nfrom IPython.display import FileLink\nFileLink(r'current_checkpoint.pt')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T12:48:53.444795Z","iopub.execute_input":"2023-11-22T12:48:53.445149Z","iopub.status.idle":"2023-11-22T12:48:54.395254Z","shell.execute_reply.started":"2023-11-22T12:48:53.445115Z","shell.execute_reply":"2023-11-22T12:48:54.394108Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/current_checkpoint.pt","text/html":"<a href='current_checkpoint.pt' target='_blank'>current_checkpoint.pt</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nres = np.load('/kaggle/working/ProgressivePrompts/BERT_codebase/results/BERT_order_2/results_dict.npy', allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T21:11:33.698167Z","iopub.execute_input":"2023-11-22T21:11:33.698541Z","iopub.status.idle":"2023-11-22T21:11:33.703984Z","shell.execute_reply.started":"2023-11-22T21:11:33.698511Z","shell.execute_reply":"2023-11-22T21:11:33.702986Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"res","metadata":{"execution":{"iopub.status.busy":"2023-11-22T21:11:36.857163Z","iopub.execute_input":"2023-11-22T21:11:36.857906Z","iopub.status.idle":"2023-11-22T21:11:36.863702Z","shell.execute_reply.started":"2023-11-22T21:11:36.857873Z","shell.execute_reply":"2023-11-22T21:11:36.862784Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array({0: {'yelp_review_full': [0.56852, 0.57696, 0.58428, 0.58576, 0.58708, 0.58952, 0.59072, 0.594, 0.5932, 0.5968, 0.59772, 0.596, 0.59764, 0.60144, 0.6008, 0.60232, 0.60364, 0.60312, 0.6048, 0.60304, 0.6046, 0.60412, 0.60524, 0.60096, 0.60588, 0.60572, 0.60696, 0.60624, 0.60708, 0.60508, 0.60916, 0.608, 0.60836, 0.60604, 0.60784, 0.60852, 0.6088, 0.60952, 0.6068, 0.6072], 'yahoo_answers_topics': [], 'amazon': [], 'dbpedia_14': [], 'ag_news': []}, 'test': {'yelp_review_full': [0.60316], 'yahoo_answers_topics': [0.792850864342221], 'amazon': [0.5776315789473684], 'dbpedia_14': [0.9896857142857143], 'ag_news': [0.9213157894736842]}, 1: {'yelp_review_full': [], 'yahoo_answers_topics': [0.7687274147866002, 0.7781033304033597, 0.7843539408145327, 0.7864049223556988, 0.7882605723215158, 0.7873815802324445, 0.78728391444477, 0.7868932512940717, 0.7889442328352378, 0.7911905459517531, 0.7896278933489599, 0.7936321906436176, 0.7917765406778006, 0.7894325617736107, 0.7930461959175701, 0.789823224924309, 0.7950971774587362, 0.7946088485203633, 0.7944135169450142, 0.793534524855943, 0.7934368590682684, 0.7938275222189667, 0.7919718722531497, 0.7918742064654751, 0.7903115538626819, 0.7922648696161735, 0.7932415274929192, 0.7914835433147769, 0.7927531985545463, 0.7918742064654751, 0.792362535403848, 0.7907022170133802, 0.791678874890126, 0.7918742064654751, 0.7897255591366344, 0.792850864342221, 0.7887489012598886, 0.7897255591366344, 0.7880652407461666, 0.7893348959859361], 'amazon': [], 'dbpedia_14': [], 'ag_news': []}, 2: {'yelp_review_full': [], 'yahoo_answers_topics': [], 'amazon': [0.5363157894736842, 0.5452631578947369, 0.5565789473684211, 0.5610526315789474, 0.5726315789473684, 0.5723684210526315, 0.5689473684210526, 0.5760526315789474, 0.5747368421052632, 0.5794736842105264, 0.5752631578947368, 0.5765789473684211, 0.5760526315789474, 0.5831578947368421, 0.5768421052631579, 0.5807894736842105, 0.5821052631578948, 0.5847368421052631, 0.5805263157894737, 0.5797368421052631, 0.5760526315789474, 0.5857894736842105, 0.5797368421052631, 0.58, 0.583421052631579, 0.5776315789473684, 0.5813157894736842, 0.5802631578947368, 0.5794736842105264, 0.5815789473684211, 0.5786842105263158, 0.5823684210526315, 0.5773684210526315, 0.5768421052631579, 0.5739473684210527, 0.5773684210526315, 0.5815789473684211, 0.5826315789473684, 0.583421052631579, 0.583421052631579], 'dbpedia_14': [], 'ag_news': []}, 3: {'yelp_review_full': [], 'yahoo_answers_topics': [], 'amazon': [], 'dbpedia_14': [0.9869428571428571, 0.9895714285714285, 0.9900857142857142, 0.9897428571428571, 0.9900857142857142, 0.9902285714285715, 0.9898857142857143, 0.9898857142857143, 0.9903142857142857, 0.9902857142857143, 0.9899142857142857, 0.9891428571428571, 0.9897428571428571, 0.9896857142857143, 0.9901428571428571, 0.9901714285714286, 0.99, 0.9900571428571429, 0.9898571428571429, 0.9898, 0.9899714285714286, 0.9896, 0.9897428571428571, 0.9899714285714286, 0.9895428571428572, 0.9894, 0.989, 0.9896285714285714, 0.9894285714285714, 0.9899714285714286, 0.9901714285714286, 0.9897428571428571, 0.9895428571428572, 0.9898, 0.9903142857142857, 0.9891428571428571, 0.9896, 0.9897714285714285, 0.9892571428571428, 0.9892285714285715], 'ag_news': []}, 4: {'yelp_review_full': [], 'yahoo_answers_topics': [], 'amazon': [], 'dbpedia_14': [], 'ag_news': [0.8986842105263158, 0.9010526315789473, 0.8968421052631579, 0.9055263157894737, 0.9063157894736842, 0.91, 0.9092105263157895, 0.9094736842105263, 0.9073684210526316, 0.9118421052631579, 0.9097368421052632, 0.9092105263157895, 0.9110526315789473, 0.9128947368421053, 0.9142105263157895, 0.9128947368421053, 0.9121052631578948, 0.9131578947368421, 0.9128947368421053, 0.9147368421052632, 0.9134210526315789, 0.9121052631578948, 0.9163157894736842, 0.911578947368421, 0.9139473684210526, 0.916578947368421, 0.9194736842105263, 0.9171052631578948, 0.9197368421052632, 0.9155263157894736, 0.9202631578947369, 0.9163157894736842, 0.9160526315789473, 0.9186842105263158, 0.9160526315789473, 0.9155263157894736, 0.9213157894736842, 0.9210526315789473, 0.9189473684210526, 0.9186842105263158]}},\n      dtype=object)"},"metadata":{}}]}]}