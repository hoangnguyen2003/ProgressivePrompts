{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6888317,"sourceType":"datasetVersion","datasetId":3910096,"isSourceIdPinned":true},{"sourceId":6990264,"sourceType":"datasetVersion","datasetId":4017745}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"mv /kaggle/input/progressive-prompts/ProgressivePrompts /kaggle/working/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-11-17T15:52:54.086426Z","iopub.execute_input":"2023-11-17T15:52:54.086824Z","iopub.status.idle":"2023-11-17T15:53:09.975239Z","shell.execute_reply.started":"2023-11-17T15:52:54.086795Z","shell.execute_reply":"2023-11-17T15:53:09.974016Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"mv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/environment.yaml': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/LICENSE': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/README.md': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/ag/readme.txt': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/ag/classes.txt': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/ag/train.csv': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/ag/test.csv': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/dbpedia/readme.txt': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/dbpedia/classes.txt': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/dbpedia/train.csv': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/dbpedia/test.csv': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/yahoo/readme.txt': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/yahoo/classes.txt': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/yahoo/train.csv': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/yahoo/test.csv': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/amazon/train.csv': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/amazon/test.csv': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/yelp_review_full/readme.txt': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/yelp_review_full/train.csv': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/datasets/src/data/yelp_review_full/test.csv': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/BERT_codebase/good_id_yahoo_test2.npy': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/BERT_codebase/train_soft_prompt.py': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/BERT_codebase/train_cl2.py': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/BERT_codebase/run_prog_prompts.sh': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/BERT_codebase/continual_learning_utils.py': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/BERT_codebase/good_id_yahoo_train2.npy': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/BERT_codebase/dataset_utils.py': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/BERT_codebase/continual_learning_one_head.py': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/BERT_codebase/model_utils.py': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/BERT_codebase/train_cl.py': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/README.md': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/T5_codebase/train_prompt.py': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/T5_codebase/good_id_yahoo_test.npy': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/T5_codebase/prompt_debug.ipynb': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/T5_codebase/train_t5_cl.py': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/T5_codebase/good_id_yahoo_train.npy': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/T5_codebase/t5_dataset.py': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/T5_codebase/t5_continual.py': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/images/test.png': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/images/illustration.png': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/config': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/info/exclude': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/refs/heads/main': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/refs/remotes/origin/HEAD': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/hooks/pre-merge-commit.sample': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/hooks/prepare-commit-msg.sample': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/hooks/update.sample': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/hooks/pre-push.sample': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/hooks/pre-rebase.sample': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/hooks/pre-applypatch.sample': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/hooks/push-to-checkout.sample': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/hooks/pre-commit.sample': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/hooks/commit-msg.sample': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/hooks/post-update.sample': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/hooks/pre-receive.sample': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/hooks/fsmonitor-watchman.sample': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/hooks/applypatch-msg.sample': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/packed-refs': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/HEAD': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/index': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/logs/refs/heads/main': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/logs/refs/remotes/origin/HEAD': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/logs/HEAD': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/objects/pack/pack-84de7d21714b70a831951b84e6d9e4c56f666412.pack': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/objects/pack/pack-84de7d21714b70a831951b84e6d9e4c56f666412.idx': Read-only file system\nmv: cannot remove '/kaggle/input/progressive-prompts/ProgressivePrompts/.git/description': Read-only file system\n","output_type":"stream"}]},{"cell_type":"code","source":"cd /kaggle/working/ProgressivePrompts/T5_codebase","metadata":{"execution":{"iopub.status.busy":"2023-11-17T15:53:09.977462Z","iopub.execute_input":"2023-11-17T15:53:09.977795Z","iopub.status.idle":"2023-11-17T15:53:09.984841Z","shell.execute_reply.started":"2023-11-17T15:53:09.977767Z","shell.execute_reply":"2023-11-17T15:53:09.983709Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/ProgressivePrompts/T5_codebase\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.mkdir(\"load_checkpoint\")","metadata":{"execution":{"iopub.status.busy":"2023-11-17T15:53:09.987386Z","iopub.execute_input":"2023-11-17T15:53:09.987730Z","iopub.status.idle":"2023-11-17T15:53:09.996385Z","shell.execute_reply.started":"2023-11-17T15:53:09.987703Z","shell.execute_reply":"2023-11-17T15:53:09.995357Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"mv /kaggle/input/t5-order-1/current_checkpoint.ptrom /kaggle/working/ProgressivePrompts/T5_codebase/load_checkpoint","metadata":{"execution":{"iopub.status.busy":"2023-11-17T15:53:09.997514Z","iopub.execute_input":"2023-11-17T15:53:09.997812Z","iopub.status.idle":"2023-11-17T15:53:35.738446Z","shell.execute_reply.started":"2023-11-17T15:53:09.997762Z","shell.execute_reply":"2023-11-17T15:53:35.737137Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"mv: cannot remove '/kaggle/input/t5-order-1/current_checkpoint.ptrom': Read-only file system\n","output_type":"stream"}]},{"cell_type":"code","source":"cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2023-11-17T15:53:35.740138Z","iopub.execute_input":"2023-11-17T15:53:35.740499Z","iopub.status.idle":"2023-11-17T15:53:35.748248Z","shell.execute_reply.started":"2023-11-17T15:53:35.740470Z","shell.execute_reply":"2023-11-17T15:53:35.747265Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"cd ProgressivePrompts","metadata":{"execution":{"iopub.status.busy":"2023-11-17T15:53:35.749723Z","iopub.execute_input":"2023-11-17T15:53:35.750025Z","iopub.status.idle":"2023-11-17T15:53:35.765995Z","shell.execute_reply.started":"2023-11-17T15:53:35.749997Z","shell.execute_reply":"2023-11-17T15:53:35.764888Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/working/ProgressivePrompts\n","output_type":"stream"}]},{"cell_type":"code","source":"conda env create -f environment.yaml","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-11-17T15:53:35.767748Z","iopub.execute_input":"2023-11-17T15:53:35.768243Z","iopub.status.idle":"2023-11-17T16:07:03.994624Z","shell.execute_reply.started":"2023-11-17T15:53:35.768185Z","shell.execute_reply":"2023-11-17T16:07:03.993066Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting package metadata (repodata.json): - WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.8.0.*, but conda is ignoring the .* and treating it as 1.8.0\nWARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\nWARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.9.0.*, but conda is ignoring the .* and treating it as 1.9.0\n\\ WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.6.0.*, but conda is ignoring the .* and treating it as 1.6.0\ndone\nSolving environment: done\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 23.7.3\n  latest version: 23.10.0\n\nPlease update conda by running\n\n    $ conda update -n base -c conda-forge conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=23.10.0\n\n\n\nDownloading and Extracting Packages\ngnutls-3.6.13        | 2.0 MB    |                                       |   0% \nstack_data-0.2.0     | 21 KB     |                                       |   0% \u001b[A\n\nlibsodium-1.0.18     | 387 KB    |                                       |   0% \u001b[A\u001b[A\n\n\npython-dateutil-2.8. | 241 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n\n\n\nnbformat-5.3.0       | 142 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\nmkl-service-2.4.0    | 61 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\nnest-asyncio-1.5.5   | 15 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\npickleshare-0.7.5    | 13 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nlerc-3.0             | 216 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\ntyping_extensions-4. | 27 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nqtpy-2.0.1           | 40 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nmarkupsafe-2.0.1     | 22 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\npandocfilters-1.5.0  | 11 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nncurses-6.3          | 1002 KB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-mutex-1.0    | 3 KB      |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndecorator-5.1.1      | 12 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npyzmq-22.3.0         | 509 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwidgetsnbextension-3 | 1.3 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndebugpy-1.5.1        | 2.0 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndbus-1.13.18         | 586 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngst-plugins-base-1.1 | 6.3 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython_abi-3.9       | 4 KB      |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngnutls-3.6.13        | 2.0 MB    | 2                                     |   1% [A\u001b[A\u001b[A\u001b[A\nstack_data-0.2.0     | 21 KB     | ###########################9          |  76% \u001b[A\n\nlibsodium-1.0.18     | 387 KB    | #5                                    |   4% \u001b[A\u001b[A\nstack_data-0.2.0     | 21 KB     | ##################################### | 100% \u001b[A\n\n\npython-dateutil-2.8. | 241 KB    | ##4                                   |   7% \u001b[A\u001b[A\u001b[A\n\n\n\n\nmkl-service-2.4.0    | 61 KB     | #########7                            |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\nnest-asyncio-1.5.5   | 15 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\npickleshare-0.7.5    | 13 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nlerc-3.0             | 216 KB    | ##7                                   |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\ntyping_extensions-4. | 27 KB     | #####################9                |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nnbformat-5.3.0       | 142 KB    | ####1                                 |  11% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nqtpy-2.0.1           | 40 KB     | ##############7                       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nncurses-6.3          | 1002 KB   | 5                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nmarkupsafe-2.0.1     | 22 KB     | ##########################3           |  71% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\npython-dateutil-2.8. | 241 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\npython-dateutil-2.8. | 241 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\npandocfilters-1.5.0  | 11 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-mutex-1.0    | 3 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\nmkl-service-2.4.0    | 61 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\nmkl-service-2.4.0    | 61 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\nnest-asyncio-1.5.5   | 15 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndecorator-5.1.1      | 12 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npyzmq-22.3.0         | 509 KB    | #1                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwidgetsnbextension-3 | 1.3 MB    | 4                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\nlibsodium-1.0.18     | 387 KB    | ##################################### | 100% \u001b[A\u001b[A\n\nlibsodium-1.0.18     | 387 KB    | ##################################### | 100% \u001b[A\u001b[A\n\n\n\n\n\n\npickleshare-0.7.5    | 13 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\ntyping_extensions-4. | 27 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndbus-1.13.18         | 586 KB    | #                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\ntyping_extensions-4. | 27 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngst-plugins-base-1.1 | 6.3 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython_abi-3.9       | 4 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nlerc-3.0             | 216 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nlerc-3.0             | 216 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngst-plugins-base-1.1 | 6.3 MB    | ###########2                          |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndebugpy-1.5.1        | 2.0 MB    | 2                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngst-plugins-base-1.1 | 6.3 MB    | ####################1                 |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndebugpy-1.5.1        | 2.0 MB    | ###########################2          |  74% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngst-plugins-base-1.1 | 6.3 MB    | ############################2         |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nqtpy-2.0.1           | 40 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nqtpy-2.0.1           | 40 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nmarkupsafe-2.0.1     | 22 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nmarkupsafe-2.0.1     | 22 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 2                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\npandocfilters-1.5.0  | 11 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 2                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 3                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nnbformat-5.3.0       | 142 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nnbformat-5.3.0       | 142 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-mutex-1.0    | 3 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 3                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndecorator-5.1.1      | 12 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 4                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 4                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 4                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 5                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 5                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 6                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 6                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 7                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 7                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 7                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 8                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 8                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 9                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | 9                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npyzmq-22.3.0         | 509 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npyzmq-22.3.0         | 509 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #1                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #1                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #2                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #2                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #2                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #4                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #4                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #4                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #5                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwidgetsnbextension-3 | 1.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwidgetsnbextension-3 | 1.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #5                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #5                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #6                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #6                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #6                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #7                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #7                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndbus-1.13.18         | 586 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndbus-1.13.18         | 586 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #8                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython_abi-3.9       | 4 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #8                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #8                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #9                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##                                    |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##1                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##1                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##2                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##2                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\ngnutls-3.6.13        | 2.0 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##3                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##3                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##4                                   |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##5                                   |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##5                                   |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##6                                   |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##6                                   |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##7                                   |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##7                                   |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##7                                   |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##8                                   |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##9                                   |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##9                                   |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##9                                   |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###                                   |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###                                   |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###1                                  |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###1                                  |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###2                                  |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###2                                  |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###3                                  |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###3                                  |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###4                                  |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###4                                  |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###4                                  |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###5                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###5                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###6                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###7                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndebugpy-1.5.1        | 2.0 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###7                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###8                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###8                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###9                                  |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###9                                  |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###9                                  |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####                                  |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####                                  |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####1                                 |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####1                                 |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nncurses-6.3          | 1002 KB   | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nncurses-6.3          | 1002 KB   | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####2                                 |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####2                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####3                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####3                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####4                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####4                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####5                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####5                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####5                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####6                                 |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngst-plugins-base-1.1 | 6.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngst-plugins-base-1.1 | 6.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####7                                 |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####7                                 |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####8                                 |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####8                                 |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####9                                 |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####                                 |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####                                 |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####                                 |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####1                                |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####1                                |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####2                                |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####2                                |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####3                                |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####3                                |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####4                                |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####4                                |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####5                                |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####5                                |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####6                                |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####6                                |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####7                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####8                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####8                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####9                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####9                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####9                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######1                               |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######1                               |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######2                               |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######2                               |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######3                               |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######3                               |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######4                               |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######4                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######5                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######5                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######6                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######7                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######7                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######8                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######8                               |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######9                               |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######                               |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######1                              |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######2                              |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######3                              |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######4                              |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######5                              |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######6                              |  21% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######7                              |  21% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######8                              |  21% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######9                              |  21% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########                              |  22% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########1                             |  22% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########2                             |  22% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########3                             |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########4                             |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########5                             |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########6                             |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########7                             |  24% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########8                             |  24% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########8                             |  24% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########9                             |  24% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########                             |  24% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########                             |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########1                            |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########2                            |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########2                            |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########3                            |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########4                            |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########4                            |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########5                            |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########5                            |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########6                            |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########7                            |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########8                            |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########8                            |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########9                            |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########                            |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########                            |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########1                           |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########1                           |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########3                           |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########4                           |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########5                           |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########5                           |  29% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########6                           |  29% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########7                           |  29% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########8                           |  29% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########9                           |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########                           |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########1                          |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########2                          |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########2                          |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########3                          |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########4                          |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########4                          |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########5                          |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########6                          |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########6                          |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########7                          |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########8                          |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########8                          |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########9                          |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########9                          |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############                          |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############1                         |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############2                         |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############2                         |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############3                         |  33% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############4                         |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############5                         |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############6                         |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############7                         |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############8                         |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############9                         |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############                         |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############                         |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############1                        |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############2                        |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############2                        |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############3                        |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############4                        |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############4                        |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############6                        |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############6                        |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############7                        |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############8                        |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############9                        |  38% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############9                        |  38% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############                        |  38% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############1                       |  38% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############2                       |  38% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############2                       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############3                       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############3                       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############4                       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############4                       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############5                       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############5                       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############6                       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############6                       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############7                       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############8                       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############9                       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############9                       |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############1                      |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############1                      |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############2                      |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############2                      |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############3                      |  42% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############4                      |  42% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############5                      |  42% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############6                      |  42% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############8                      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############9                      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################                      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################                      |  43% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################1                     |  44% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################2                     |  44% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################3                     |  44% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################4                     |  45% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################5                     |  45% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################7                     |  45% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################8                     |  45% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################9                     |  46% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################9                     |  46% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################                     |  46% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################1                    |  46% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################2                    |  47% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################3                    |  47% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################4                    |  47% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################5                    |  47% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################6                    |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################7                    |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################7                    |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################9                    |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################                    |  49% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################1                   |  49% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################2                   |  49% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################2                   |  49% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################3                   |  49% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################4                   |  50% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################5                   |  50% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################7                   |  51% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################8                   |  51% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################9                   |  51% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################9                   |  51% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################                   |  52% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################1                  |  52% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################2                  |  52% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################2                  |  52% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################3                  |  52% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################4                  |  53% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################5                  |  53% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################6                  |  53% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################8                  |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################9                  |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################                  |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################                  |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################1                 |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################2                 |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################2                 |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################3                 |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################3                 |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################4                 |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################5                 |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################6                 |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################6                 |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################6                 |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################7                 |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################8                 |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################9                 |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####################                 |  57% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####################                 |  57% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####################1                |  57% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####################1                |  57% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####################2                |  57% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####################3                |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####################3                |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####################4                |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####################5                |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####################5                |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####################6                |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####################8                |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #####################9                |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######################                |  60% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######################1               |  60% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######################2               |  60% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######################3               |  60% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######################4               |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######################5               |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######################5               |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######################6               |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######################6               |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######################8               |  62% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######################8               |  62% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ######################9               |  62% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######################               |  62% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######################1              |  62% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######################1              |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######################2              |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######################3              |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######################4              |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######################5              |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######################6              |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######################6              |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######################7              |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######################8              |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######################8              |  64% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #######################9              |  65% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########################              |  65% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########################1             |  65% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########################1             |  65% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########################2             |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########################3             |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########################4             |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########################4             |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########################5             |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########################6             |  67% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########################7             |  67% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########################8             |  67% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########################9             |  67% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ########################9             |  68% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########################             |  68% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########################             |  68% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########################2            |  68% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########################3            |  69% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########################4            |  69% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########################5            |  69% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########################7            |  69% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #########################8            |  70% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########################            |  70% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########################1           |  71% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########################2           |  71% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########################3           |  71% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########################4           |  72% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########################5           |  72% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########################7           |  72% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##########################8           |  73% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########################           |  73% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########################3          |  74% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########################5          |  74% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###########################7          |  75% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############################          |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############################3         |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############################5         |  77% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ############################8         |  78% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############################         |  79% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############################3        |  79% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############################5        |  80% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #############################7        |  80% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############################        |  81% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############################2       |  82% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############################5       |  82% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############################7       |  83% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##############################9       |  84% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############################2      |  84% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############################4      |  85% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############################6      |  86% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###############################9      |  86% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################################1     |  87% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################################4     |  88% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################################6     |  88% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ################################8     |  89% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################################1    |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################################3    |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################################5    |  91% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | #################################8    |  91% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################################    |  92% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################################2   |  93% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################################4   |  93% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################################7   |  94% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################################9   |  94% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################################1  |  95% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################################2  |  95% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################################4  |  96% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################################6  |  96% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################################7  |  97% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ###################################9  |  97% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################################1 |  98% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################################2 |  98% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################################3 |  98% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################################5 |  99% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ####################################7 |  99% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch-1.10.1       | 1.21 GB   | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \n                                                                                \u001b[A\n\n                                                                                \u001b[A\u001b[A\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: / By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n\ndone\nInstalling pip dependencies: \\ Ran pip subprocess with arguments:\n['/opt/conda/envs/nlp/bin/python', '-m', 'pip', 'install', '-U', '-r', '/kaggle/working/ProgressivePrompts/condaenv.ppb9logi.requirements.txt', '--exists-action=b']\nPip subprocess output:\n| Collecting adapter-transformers==3.0.1\n  Downloading adapter_transformers-3.0.1-py3-none-any.whl (3.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/3.9 MB 34.0 MB/s eta 0:00:00\nCollecting aiohttp==3.8.1\n  Downloading aiohttp-3.8.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 56.4 MB/s eta 0:00:00\nCollecting aiosignal==1.2.0\n  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\nCollecting async-timeout==4.0.2\n  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\nCollecting certifi==2022.6.15\n  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.2/160.2 kB 14.1 MB/s eta 0:00:00\nCollecting charset-normalizer==2.0.12\n  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\nCollecting click==8.1.3\n  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 kB 9.5 MB/s eta 0:00:00\nCollecting cycler==0.11.0\n  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\nCollecting data==0.4\n  Downloading data-0.4.tar.gz (7.0 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting datasets==2.3.2\n  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 28.0 MB/s eta 0:00:00\nCollecting dill==0.3.5.1\n  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.8/95.8 kB 9.0 MB/s eta 0:00:00\nCollecting filelock==3.7.1\n  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\nCollecting fonttools==4.33.3\n  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 930.9/930.9 kB 48.1 MB/s eta 0:00:00\nCollecting frozenlist==1.3.0\n  Downloading frozenlist-1.3.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 156.2/156.2 kB 13.7 MB/s eta 0:00:00\nCollecting fsspec==2022.5.0\n  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 13.6 MB/s eta 0:00:00\nCollecting funcsigs==1.0.2\n  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\nCollecting future==0.18.2\n  Downloading future-0.18.2.tar.gz (829 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 829.2/829.2 kB 47.6 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting huggingface-hub==0.7.0\n  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.2/86.2 kB 8.1 MB/s eta 0:00:00\nCollecting idna==3.3\n  Downloading idna-3.3-py3-none-any.whl (61 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 5.4 MB/s eta 0:00:00\nCollecting joblib==1.1.0\n  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.0/307.0 kB 24.9 MB/s eta 0:00:00\nCollecting kiwisolver==1.4.3\n  Downloading kiwisolver-1.4.3-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 59.1 MB/s eta 0:00:00\nCollecting latex==0.7.0\n  Downloading latex-0.7.0.tar.gz (6.5 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting matplotlib==3.5.2\n  Downloading matplotlib-3.5.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.2/11.2 MB 76.9 MB/s eta 0:00:00\nCollecting multidict==6.0.2\n  Downloading multidict-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 kB 9.2 MB/s eta 0:00:00\nCollecting multiprocess==0.70.13\n  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.3/132.3 kB 9.0 MB/s eta 0:00:00\nCollecting nltk==3.7\n  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 61.5 MB/s eta 0:00:00\nCollecting pandas==1.4.2\n  Downloading pandas-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 20.1 MB/s eta 0:00:00\nCollecting pyarrow==8.0.0\n  Downloading pyarrow-8.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.4/29.4 MB 42.2 MB/s eta 0:00:00\nRequirement already satisfied: pyparsing==3.0.9 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from -r /kaggle/working/ProgressivePrompts/condaenv.ppb9logi.requirements.txt (line 29)) (3.0.9)\nCollecting pytorch-ranger==0.1.1\n  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\nCollecting pytz==2022.1\n  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 503.5/503.5 kB 32.3 MB/s eta 0:00:00\nCollecting pyyaml==6.0\n  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 661.8/661.8 kB 39.9 MB/s eta 0:00:00\nCollecting regex==2022.6.2\n  Downloading regex-2022.6.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 763.2/763.2 kB 42.1 MB/s eta 0:00:00\nCollecting requests==2.28.0\n  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 5.9 MB/s eta 0:00:00\nCollecting responses==0.18.0\n  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\nCollecting sacremoses==0.0.53\n  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 880.6/880.6 kB 45.5 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting scikit-learn==1.1.1\n  Downloading scikit_learn-1.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 30.8/30.8 MB 37.8 MB/s eta 0:00:00\nCollecting scipy==1.8.1\n  Downloading scipy-1.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.2/42.2 MB 29.0 MB/s eta 0:00:00\nCollecting seaborn==0.12.0\n  Downloading seaborn-0.12.0-py3-none-any.whl (285 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 285.1/285.1 kB 25.6 MB/s eta 0:00:00\nCollecting sentencepiece==0.1.96\n  Downloading sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 53.8 MB/s eta 0:00:00\nCollecting shutilwhich==1.1.0\n  Downloading shutilwhich-1.1.0.tar.gz (2.3 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting sklearn==0.0\n  Downloading sklearn-0.0.tar.gz (1.1 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting tempdir==0.7.1\n  Downloading tempdir-0.7.1.tar.gz (5.9 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting threadpoolctl==3.1.0\n  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\nCollecting tokenizers==0.12.1\n  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 71.4 MB/s eta 0:00:00\nCollecting torch-optimizer==0.3.0\n  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.9/61.9 kB 2.4 MB/s eta 0:00:00\nCollecting tqdm==4.64.0\n  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.4/78.4 kB 8.2 MB/s eta 0:00:00\nCollecting transformers==4.20.0\n  Downloading transformers-4.20.0-py3-none-any.whl (4.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 84.8 MB/s eta 0:00:00\nCollecting urllib3==1.26.9\n  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.0/139.0 kB 12.7 MB/s eta 0:00:00\nCollecting xxhash==3.0.0\n  Downloading xxhash-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.2/211.2 kB 20.6 MB/s eta 0:00:00\nCollecting yarl==1.7.2\n  Downloading yarl-1.7.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (304 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 304.5/304.5 kB 24.1 MB/s eta 0:00:00\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from adapter-transformers==3.0.1->-r /kaggle/working/ProgressivePrompts/condaenv.ppb9logi.requirements.txt (line 1)) (21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from adapter-transformers==3.0.1->-r /kaggle/working/ProgressivePrompts/condaenv.ppb9logi.requirements.txt (line 1)) (1.22.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from aiohttp==3.8.1->-r /kaggle/working/ProgressivePrompts/condaenv.ppb9logi.requirements.txt (line 2)) (21.4.0)\nRequirement already satisfied: six in /opt/conda/envs/nlp/lib/python3.9/site-packages (from data==0.4->-r /kaggle/working/ProgressivePrompts/condaenv.ppb9logi.requirements.txt (line 9)) (1.16.0)\nRequirement already satisfied: decorator in /opt/conda/envs/nlp/lib/python3.9/site-packages (from data==0.4->-r /kaggle/working/ProgressivePrompts/condaenv.ppb9logi.requirements.txt (line 9)) (5.1.1)\nCollecting fsspec[http]>=2021.05.0\n  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.4/166.4 kB 14.4 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from huggingface-hub==0.7.0->-r /kaggle/working/ProgressivePrompts/condaenv.ppb9logi.requirements.txt (line 18)) (4.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from matplotlib==3.5.2->-r /kaggle/working/ProgressivePrompts/condaenv.ppb9logi.requirements.txt (line 23)) (2.8.2)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from matplotlib==3.5.2->-r /kaggle/working/ProgressivePrompts/condaenv.ppb9logi.requirements.txt (line 23)) (9.1.1)\nRequirement already satisfied: torch in /opt/conda/envs/nlp/lib/python3.9/site-packages (from pytorch-ranger==0.1.1->-r /kaggle/working/ProgressivePrompts/condaenv.ppb9logi.requirements.txt (line 30)) (1.10.1)\n  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.4/173.4 kB 15.7 MB/s eta 0:00:00\n  Downloading fsspec-2023.9.1-py3-none-any.whl (173 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.4/173.4 kB 14.4 MB/s eta 0:00:00\n  Downloading fsspec-2023.9.0-py3-none-any.whl (173 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.2/173.2 kB 14.4 MB/s eta 0:00:00\n  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.8/163.8 kB 14.7 MB/s eta 0:00:00\n  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.1/160.1 kB 14.8 MB/s eta 0:00:00\n  Downloading fsspec-2023.4.0-py3-none-any.whl (153 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.0/154.0 kB 15.1 MB/s eta 0:00:00\n  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 145.4/145.4 kB 14.5 MB/s eta 0:00:00\n  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.0/143.0 kB 14.0 MB/s eta 0:00:00\n  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.5/139.5 kB 6.8 MB/s eta 0:00:00\n  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.8/138.8 kB 10.0 MB/s eta 0:00:00\n  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.8/140.8 kB 13.3 MB/s eta 0:00:00\n  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.2/141.2 kB 8.6 MB/s eta 0:00:00\n  Downloading fsspec-2022.7.0-py3-none-any.whl (141 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.2/141.2 kB 13.5 MB/s eta 0:00:00\nBuilding wheels for collected packages: data, future, latex, sacremoses, shutilwhich, sklearn, tempdir\n  Building wheel for data (setup.py): started\n  Building wheel for data (setup.py): finished with status 'done'\n  Created wheel for data: filename=data-0.4-py3-none-any.whl size=7227 sha256=8eb9148311fda619653442e96fa1bf8e69b6789347244d35e49e5656f8377e3d\n  Stored in directory: /root/.cache/pip/wheels/8a/0b/a3/37ca07d5a2838bba2e475e8090455e40b94631bd57a99a35f4\n  Building wheel for future (setup.py): started\n  Building wheel for future (setup.py): finished with status 'done'\n  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=76ec4bc1d5027fbe09e4aee1a7755a7d2cdb5ef558703c069104556aa9c07c75\n  Stored in directory: /root/.cache/pip/wheels/2f/a0/d3/4030d9f80e6b3be787f19fc911b8e7aa462986a40ab1e4bb94\n  Building wheel for latex (setup.py): started\n  Building wheel for latex (setup.py): finished with status 'done'\n  Created wheel for latex: filename=latex-0.7.0-py3-none-any.whl size=7588 sha256=d158fa39004605a9638b218ac3ce72669c0ef6f92539afbf895da9c9e06db375\n  Stored in directory: /root/.cache/pip/wheels/94/84/e5/5ce582523fd479d00356867953085a67c47fbbc86506aa92f8\n  Building wheel for sacremoses (setup.py): started\n  Building wheel for sacremoses (setup.py): finished with status 'done'\n  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=12276f7a9b8ab56259e4543b297e4e84c03d9f87047abb75db62ed51af8fe49b\n  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n  Building wheel for shutilwhich (setup.py): started\n  Building wheel for shutilwhich (setup.py): finished with status 'done'\n  Created wheel for shutilwhich: filename=shutilwhich-1.1.0-py3-none-any.whl size=2767 sha256=50153a491cad42b3894509be7352e8a6315f3be948f298a1cda4efe772b93568\n  Stored in directory: /root/.cache/pip/wheels/84/c7/f5/fed66dce1ed897b44e0da776b6a592dfad0a70f7dd61f73a9d\n  Building wheel for sklearn (setup.py): started\n  Building wheel for sklearn (setup.py): finished with status 'done'\n  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1304 sha256=4664496a134449aa6c853a8935b014d47c36c62f2d6bf1d8acaa3c1f378e9e74\n  Stored in directory: /root/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n  Building wheel for tempdir (setup.py): started\n  Building wheel for tempdir (setup.py): finished with status 'done'\n  Created wheel for tempdir: filename=tempdir-0.7.1-py3-none-any.whl size=2197 sha256=b005d204f20f8a8fe8355c7de3ef0a3cf94e91a4d85c8460958b4d76f60094b2\n  Stored in directory: /root/.cache/pip/wheels/31/7b/e3/af441c2f71a48c30809aada978c1433b163a0747e73b5805ca\nSuccessfully built data future latex sacremoses shutilwhich sklearn tempdir\nInstalling collected packages: tokenizers, tempdir, shutilwhich, sentencepiece, pytz, funcsigs, xxhash, urllib3, tqdm, threadpoolctl, scipy, regex, pyyaml, pyarrow, multidict, kiwisolver, joblib, idna, future, fsspec, frozenlist, fonttools, filelock, dill, data, cycler, click, charset-normalizer, certifi, async-timeout, yarl, scikit-learn, sacremoses, requests, pytorch-ranger, pandas, nltk, multiprocess, matplotlib, latex, aiosignal, torch-optimizer, sklearn, seaborn, responses, huggingface-hub, aiohttp, transformers, adapter-transformers, datasets\nSuccessfully installed adapter-transformers-3.0.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 certifi-2022.6.15 charset-normalizer-2.0.12 click-8.1.3 cycler-0.11.0 data-0.4 datasets-2.3.2 dill-0.3.5.1 filelock-3.7.1 fonttools-4.33.3 frozenlist-1.3.0 fsspec-2022.5.0 funcsigs-1.0.2 future-0.18.2 huggingface-hub-0.7.0 idna-3.3 joblib-1.1.0 kiwisolver-1.4.3 latex-0.7.0 matplotlib-3.5.2 multidict-6.0.2 multiprocess-0.70.13 nltk-3.7 pandas-1.4.2 pyarrow-8.0.0 pytorch-ranger-0.1.1 pytz-2022.1 pyyaml-6.0 regex-2022.6.2 requests-2.28.0 responses-0.18.0 sacremoses-0.0.53 scikit-learn-1.1.1 scipy-1.8.1 seaborn-0.12.0 sentencepiece-0.1.96 shutilwhich-1.1.0 sklearn-0.0 tempdir-0.7.1 threadpoolctl-3.1.0 tokenizers-0.12.1 torch-optimizer-0.3.0 tqdm-4.64.0 transformers-4.20.0 urllib3-1.26.9 xxhash-3.0.0 yarl-1.7.2\n\ndone\n#\n# To activate this environment, use\n#\n#     $ conda activate nlp\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"cd T5_codebase","metadata":{"execution":{"iopub.status.busy":"2023-11-17T16:07:03.996279Z","iopub.execute_input":"2023-11-17T16:07:03.999542Z","iopub.status.idle":"2023-11-17T16:07:04.008642Z","shell.execute_reply.started":"2023-11-17T16:07:03.999500Z","shell.execute_reply":"2023-11-17T16:07:04.007572Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/working/ProgressivePrompts/T5_codebase\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.mkdir(\"results\")\nos.mkdir(\"save_checkpoint\")","metadata":{"execution":{"iopub.status.busy":"2023-11-17T16:07:04.011601Z","iopub.execute_input":"2023-11-17T16:07:04.012000Z","iopub.status.idle":"2023-11-17T16:07:04.941358Z","shell.execute_reply.started":"2023-11-17T16:07:04.011959Z","shell.execute_reply":"2023-11-17T16:07:04.940008Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list dbpedia_14 amazon yahoo_answers_topics ag_news \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 50 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 \\\n--save_name T5_order_1_run1 --save_dir results","metadata":{"execution":{"iopub.status.busy":"2023-11-04T21:06:40.273698Z","iopub.execute_input":"2023-11-04T21:06:40.273980Z","iopub.status.idle":"2023-11-05T07:29:30.438194Z","shell.execute_reply.started":"2023-11-04T21:06:40.273956Z","shell.execute_reply":"2023-11-05T07:29:30.437044Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Downloading: 100%|█████████████████████████| 1.18k/1.18k [00:00<00:00, 1.00MB/s]\nDownloading: 100%|█████████████████████████| 2.75G/2.75G [00:53<00:00, 55.3MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 12.1MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 3.10MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.42MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|████████████████████| 68.3M/68.3M [00:01<00:00, 47.6MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7a0e303d0ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:00<00:00, 820.97ex/s]\nk =  -1   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 852.42ex/s]\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 868.91ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-8d3084efc09afa17.arrow\n100%|██████████████████████████████████████| 7000/7000 [00:08<00:00, 830.40ex/s]\namazon\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:08<00:00, 729.91ex/s]\nk =  -1   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 726.47ex/s]\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 754.21ex/s]\n  0%|                                                  | 0/2500 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 2500/2500 [00:03<00:00, 729.82ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.56MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.54MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|██████████████████████| 319M/319M [00:08<00:00, 38.1MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [01:40<00:00, 498.13ex/s]\nk =  -1   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 514.86ex/s]\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 509.97ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-d9f1c8a7f3968471.arrow\n100%|██████████████████████████████████████| 5000/5000 [00:10<00:00, 489.04ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 1.72MB/s]                            \nDownloading metadata: 2.65kB [00:00, 1.79MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 56.9MB/s]                                      \nDownloading data: 1.86MB [00:00, 27.4MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [00:51<00:00, 973.52ex/s]\nk =  -1   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 986.58ex/s]\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 974.24ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n100%|██████████████████████████████████████| 2000/2000 [00:02<00:00, 995.80ex/s]\ntask =  dbpedia_14\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n0\n100%|███████████████████████████████████| 12500/12500 [2:24:55<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 875/875 [07:02<00:00,  2.07it/s]\n0 dbpedia_14 -> 0.9885714285714285\n1\n100%|███████████████████████████████████| 12500/12500 [2:24:55<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 875/875 [07:01<00:00,  2.08it/s]\n1 dbpedia_14 -> 0.9905714285714285\n2\n100%|███████████████████████████████████| 12500/12500 [2:24:53<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 875/875 [07:00<00:00,  2.08it/s]\n2 dbpedia_14 -> 0.9908571428571429\n3\n100%|███████████████████████████████████| 12500/12500 [2:24:52<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 875/875 [06:59<00:00,  2.08it/s]\n3 dbpedia_14 -> 0.9917142857142857\n4\n  3%|█                                    | 379/12500 [04:23<2:20:23,  1.44it/s]^C\n  3%|█                                    | 379/12500 [04:23<2:20:30,  1.44it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in train_one_task\n    batch = {k:batch[k].to('cuda') for k in batch}\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in <dictcomp>\n    batch = {k:batch[k].to('cuda') for k in batch}\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list dbpedia_14 amazon yahoo_answers_topics ag_news \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 50 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 \\\n--save_name T5_order_1_run1 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-05T08:51:10.207058Z","iopub.execute_input":"2023-11-05T08:51:10.207388Z","iopub.status.idle":"2023-11-05T19:29:44.295699Z","shell.execute_reply.started":"2023-11-05T08:51:10.207355Z","shell.execute_reply":"2023-11-05T19:29:44.294520Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading: 100%|█████████████████████████| 1.18k/1.18k [00:00<00:00, 1.18MB/s]\nDownloading: 100%|█████████████████████████| 2.75G/2.75G [01:11<00:00, 41.5MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 45.2MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 4.21MB/s]                            \nDownloading metadata: 2.47kB [00:00, 2.12MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|████████████████████| 68.3M/68.3M [00:05<00:00, 11.6MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x79ed243bcee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [00:58<00:00, 859.86ex/s]\nk =  -1   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 864.66ex/s]\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 864.24ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-8d3084efc09afa17.arrow\n100%|██████████████████████████████████████| 7000/7000 [00:08<00:00, 849.68ex/s]\namazon\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:07<00:00, 745.58ex/s]\nk =  -1   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 745.82ex/s]\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 755.86ex/s]\n  0%|                                                  | 0/2500 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 2500/2500 [00:03<00:00, 765.05ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.65MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.48MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|██████████████████████| 319M/319M [00:22<00:00, 14.3MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [01:37<00:00, 510.82ex/s]\nk =  -1   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 527.74ex/s]\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 523.52ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-d9f1c8a7f3968471.arrow\n100%|██████████████████████████████████████| 5000/5000 [00:09<00:00, 507.43ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 3.20MB/s]                            \nDownloading metadata: 2.65kB [00:00, 1.94MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 64.1MB/s]                                      \nDownloading data: 1.86MB [00:00, 53.2MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [00:50<00:00, 999.15ex/s]\nk =  -1   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1010.97ex/s]\n100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1024.81ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n100%|█████████████████████████████████████| 2000/2000 [00:01<00:00, 1017.65ex/s]\ntask =  dbpedia_14\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n4\n100%|███████████████████████████████████| 12500/12500 [2:24:54<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 875/875 [06:58<00:00,  2.09it/s]\n4 dbpedia_14 -> 0.9908571428571429\n5\n100%|███████████████████████████████████| 12500/12500 [2:24:46<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 875/875 [06:53<00:00,  2.11it/s]\n5 dbpedia_14 -> 0.9905714285714285\n6\n100%|███████████████████████████████████| 12500/12500 [2:24:48<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 875/875 [06:54<00:00,  2.11it/s]\n6 dbpedia_14 -> 0.9914285714285714\n7\n100%|███████████████████████████████████| 12500/12500 [2:24:52<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 875/875 [06:58<00:00,  2.09it/s]\n7 dbpedia_14 -> 0.9911428571428571\n8\n 14%|████▉                               | 1709/12500 [19:47<2:04:55,  1.44it/s]^C\n 14%|████▉                               | 1709/12500 [19:48<2:05:01,  1.44it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in train_one_task\n    batch = {k:batch[k].to('cuda') for k in batch}\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in <dictcomp>\n    batch = {k:batch[k].to('cuda') for k in batch}\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list dbpedia_14 amazon yahoo_answers_topics ag_news \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 50 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 \\\n--save_name T5_order_1_run1 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-06T13:03:40.476685Z","iopub.execute_input":"2023-11-06T13:03:40.477056Z","iopub.status.idle":"2023-11-06T23:37:09.080189Z","shell.execute_reply.started":"2023-11-06T13:03:40.477029Z","shell.execute_reply":"2023-11-06T23:37:09.079035Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading: 100%|██████████████████████████| 1.18k/1.18k [00:00<00:00, 827kB/s]\nDownloading: 100%|█████████████████████████| 2.75G/2.75G [01:16<00:00, 38.6MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 1.01MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 3.38MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.68MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|████████████████████| 68.3M/68.3M [00:05<00:00, 12.2MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x79cea0019ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:00<00:00, 825.04ex/s]\nk =  -1   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 828.68ex/s]\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 819.87ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-8d3084efc09afa17.arrow\n100%|██████████████████████████████████████| 7000/7000 [00:08<00:00, 827.07ex/s]\namazon\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:09<00:00, 714.77ex/s]\nk =  -1   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 697.25ex/s]\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 715.82ex/s]\n  0%|                                                  | 0/2500 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 2500/2500 [00:03<00:00, 741.83ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.26MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.20MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|██████████████████████| 319M/319M [00:42<00:00, 7.51MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [01:41<00:00, 491.68ex/s]\nk =  -1   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 502.64ex/s]\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 510.60ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-d9f1c8a7f3968471.arrow\n100%|██████████████████████████████████████| 5000/5000 [00:10<00:00, 490.41ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 2.50MB/s]                            \nDownloading metadata: 2.65kB [00:00, 1.59MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 55.1MB/s]                                      \nDownloading data: 1.86MB [00:00, 51.6MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [00:52<00:00, 956.63ex/s]\nk =  -1   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 969.93ex/s]\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 986.70ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n100%|██████████████████████████████████████| 2000/2000 [00:02<00:00, 969.64ex/s]\ntask =  dbpedia_14\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n8\n100%|███████████████████████████████████| 12500/12500 [2:24:56<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 875/875 [06:54<00:00,  2.11it/s]\n8 dbpedia_14 -> 0.9877142857142858\n9\n100%|███████████████████████████████████| 12500/12500 [2:24:54<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 875/875 [06:55<00:00,  2.11it/s]\n9 dbpedia_14 -> 0.99\nUpdated progressive prompts  torch.Size([50, 1024])\ndbpedia_14 [0.9885714285714285, 0.9905714285714285, 0.9908571428571429, 0.9917142857142857, 0.9908571428571429, 0.9905714285714285, 0.9914285714285714, 0.9911428571428571, 0.9877142857142858, 0.99]\nCalculating test acc ...\n/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py:887: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  curr_prompt = torch.tensor(self.previous_prompts, requires_grad=False).to(self.device)\n100%|█████████████████████████████████████████| 875/875 [06:51<00:00,  2.13it/s]\ntask =  amazon\nprogressive prompts\n0\n100%|███████████████████████████████████| 12500/12500 [2:24:19<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 313/313 [02:06<00:00,  2.46it/s]\n0 amazon -> 0.5704\n1\n100%|███████████████████████████████████| 12500/12500 [2:24:25<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 313/313 [02:07<00:00,  2.45it/s]\n1 amazon -> 0.6288\n2\n 12%|████▎                               | 1517/12500 [17:31<2:06:51,  1.44it/s]^C\n 12%|████▎                               | 1517/12500 [17:31<2:06:54,  1.44it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in train_one_task\n    batch = {k:batch[k].to('cuda') for k in batch}\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in <dictcomp>\n    batch = {k:batch[k].to('cuda') for k in batch}\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list dbpedia_14 amazon yahoo_answers_topics ag_news \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 50 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 \\\n--save_name T5_order_1_run1 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:50:36.466723Z","iopub.execute_input":"2023-11-07T01:50:36.467239Z","iopub.status.idle":"2023-11-07T12:01:08.344466Z","shell.execute_reply.started":"2023-11-07T01:50:36.467214Z","shell.execute_reply":"2023-11-07T12:01:08.343292Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading: 100%|██████████████████████████| 1.18k/1.18k [00:00<00:00, 917kB/s]\nDownloading: 100%|█████████████████████████| 2.75G/2.75G [00:50<00:00, 58.2MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 3.14MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 3.99MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.89MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|████████████████████| 68.3M/68.3M [00:02<00:00, 32.0MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7b744038aee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [00:58<00:00, 853.91ex/s]\nk =  -1   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 859.75ex/s]\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 874.82ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-8d3084efc09afa17.arrow\n100%|██████████████████████████████████████| 7000/7000 [00:08<00:00, 847.20ex/s]\namazon\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:07<00:00, 744.39ex/s]\nk =  -1   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 741.99ex/s]\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 764.08ex/s]\n  0%|                                                  | 0/2500 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 2500/2500 [00:03<00:00, 769.88ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.63MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.26MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|██████████████████████| 319M/319M [00:09<00:00, 33.9MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [01:37<00:00, 511.37ex/s]\nk =  -1   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 525.03ex/s]\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 515.93ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-d9f1c8a7f3968471.arrow\n100%|██████████████████████████████████████| 5000/5000 [00:09<00:00, 510.16ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 3.38MB/s]                            \nDownloading metadata: 2.65kB [00:00, 1.86MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 59.4MB/s]                                      \nDownloading data: 1.86MB [00:00, 35.0MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [00:52<00:00, 960.78ex/s]\nk =  -1   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 983.20ex/s]\n100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1003.13ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n100%|██████████████████████████████████████| 2000/2000 [00:02<00:00, 987.87ex/s]\ntask =  amazon\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n2\n100%|███████████████████████████████████| 12500/12500 [2:24:31<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 313/313 [02:08<00:00,  2.44it/s]\n2 amazon -> 0.6248\n3\n100%|███████████████████████████████████| 12500/12500 [2:24:27<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 313/313 [02:07<00:00,  2.45it/s]\n3 amazon -> 0.632\n4\n100%|███████████████████████████████████| 12500/12500 [2:24:23<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 313/313 [02:08<00:00,  2.44it/s]\n4 amazon -> 0.6232\n5\n100%|███████████████████████████████████| 12500/12500 [2:24:24<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 313/313 [02:08<00:00,  2.44it/s]\n5 amazon -> 0.6368\n6\n  9%|███▎                                | 1153/12500 [13:19<2:11:06,  1.44it/s]^C\n  9%|███▎                                | 1153/12500 [13:19<2:11:10,  1.44it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 775, in train_one_task\n    loss = self.train_step_lester(batch,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 456, in train_step_lester\n    outputs = model(\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list dbpedia_14 amazon yahoo_answers_topics ag_news \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 50 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 \\\n--save_name T5_order_1_run1 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-07T13:00:53.354619Z","iopub.execute_input":"2023-11-07T13:00:53.354897Z","iopub.status.idle":"2023-11-07T18:20:04.068179Z","shell.execute_reply.started":"2023-11-07T13:00:53.354873Z","shell.execute_reply":"2023-11-07T18:20:04.067033Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading: 100%|██████████████████████████| 1.18k/1.18k [00:00<00:00, 928kB/s]\nDownloading: 100%|█████████████████████████| 2.75G/2.75G [00:50<00:00, 58.1MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 4.27MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 3.65MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.65MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|████████████████████| 68.3M/68.3M [00:02<00:00, 31.9MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7d142c046ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:00<00:00, 827.33ex/s]\nk =  -1   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 829.38ex/s]\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 852.36ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-8d3084efc09afa17.arrow\n100%|██████████████████████████████████████| 7000/7000 [00:08<00:00, 820.45ex/s]\namazon\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:09<00:00, 717.50ex/s]\nk =  -1   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 738.07ex/s]\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 746.59ex/s]\n  0%|                                                  | 0/2500 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 2500/2500 [00:03<00:00, 739.99ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.34MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.14MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|██████████████████████| 319M/319M [00:11<00:00, 28.4MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [01:40<00:00, 499.60ex/s]\nk =  -1   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 518.17ex/s]\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 508.29ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-d9f1c8a7f3968471.arrow\n100%|██████████████████████████████████████| 5000/5000 [00:10<00:00, 497.55ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 2.18MB/s]                            \nDownloading metadata: 2.65kB [00:00, 1.55MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 60.0MB/s]                                      \nDownloading data: 1.86MB [00:00, 38.9MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [00:51<00:00, 974.64ex/s]\nk =  -1   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 985.97ex/s]\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 983.47ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n100%|██████████████████████████████████████| 2000/2000 [00:02<00:00, 997.94ex/s]\ntask =  amazon\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n6\n100%|███████████████████████████████████| 12500/12500 [2:24:31<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 313/313 [02:08<00:00,  2.44it/s]\n6 amazon -> 0.6384\n7\n100%|███████████████████████████████████| 12500/12500 [2:24:32<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 313/313 [02:08<00:00,  2.45it/s]\n7 amazon -> 0.6432\n8\n 10%|███▊                                | 1308/12500 [15:07<2:09:24,  1.44it/s]^C\n 10%|███▊                                | 1308/12500 [15:07<2:09:27,  1.44it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 775, in train_one_task\n    loss = self.train_step_lester(batch,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 456, in train_step_lester\n    outputs = model(\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list dbpedia_14 amazon yahoo_answers_topics ag_news \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 50 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 \\\n--save_name T5_order_1_run1 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-07T19:26:45.039827Z","iopub.execute_input":"2023-11-07T19:26:45.040102Z","iopub.status.idle":"2023-11-08T05:38:08.008571Z","shell.execute_reply.started":"2023-11-07T19:26:45.040078Z","shell.execute_reply":"2023-11-08T05:38:08.007314Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading: 100%|██████████████████████████| 1.18k/1.18k [00:00<00:00, 928kB/s]\nDownloading: 100%|█████████████████████████| 2.75G/2.75G [00:51<00:00, 57.7MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 19.0MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 4.02MB/s]                            \nDownloading metadata: 2.47kB [00:00, 2.14MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|████████████████████| 68.3M/68.3M [00:02<00:00, 32.1MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7cc61c057ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [00:58<00:00, 858.01ex/s]\nk =  -1   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 865.61ex/s]\n100%|██████████████████████████████████████| 3500/3500 [00:03<00:00, 886.54ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-8d3084efc09afa17.arrow\n100%|██████████████████████████████████████| 7000/7000 [00:08<00:00, 856.52ex/s]\namazon\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:07<00:00, 744.96ex/s]\nk =  -1   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 768.03ex/s]\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 764.45ex/s]\n  0%|                                                  | 0/2500 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 2500/2500 [00:03<00:00, 748.07ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.34MB/s]                            \nDownloading metadata: 1.88kB [00:00, 922kB/s]                                   \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|██████████████████████| 319M/319M [00:10<00:00, 29.5MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [01:36<00:00, 515.75ex/s]\nk =  -1   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 531.84ex/s]\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 524.09ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-d9f1c8a7f3968471.arrow\n100%|██████████████████████████████████████| 5000/5000 [00:09<00:00, 511.02ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 3.14MB/s]                            \nDownloading metadata: 2.65kB [00:00, 2.35MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 58.2MB/s]                                      \nDownloading data: 1.86MB [00:00, 36.9MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|███████████████████████████████████| 50000/50000 [00:49<00:00, 1003.14ex/s]\nk =  -1   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1009.89ex/s]\n100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1015.82ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n100%|█████████████████████████████████████| 2000/2000 [00:01<00:00, 1012.98ex/s]\ntask =  amazon\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n8\n100%|███████████████████████████████████| 12500/12500 [2:24:24<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 313/313 [02:08<00:00,  2.44it/s]\n8 amazon -> 0.6448\n9\n100%|███████████████████████████████████| 12500/12500 [2:24:22<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 313/313 [02:07<00:00,  2.45it/s]\n9 amazon -> 0.6368\nUpdated progressive prompts  torch.Size([100, 1024])\namazon [0.5704, 0.6288, 0.6248, 0.632, 0.6232, 0.6368, 0.6384, 0.6432, 0.6448, 0.6368]\nCalculating test acc ...\n/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py:887: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  curr_prompt = torch.tensor(self.previous_prompts, requires_grad=False).to(self.device)\n100%|█████████████████████████████████████████| 313/313 [02:07<00:00,  2.45it/s]\ntask =  yahoo_answers_topics\nprogressive prompts\n0\n100%|███████████████████████████████████| 12500/12500 [2:24:45<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 625/625 [05:08<00:00,  2.02it/s]\n0 yahoo_answers_topics -> 0.7636\n1\n100%|███████████████████████████████████| 12500/12500 [2:24:47<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 625/625 [05:09<00:00,  2.02it/s]\n1 yahoo_answers_topics -> 0.7696\n2\n  4%|█▍                                   | 465/12500 [05:23<2:19:20,  1.44it/s]^C\n  4%|█▍                                   | 465/12500 [05:23<2:19:26,  1.44it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in train_one_task\n    batch = {k:batch[k].to('cuda') for k in batch}\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in <dictcomp>\n    batch = {k:batch[k].to('cuda') for k in batch}\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list dbpedia_14 amazon yahoo_answers_topics ag_news \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 50 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 \\\n--save_name T5_order_1_run1 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-08T06:20:38.504230Z","iopub.execute_input":"2023-11-08T06:20:38.504527Z","iopub.status.idle":"2023-11-08T17:07:08.685776Z","shell.execute_reply.started":"2023-11-08T06:20:38.504502Z","shell.execute_reply":"2023-11-08T17:07:08.684782Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading: 100%|█████████████████████████| 1.18k/1.18k [00:00<00:00, 1.28MB/s]\nDownloading: 100%|█████████████████████████| 2.75G/2.75G [00:50<00:00, 58.1MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 2.23MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 4.45MB/s]                            \nDownloading metadata: 2.47kB [00:00, 2.27MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|████████████████████| 68.3M/68.3M [00:01<00:00, 35.0MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7b5670375ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [00:57<00:00, 865.07ex/s]\nk =  -1   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|██████████████████████████████████████| 3500/3500 [00:03<00:00, 876.75ex/s]\n100%|██████████████████████████████████████| 3500/3500 [00:03<00:00, 886.49ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-8d3084efc09afa17.arrow\n100%|██████████████████████████████████████| 7000/7000 [00:07<00:00, 875.04ex/s]\namazon\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:05<00:00, 760.31ex/s]\nk =  -1   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 759.50ex/s]\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 785.13ex/s]\n  0%|                                                  | 0/2500 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 2500/2500 [00:03<00:00, 784.50ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.65MB/s]                            \nDownloading metadata: 1.88kB [00:00, 2.00MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|██████████████████████| 319M/319M [00:12<00:00, 25.7MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [01:35<00:00, 523.81ex/s]\nk =  -1   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 533.43ex/s]\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 534.05ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-d9f1c8a7f3968471.arrow\n100%|██████████████████████████████████████| 5000/5000 [00:09<00:00, 525.78ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 3.70MB/s]                            \nDownloading metadata: 2.65kB [00:00, 2.54MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 62.3MB/s]                                      \nDownloading data: 1.86MB [00:00, 43.9MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|███████████████████████████████████| 50000/50000 [00:49<00:00, 1015.51ex/s]\nk =  -1   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1017.44ex/s]\n100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1034.75ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n100%|█████████████████████████████████████| 2000/2000 [00:01<00:00, 1010.23ex/s]\ntask =  yahoo_answers_topics\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n2\n100%|███████████████████████████████████| 12500/12500 [2:24:42<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 625/625 [05:06<00:00,  2.04it/s]\n2 yahoo_answers_topics -> 0.7684\n3\n100%|███████████████████████████████████| 12500/12500 [2:24:42<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 625/625 [05:06<00:00,  2.04it/s]\n3 yahoo_answers_topics -> 0.7776\n4\n100%|███████████████████████████████████| 12500/12500 [2:24:42<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 625/625 [05:07<00:00,  2.03it/s]\n4 yahoo_answers_topics -> 0.7748\n5\n100%|███████████████████████████████████| 12500/12500 [2:24:41<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 625/625 [05:08<00:00,  2.03it/s]\n5 yahoo_answers_topics -> 0.7812\n6\n 25%|█████████                           | 3141/12500 [36:22<1:48:21,  1.44it/s]^C\n 25%|█████████                           | 3141/12500 [36:22<1:48:22,  1.44it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in train_one_task\n    batch = {k:batch[k].to('cuda') for k in batch}\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in <dictcomp>\n    batch = {k:batch[k].to('cuda') for k in batch}\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list dbpedia_14 amazon yahoo_answers_topics ag_news \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 50 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 \\\n--save_name T5_order_1_run1 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-08T17:50:31.227135Z","iopub.execute_input":"2023-11-08T17:50:31.227970Z","iopub.status.idle":"2023-11-09T01:38:22.806897Z","shell.execute_reply.started":"2023-11-08T17:50:31.227936Z","shell.execute_reply":"2023-11-09T01:38:22.805797Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading: 100%|█████████████████████████| 1.18k/1.18k [00:00<00:00, 1.32MB/s]\nDownloading: 100%|█████████████████████████| 2.75G/2.75G [00:50<00:00, 58.3MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 6.16MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 3.44MB/s]                            \nDownloading metadata: 2.47kB [00:00, 2.06MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|████████████████████| 68.3M/68.3M [00:02<00:00, 32.3MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7c1c3c294ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [00:57<00:00, 863.30ex/s]\nk =  -1   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 872.26ex/s]\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 868.64ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-8d3084efc09afa17.arrow\n100%|██████████████████████████████████████| 7000/7000 [00:08<00:00, 860.86ex/s]\namazon\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:06<00:00, 747.23ex/s]\nk =  -1   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 778.44ex/s]\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 760.13ex/s]\n  0%|                                                  | 0/2500 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 2500/2500 [00:03<00:00, 744.22ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 3.08MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.66MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|██████████████████████| 319M/319M [00:09<00:00, 34.8MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [01:36<00:00, 516.01ex/s]\nk =  -1   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 526.44ex/s]\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 531.28ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-d9f1c8a7f3968471.arrow\n100%|██████████████████████████████████████| 5000/5000 [00:09<00:00, 507.27ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 3.42MB/s]                            \nDownloading metadata: 2.65kB [00:00, 2.15MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 61.1MB/s]                                      \nDownloading data: 1.86MB [00:00, 43.0MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [00:50<00:00, 999.03ex/s]\nk =  -1   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 987.22ex/s]\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 997.74ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n100%|█████████████████████████████████████| 2000/2000 [00:01<00:00, 1026.86ex/s]\ntask =  yahoo_answers_topics\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n6\n100%|███████████████████████████████████| 12500/12500 [2:24:48<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 625/625 [05:06<00:00,  2.04it/s]\n6 yahoo_answers_topics -> 0.7844\n7\n100%|███████████████████████████████████| 12500/12500 [2:24:45<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 625/625 [05:06<00:00,  2.04it/s]\n7 yahoo_answers_topics -> 0.7884\n8\n100%|███████████████████████████████████| 12500/12500 [2:24:39<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 625/625 [05:05<00:00,  2.05it/s]\n8 yahoo_answers_topics -> 0.788\n9\n  5%|█▉                                   | 665/12500 [07:41<2:16:59,  1.44it/s]^C\n  5%|█▉                                   | 665/12500 [07:42<2:17:05,  1.44it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 781, in train_one_task\n    loss.backward()\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/_tensor.py\", line 307, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 154, in backward\n    Variable._execution_engine.run_backward(\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list dbpedia_14 amazon yahoo_answers_topics ag_news \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 50 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 \\\n--save_name T5_order_1_run1 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-09T02:46:24.829898Z","iopub.execute_input":"2023-11-09T02:46:24.830239Z","iopub.status.idle":"2023-11-09T12:58:56.231963Z","shell.execute_reply.started":"2023-11-09T02:46:24.830214Z","shell.execute_reply":"2023-11-09T12:58:56.230721Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading: 100%|█████████████████████████| 1.18k/1.18k [00:00<00:00, 1.17MB/s]\nDownloading: 100%|█████████████████████████| 2.75G/2.75G [00:53<00:00, 55.7MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 2.10MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 4.25MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.91MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|████████████████████| 68.3M/68.3M [00:04<00:00, 16.5MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7d914c193f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:00<00:00, 829.77ex/s]\nk =  -1   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 848.46ex/s]\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 874.87ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-8d3084efc09afa17.arrow\n100%|██████████████████████████████████████| 7000/7000 [00:08<00:00, 845.00ex/s]\namazon\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:07<00:00, 737.31ex/s]\nk =  -1   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 759.87ex/s]\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 758.48ex/s]\n  0%|                                                  | 0/2500 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 2500/2500 [00:03<00:00, 767.59ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.06MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.48MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|██████████████████████| 319M/319M [00:14<00:00, 22.2MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [01:39<00:00, 503.30ex/s]\nk =  -1   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 518.01ex/s]\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 505.65ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-d9f1c8a7f3968471.arrow\n100%|██████████████████████████████████████| 5000/5000 [00:09<00:00, 501.13ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 2.88MB/s]                            \nDownloading metadata: 2.65kB [00:00, 2.34MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 61.1MB/s]                                      \nDownloading data: 1.86MB [00:00, 47.7MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [00:50<00:00, 987.26ex/s]\nk =  -1   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 968.45ex/s]\n100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1006.14ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n100%|██████████████████████████████████████| 2000/2000 [00:02<00:00, 988.21ex/s]\ntask =  yahoo_answers_topics\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n9\n100%|███████████████████████████████████| 12500/12500 [2:24:52<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 625/625 [05:12<00:00,  2.00it/s]\n9 yahoo_answers_topics -> 0.7832\nUpdated progressive prompts  torch.Size([150, 1024])\nyahoo_answers_topics [0.7636, 0.7696, 0.7684, 0.7776, 0.7748, 0.7812, 0.7844, 0.7884, 0.788, 0.7832]\nCalculating test acc ...\n/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py:887: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  curr_prompt = torch.tensor(self.previous_prompts, requires_grad=False).to(self.device)\n100%|█████████████████████████████████████████| 625/625 [05:11<00:00,  2.01it/s]\ntask =  ag_news\nprogressive prompts\n0\n100%|███████████████████████████████████| 12500/12500 [2:24:28<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 250/250 [01:42<00:00,  2.45it/s]\n0 ag_news -> 0.915\n1\n100%|███████████████████████████████████| 12500/12500 [2:24:25<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 250/250 [01:41<00:00,  2.45it/s]\n1 ag_news -> 0.917\n2\n100%|███████████████████████████████████| 12500/12500 [2:24:19<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 250/250 [01:42<00:00,  2.45it/s]\n2 ag_news -> 0.922\n3\n  5%|█▉                                   | 665/12500 [07:40<2:16:39,  1.44it/s]^C\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list dbpedia_14 amazon yahoo_answers_topics ag_news \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 50 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 \\\n--save_name T5_order_1_run1 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-09T15:00:14.077766Z","iopub.execute_input":"2023-11-09T15:00:14.078026Z","iopub.status.idle":"2023-11-10T01:17:52.431630Z","shell.execute_reply.started":"2023-11-09T15:00:14.078003Z","shell.execute_reply":"2023-11-10T01:17:52.430594Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Downloading: 100%|█████████████████████████| 1.18k/1.18k [00:00<00:00, 1.27MB/s]\nDownloading: 100%|█████████████████████████| 2.75G/2.75G [00:49<00:00, 59.5MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 9.94MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 3.92MB/s]                            \nDownloading metadata: 2.47kB [00:00, 2.27MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|████████████████████| 68.3M/68.3M [00:02<00:00, 33.3MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7f2adc21eee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [00:57<00:00, 862.10ex/s]\nk =  -1   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 872.54ex/s]\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 873.88ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-8d3084efc09afa17.arrow\n100%|██████████████████████████████████████| 7000/7000 [00:08<00:00, 871.54ex/s]\namazon\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:06<00:00, 747.71ex/s]\nk =  -1   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 763.98ex/s]\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 760.07ex/s]\n  0%|                                                  | 0/2500 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 2500/2500 [00:03<00:00, 776.12ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.74MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.73MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|██████████████████████| 319M/319M [01:31<00:00, 3.50MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [01:36<00:00, 516.27ex/s]\nk =  -1   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 534.06ex/s]\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 532.48ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-d9f1c8a7f3968471.arrow\n100%|██████████████████████████████████████| 5000/5000 [00:09<00:00, 521.08ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 3.09MB/s]                            \nDownloading metadata: 2.65kB [00:00, 2.19MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 58.2MB/s]                                      \nDownloading data: 1.86MB [00:00, 38.0MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|███████████████████████████████████| 50000/50000 [00:49<00:00, 1005.31ex/s]\nk =  -1   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1016.91ex/s]\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 994.95ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n100%|█████████████████████████████████████| 2000/2000 [00:01<00:00, 1000.11ex/s]\ntask =  ag_news\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n3\n100%|███████████████████████████████████| 12500/12500 [2:24:24<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 250/250 [01:42<00:00,  2.45it/s]\n3 ag_news -> 0.916\n4\n100%|███████████████████████████████████| 12500/12500 [2:24:23<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 250/250 [01:42<00:00,  2.45it/s]\n4 ag_news -> 0.922\n5\n100%|███████████████████████████████████| 12500/12500 [2:24:16<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 250/250 [01:41<00:00,  2.46it/s]\n5 ag_news -> 0.92\n6\n100%|███████████████████████████████████| 12500/12500 [2:24:17<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 250/250 [01:41<00:00,  2.46it/s]\n6 ag_news -> 0.929\n7\n 15%|█████▎                              | 1848/12500 [21:19<2:03:16,  1.44it/s]^C\n 15%|█████▎                              | 1848/12500 [21:19<2:02:54,  1.44it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in train_one_task\n    batch = {k:batch[k].to('cuda') for k in batch}\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in <dictcomp>\n    batch = {k:batch[k].to('cuda') for k in batch}\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list dbpedia_14 amazon yahoo_answers_topics ag_news \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 50 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 \\\n--save_name T5_order_1_run1 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-10T02:17:36.020635Z","iopub.execute_input":"2023-11-10T02:17:36.020936Z","iopub.status.idle":"2023-11-10T07:25:58.128757Z","shell.execute_reply.started":"2023-11-10T02:17:36.020907Z","shell.execute_reply":"2023-11-10T07:25:58.127563Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading: 100%|█████████████████████████| 1.18k/1.18k [00:00<00:00, 1.32MB/s]\nDownloading: 100%|█████████████████████████| 2.75G/2.75G [00:52<00:00, 55.9MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 3.18MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 3.96MB/s]                            \nDownloading metadata: 2.47kB [00:00, 2.05MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|████████████████████| 68.3M/68.3M [00:03<00:00, 22.7MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7bf20022bee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [00:58<00:00, 853.62ex/s]\nk =  -1   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 853.27ex/s]\n100%|██████████████████████████████████████| 3500/3500 [00:03<00:00, 897.22ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-8d3084efc09afa17.arrow\n100%|██████████████████████████████████████| 7000/7000 [00:08<00:00, 848.42ex/s]\namazon\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:06<00:00, 752.68ex/s]\nk =  -1   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 765.65ex/s]\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 784.27ex/s]\n  0%|                                                  | 0/2500 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 2500/2500 [00:03<00:00, 767.26ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 3.17MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.59MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|██████████████████████| 319M/319M [00:13<00:00, 23.3MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [01:35<00:00, 525.95ex/s]\nk =  -1   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 534.83ex/s]\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 536.17ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-d9f1c8a7f3968471.arrow\n100%|██████████████████████████████████████| 5000/5000 [00:09<00:00, 522.42ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 3.44MB/s]                            \nDownloading metadata: 2.65kB [00:00, 2.38MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 61.8MB/s]                                      \nDownloading data: 1.86MB [00:00, 48.3MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|███████████████████████████████████| 50000/50000 [00:49<00:00, 1015.94ex/s]\nk =  -1   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1004.09ex/s]\n100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1033.79ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n100%|█████████████████████████████████████| 2000/2000 [00:01<00:00, 1025.71ex/s]\ntask =  ag_news\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n7\n100%|███████████████████████████████████| 12500/12500 [2:24:34<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 250/250 [01:42<00:00,  2.43it/s]\n7 ag_news -> 0.926\n8\n100%|███████████████████████████████████| 12500/12500 [2:24:33<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 250/250 [01:42<00:00,  2.44it/s]\n8 ag_news -> 0.922\n9\n  4%|█▎                                   | 457/12500 [05:16<2:19:14,  1.44it/s]^C\n  4%|█▎                                   | 457/12500 [05:17<2:19:29,  1.44it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 781, in train_one_task\n    loss.backward()\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/_tensor.py\", line 307, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 154, in backward\n    Variable._execution_engine.run_backward(\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list dbpedia_14 amazon yahoo_answers_topics ag_news \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 50 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 \\\n--save_name T5_order_1_run1 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-10T08:52:36.035328Z","iopub.execute_input":"2023-11-10T08:52:36.035606Z","iopub.status.idle":"2023-11-10T11:32:12.440318Z","shell.execute_reply.started":"2023-11-10T08:52:36.035583Z","shell.execute_reply":"2023-11-10T11:32:12.439194Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading: 100%|█████████████████████████| 1.18k/1.18k [00:00<00:00, 1.02MB/s]\nDownloading: 100%|█████████████████████████| 2.75G/2.75G [01:16<00:00, 38.5MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 3.97MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 3.97MB/s]                            \nDownloading metadata: 2.47kB [00:00, 2.33MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|████████████████████| 68.3M/68.3M [00:09<00:00, 7.22MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x79ce40260ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [00:59<00:00, 847.40ex/s]\nk =  -1   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 799.16ex/s]\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 811.21ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-8d3084efc09afa17.arrow\n100%|██████████████████████████████████████| 7000/7000 [00:08<00:00, 790.18ex/s]\namazon\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:11<00:00, 697.46ex/s]\nk =  -1   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 738.24ex/s]\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 727.66ex/s]\n  0%|                                                  | 0/2500 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 2500/2500 [00:03<00:00, 729.44ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.71MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.50MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|██████████████████████| 319M/319M [00:20<00:00, 15.5MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [01:39<00:00, 501.76ex/s]\nk =  -1   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 519.62ex/s]\n100%|██████████████████████████████████████| 2500/2500 [00:04<00:00, 506.25ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-d9f1c8a7f3968471.arrow\n100%|██████████████████████████████████████| 5000/5000 [00:09<00:00, 507.79ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 2.53MB/s]                            \nDownloading metadata: 2.65kB [00:00, 2.21MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 62.0MB/s]                                      \nDownloading data: 1.86MB [00:00, 53.5MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [00:51<00:00, 965.03ex/s]\nk =  -1   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 925.00ex/s]\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 977.39ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n100%|██████████████████████████████████████| 2000/2000 [00:02<00:00, 964.15ex/s]\ntask =  ag_news\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n9\n100%|███████████████████████████████████| 12500/12500 [2:24:35<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 250/250 [01:42<00:00,  2.43it/s]\n9 ag_news -> 0.93\nUpdated progressive prompts  torch.Size([200, 1024])\nag_news [0.915, 0.917, 0.922, 0.916, 0.922, 0.92, 0.929, 0.926, 0.922, 0.93]\nCalculating test acc ...\n/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py:887: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  curr_prompt = torch.tensor(self.previous_prompts, requires_grad=False).to(self.device)\n100%|█████████████████████████████████████████| 250/250 [01:42<00:00,  2.43it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"mv /kaggle/working/ProgressivePrompts/T5_codebase/save_checkpoint/current_checkpoint.pt /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2023-11-10T07:26:38.718140Z","iopub.execute_input":"2023-11-10T07:26:38.718534Z","iopub.status.idle":"2023-11-10T07:26:39.658564Z","shell.execute_reply.started":"2023-11-10T07:26:38.718497Z","shell.execute_reply":"2023-11-10T07:26:39.657332Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-11-10T07:26:42.762343Z","iopub.execute_input":"2023-11-10T07:26:42.762720Z","iopub.status.idle":"2023-11-10T07:26:42.769851Z","shell.execute_reply.started":"2023-11-10T07:26:42.762686Z","shell.execute_reply":"2023-11-10T07:26:42.768908Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'current_checkpoint.pt')","metadata":{"execution":{"iopub.status.busy":"2023-11-10T07:26:50.688125Z","iopub.execute_input":"2023-11-10T07:26:50.688771Z","iopub.status.idle":"2023-11-10T07:26:50.694927Z","shell.execute_reply.started":"2023-11-10T07:26:50.688741Z","shell.execute_reply":"2023-11-10T07:26:50.694019Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/current_checkpoint.pt","text/html":"<a href='current_checkpoint.pt' target='_blank'>current_checkpoint.pt</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list dbpedia_14 amazon yahoo_answers_topics ag_news \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 50 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 \\\n--save_name T5_order_1_run1 --save_dir results --continue_train 1 --test_eval_after_every_task 1","metadata":{"execution":{"iopub.status.busy":"2023-11-17T16:07:04.943296Z","iopub.execute_input":"2023-11-17T16:07:04.943787Z","iopub.status.idle":"2023-11-17T19:01:20.802298Z","shell.execute_reply.started":"2023-11-17T16:07:04.943740Z","shell.execute_reply":"2023-11-17T19:01:20.801251Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Downloading: 100%|██████████████████████████| 1.18k/1.18k [00:00<00:00, 809kB/s]\nDownloading: 100%|█████████████████████████| 2.75G/2.75G [02:19<00:00, 21.2MB/s]\nDownloading: 100%|███████████████████████████| 773k/773k [00:00<00:00, 6.93MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 2.94MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.44MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|████████████████████| 68.3M/68.3M [00:01<00:00, 36.1MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7b97501aeee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:07<00:00, 743.65ex/s]\nk =  -1   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 756.57ex/s]\n100%|██████████████████████████████████████| 3500/3500 [00:04<00:00, 744.24ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c/cache-8d3084efc09afa17.arrow\n100%|██████████████████████████████████████| 7000/7000 [00:09<00:00, 740.90ex/s]\namazon\n  0%|                                                 | 0/50000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|████████████████████████████████████| 50000/50000 [01:17<00:00, 642.79ex/s]\nk =  -1   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 659.05ex/s]\n100%|██████████████████████████████████████| 1250/1250 [00:01<00:00, 670.57ex/s]\n  0%|                                                  | 0/2500 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████████████████████████████████| 2500/2500 [00:03<00:00, 672.67ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 1.79MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.05MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|██████████████████████| 319M/319M [00:10<00:00, 31.2MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [01:50<00:00, 451.44ex/s]\nk =  -1   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|██████████████████████████████████████| 2500/2500 [00:05<00:00, 454.68ex/s]\n100%|██████████████████████████████████████| 2500/2500 [00:05<00:00, 461.33ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439/cache-d9f1c8a7f3968471.arrow\n100%|██████████████████████████████████████| 5000/5000 [00:10<00:00, 454.87ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 1.92MB/s]                            \nDownloading metadata: 2.65kB [00:00, 893kB/s]                                   \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 51.6MB/s]                                      \nDownloading data: 1.86MB [00:00, 26.2MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|████████████████████████████████████| 50000/50000 [00:57<00:00, 871.46ex/s]\nk =  -1   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 886.22ex/s]\n100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 870.63ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n100%|██████████████████████████████████████| 2000/2000 [00:02<00:00, 880.87ex/s]\ntask =  ag_news\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n9\n100%|███████████████████████████████████| 12500/12500 [2:24:32<00:00,  1.44it/s]\ntorch.Size([50, 1024])\n100%|█████████████████████████████████████████| 250/250 [01:44<00:00,  2.40it/s]\n9 ag_news -> 0.931\nUpdated progressive prompts  torch.Size([200, 1024])\nag_news [0.915, 0.917, 0.922, 0.916, 0.922, 0.92, 0.929, 0.926, 0.922, 0.931]\nCalculating test acc ...\n/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py:887: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  curr_prompt = torch.tensor(self.previous_prompts, requires_grad=False).to(self.device)\n100%|█████████████████████████████████████████| 875/875 [06:27<00:00,  2.26it/s]\n100%|█████████████████████████████████████████| 313/313 [02:08<00:00,  2.44it/s]\n100%|█████████████████████████████████████████| 625/625 [04:36<00:00,  2.26it/s]\n100%|█████████████████████████████████████████| 250/250 [01:42<00:00,  2.44it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nres = np.load('/kaggle/working/ProgressivePrompts/T5_codebase/results/T5_order_1_run1/results_dict.npy', allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T19:18:10.002030Z","iopub.execute_input":"2023-11-17T19:18:10.002900Z","iopub.status.idle":"2023-11-17T19:18:10.007496Z","shell.execute_reply.started":"2023-11-17T19:18:10.002868Z","shell.execute_reply":"2023-11-17T19:18:10.006524Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"res","metadata":{"execution":{"iopub.status.busy":"2023-11-17T19:18:12.517096Z","iopub.execute_input":"2023-11-17T19:18:12.517449Z","iopub.status.idle":"2023-11-17T19:18:12.523391Z","shell.execute_reply.started":"2023-11-17T19:18:12.517418Z","shell.execute_reply":"2023-11-17T19:18:12.522528Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"array({'test': {'dbpedia_14': 0.9902857142857143, 'amazon': 0.6488, 'yahoo_answers_topics': 0.7824, 3: {'dbpedia_14': 0.0, 'amazon': 0.0, 'yahoo_answers_topics': 0.2276, 'ag_news': 0.94}}, 'dbpedia_14': [0.9885714285714285, 0.9905714285714285, 0.9908571428571429, 0.9917142857142857, 0.9908571428571429, 0.9905714285714285, 0.9914285714285714, 0.9911428571428571, 0.9877142857142858, 0.99], 'amazon': [0.5704, 0.6288, 0.6248, 0.632, 0.6232, 0.6368, 0.6384, 0.6432, 0.6448, 0.6368], 'yahoo_answers_topics': [0.7636, 0.7696, 0.7684, 0.7776, 0.7748, 0.7812, 0.7844, 0.7884, 0.788, 0.7832], 'ag_news': [0.915, 0.917, 0.922, 0.916, 0.922, 0.92, 0.929, 0.926, 0.922, 0.931]},\n      dtype=object)"},"metadata":{}}]}]}