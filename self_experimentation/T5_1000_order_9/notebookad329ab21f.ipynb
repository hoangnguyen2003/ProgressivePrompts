{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7006383,"sourceType":"datasetVersion","datasetId":4020235},{"sourceId":7071251,"sourceType":"datasetVersion","datasetId":4064434,"isSourceIdPinned":false}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"mv /kaggle/input/large-number-tasks/ProgressivePrompts /kaggle/working/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-11-28T07:24:34.553768Z","iopub.execute_input":"2023-11-28T07:24:34.554042Z","iopub.status.idle":"2023-11-28T07:24:38.346025Z","shell.execute_reply.started":"2023-11-28T07:24:34.554016Z","shell.execute_reply":"2023-11-28T07:24:38.344451Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"mv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/environment.yaml': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/LICENSE': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/datasets/README.md': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/datasets/src/data/amazon/train.csv': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/datasets/src/data/amazon/test.csv': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/BERT_codebase/train_soft_prompt.py': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/BERT_codebase/train_cl2.py': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/BERT_codebase/run_prog_prompts.sh': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/BERT_codebase/good_id_yahoo_test.npy': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/BERT_codebase/continual_learning_utils.py': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/BERT_codebase/dataset_utils.py': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/BERT_codebase/continual_learning_one_head.py': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/BERT_codebase/model_utils.py': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/BERT_codebase/train_cl.py': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/BERT_codebase/good_id_yahoo_train.npy': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/README.md': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/T5_codebase/train_prompt.py': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/T5_codebase/good_id_yahoo_test.npy': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/T5_codebase/prompt_debug.ipynb': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/T5_codebase/train_t5_cl.py': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/T5_codebase/good_id_yahoo_train.npy': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/T5_codebase/t5_dataset.py': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/T5_codebase/t5_continual.py': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/experimental_results/T5_order_3/prompts.npy': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/experimental_results/T5_order_3/ProgressivePrompts.ipynb': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/experimental_results/T5_order_3/results_dict.npy': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/experimental_results/T5_order_1_large_data/prompts.npy': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/experimental_results/T5_order_1_large_data/ProgressivePrompts.ipynb': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/experimental_results/T5_order_1_large_data/results_dict.npy': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/experimental_results/T5_order_1/prompts.npy': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/experimental_results/T5_order_1/ProgressivePrompts.ipynb': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/experimental_results/T5_order_1/results_dict.npy': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/experimental_results/T5_order_2/prompts.npy': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/experimental_results/T5_order_2/ProgressivePrompts.ipynb': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/experimental_results/T5_order_2/results_dict.npy': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/images/test.png': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/images/illustration.png': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/COMMIT_EDITMSG': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/config': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/info/exclude': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/refs/heads/main': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/refs/remotes/origin/HEAD': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/refs/remotes/origin/main': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/hooks/pre-merge-commit.sample': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/hooks/prepare-commit-msg.sample': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/hooks/update.sample': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/hooks/pre-push.sample': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/hooks/pre-rebase.sample': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/hooks/pre-applypatch.sample': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/hooks/push-to-checkout.sample': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/hooks/pre-commit.sample': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/hooks/commit-msg.sample': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/hooks/post-update.sample': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/hooks/pre-receive.sample': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/hooks/fsmonitor-watchman.sample': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/hooks/applypatch-msg.sample': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/packed-refs': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/HEAD': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/index': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/logs/refs/heads/main': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/logs/refs/remotes/origin/HEAD': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/logs/refs/remotes/origin/main': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/logs/HEAD': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/17/3073af254c62314fdc01c681efdcd97e84b4b1': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/81/a65f669bfe32428a16c002c013281766154547': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/19/da081be6da60776c1e22835e7ec851324bce63': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/a2/d0f9c9e8e27b4d05707a0e3c5c2e89a82c774e': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/cf/0bbe3e07373bf1036d6c251e89ed41882164ae': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/b8/f11ebedef504a56274bd8e394f477adce5f665': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/35/9420243d81df801369edf1b9c98bc47fdc9428': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/1b/711698907ee5dafbd93580c0c04762c2afcefe': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/9e/875fe63587e9a1b4163725d98d9218df19bdcc': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/32/e7ccdf9eb96252bbaa7ac4f7860e9ce01096a0': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/f0/1fd00c5f2937b543ea0068710baba945aa2852': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/d9/35124b6b2d9de57cf19c5400d026db6c345caa': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/e0/24ec3e7a4c5202e228d0acf2d1ebf291b7417c': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/b9/1a477b2ae70dddd3e48d37cc14c482e3d15284': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/31/fd669b4a8c707ce70f444c321c5a294528742a': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/f9/e1364cb8f4f2be90c2ad9553bf7908f518b517': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/b6/bfe75ccdc8106addce62b84fdd133ef51bc5b0': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/ec/fc9fd96f0f6cb09a677d2146bdd847f710f054': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/a5/bd66e751c6d8e67b1274d7e912086829d637f4': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/72/e613cdd187afa39da70a6c28a00aa6382f2854': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/6c/0ec92eeabff3e970ea3bf90f8f2e1cba2ad400': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/99/493a58cf13b9508d5b16cce3d83c6609c74ce4': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/b5/36d14aebe9d722e9ec4dd3ef16226ee5fb71ae': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/c6/12e23b5af8174b37362a2fc670f0866ab93fc5': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/15/a62685050dda0e06674ef779862c155ee4910e': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/ae/84b053f492814d7df8b2545c947499097505c8': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/ae/36bb87b2d76010cd5412abc4109b79145e931f': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/90/c2ab740759bfbeddd6af91127d307b7b68b128': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/90/016de1039ed219527ce7df13f178d57fe72e5d': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/95/8ea192f5c74ef5c1ec52ca0782e8d4fb342bb6': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/f3/74e755a13013977766d0fc27f6efae292bb76c': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/ed/b1eb648d6a47d9639f432bb272bfc5748c9780': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/09/a778db5cc6a2d76fb8a4381c23e3e53fc107c8': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/09/9ad7d0d7ff73928f2f5c835f0775eae216384c': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/pack/pack-2b0fcac691b2eb0f3ef8bd98f30bbfbe9aeb00a4.idx': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/pack/pack-2b0fcac691b2eb0f3ef8bd98f30bbfbe9aeb00a4.pack': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/4f/2d0858f89dae4cd6b05d4bc2219dfc25a3f862': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/30/3f870f1b0db5dbf5629f1957998ad118169aa5': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/7a/f726782eff88fb94ea66636e119586ebc963f5': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/03/615df04c03cdcb604660fc20493f8357d67102': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/76/0587e33f7dd96bb6567721f51d21bb8503ba51': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/84/84f343ff01da6d017ea1ffe521c9a2eb5ddb74': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/df/6dd1a450f30d0a415ff112e722492717573d71': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/1f/ef8898c9d495d89e62856d22a1716740f910c1': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/82/809ee59a913b718fa3a411d3bc114fc1199d6f': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/aa/6d2861208453a57c22794cf329876ca2709b9e': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/6b/880082ab29e0d47e018cb45c5b29df465ad93a': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/21/a15ef078c08b3da1fe2bd63a71dcfe8d45ede0': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/44/6d4509e4471808b7362a244f3c563beafafae1': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/d3/4a901f7047255683affba54dba83f1ba1cf019': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/8c/699a35e2df8ba0a5e9aebf490f1f9afe83fa66': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/fa/50a91b2d6ff41548d1cbe6e69d6acda920c1a7': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/c1/e7918e4f41cf3b4e577dc9351ba18acecc44c0': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/48/5a32f4519ff5d933f62c3afe1408c67c0dd6f6': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/3b/98e6e1b9de5d291e2b4a4295b1fa7d191e71c8': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/fd/0c4132a28b4dc39554b24d0c7624e09295aad3': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/24/6e61cc804fa9b700bf59882fce672a8d502c61': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/6a/a62969aba32285b7fa80648fe7d0e457a6e178': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/objects/6a/8d9b684a03866bbfdc58206d3693daaf04493d': Read-only file system\nmv: cannot remove '/kaggle/input/large-number-tasks/ProgressivePrompts/.git/description': Read-only file system\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/ProgressivePrompts/T5_codebase\nimport os\nos.mkdir(\"load_checkpoint\")\n%mv /kaggle/input/t5-1000-2/current_checkpoint.pt /kaggle/working/ProgressivePrompts/T5_codebase/load_checkpoint","metadata":{"execution":{"iopub.status.busy":"2023-11-28T07:24:38.354340Z","iopub.execute_input":"2023-11-28T07:24:38.355265Z","iopub.status.idle":"2023-11-28T07:25:04.085853Z","shell.execute_reply.started":"2023-11-28T07:24:38.355204Z","shell.execute_reply":"2023-11-28T07:25:04.084408Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/ProgressivePrompts/T5_codebase\nmv: cannot remove '/kaggle/input/t5-1000-2/current_checkpoint.pt': Read-only file system\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/ProgressivePrompts\n%conda env create -f environment.yaml","metadata":{"execution":{"iopub.status.busy":"2023-11-28T07:25:04.087805Z","iopub.execute_input":"2023-11-28T07:25:04.088375Z","iopub.status.idle":"2023-11-28T07:37:15.504272Z","shell.execute_reply.started":"2023-11-28T07:25:04.088313Z","shell.execute_reply":"2023-11-28T07:37:15.503120Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/ProgressivePrompts\nRetrieving notices: ...working... done\nCollecting package metadata (repodata.json): | WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.9.0.*, but conda is ignoring the .* and treating it as 1.9.0\nWARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\nWARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.8.0.*, but conda is ignoring the .* and treating it as 1.8.0\nWARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.6.0.*, but conda is ignoring the .* and treating it as 1.6.0\ndone\nSolving environment: done\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 23.7.4\n  latest version: 23.10.0\n\nPlease update conda by running\n\n    $ conda update -n base -c conda-forge conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=23.10.0\n\n\n\nDownloading and Extracting Packages\nbzip2-1.0.8          | 484 KB    |                                       |   0% \nlibzlib-1.2.12       | 63 KB     |                                       |   0% \u001b[A\n\njupyterlab_widgets-1 | 134 KB    |                                       |   0% \u001b[A\u001b[A\n\n\nnbformat-5.3.0       | 142 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n\n\n\nsend2trash-1.8.0     | 19 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\npython-fastjsonschem | 27 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\njupyter_core-4.9.2   | 84 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nwcwidth-0.2.5        | 34 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\npyzmq-22.3.0         | 509 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nprompt-toolkit-3.0.2 | 254 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nterminado-0.13.1     | 30 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\njupyter-1.0.0        | 8 KB      |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nglib-2.56.2          | 5.0 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibuuid-1.0.3        | 18 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnbclient-0.5.13      | 96 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy-base-1.22.3    | 6.8 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxorg-libxau-1.0.9    | 13 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexecuting-0.8.3      | 18 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nld_impl_linux-64-2.3 | 667 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njpeg-9e              | 268 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlcms2-2.12           | 443 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmistune-0.8.4        | 57 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\nbzip2-1.0.8          | 484 KB    | #2                                    |   3% \u001b[A\u001b[A\nlibzlib-1.2.12       | 63 KB     | #########4                            |  25% \u001b[A\n\n\n\nsend2trash-1.8.0     | 19 KB     | ###############################2      |  84% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\nnbformat-5.3.0       | 142 KB    | ####1                                 |  11% \u001b[A\u001b[A\u001b[A\n\n\n\nsend2trash-1.8.0     | 19 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\njupyter_core-4.9.2   | 84 KB     | #######                               |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nwcwidth-0.2.5        | 34 KB     | #################6                    |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\npython-fastjsonschem | 27 KB     | #####################8                |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nlibzlib-1.2.12       | 63 KB     | ##################################### | 100% \u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nprompt-toolkit-3.0.2 | 254 KB    | ##3                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nterminado-0.13.1     | 30 KB     | ###################7                  |  53% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\npyzmq-22.3.0         | 509 KB    | #1                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\njupyter-1.0.0        | 8 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nwcwidth-0.2.5        | 34 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nwcwidth-0.2.5        | 34 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibuuid-1.0.3        | 18 KB     | ################################3     |  87% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nglib-2.56.2          | 5.0 MB    | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ##                                    |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnbclient-0.5.13      | 96 KB     | ######1                               |  17% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxorg-libxau-1.0.9    | 13 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy-base-1.22.3    | 6.8 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexecuting-0.8.3      | 18 KB     | #################################2    |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\njupyterlab_widgets-1 | 134 KB    | ##################################### | 100% \u001b[A\u001b[A\n\njupyterlab_widgets-1 | 134 KB    | ##################################### | 100% \u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nld_impl_linux-64-2.3 | 667 KB    | 8                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nglib-2.56.2          | 5.0 MB    | ############5                         |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ###3                                  |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy-base-1.22.3    | 6.8 MB    | ########1                             |  22% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlcms2-2.12           | 443 KB    | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njpeg-9e              | 268 KB    | ##2                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nglib-2.56.2          | 5.0 MB    | #####################5                |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ####4                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmistune-0.8.4        | 57 KB     | ##########4                           |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\npython-fastjsonschem | 27 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\npython-fastjsonschem | 27 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy-base-1.22.3    | 6.8 MB    | ##############6                       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nbzip2-1.0.8          | 484 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | #####6                                |  15% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy-base-1.22.3    | 6.8 MB    | #####################7                |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | #######                               |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy-base-1.22.3    | 6.8 MB    | ############################7         |  78% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\njupyter_core-4.9.2   | 84 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\njupyter_core-4.9.2   | 84 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nterminado-0.13.1     | 30 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\nterminado-0.13.1     | 30 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\njupyter-1.0.0        | 8 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy-base-1.22.3    | 6.8 MB    | ###################################2  |  95% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ########2                             |  22% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | #########4                            |  26% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ##########6                           |  29% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ###########8                          |  32% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | #############                         |  35% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nnbformat-5.3.0       | 142 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\nnbformat-5.3.0       | 142 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ##############1                       |  38% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibuuid-1.0.3        | 18 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ###############4                      |  42% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ################5                     |  45% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | #################7                    |  48% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nprompt-toolkit-3.0.2 | 254 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\nprompt-toolkit-3.0.2 | 254 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ###################1                  |  52% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxorg-libxau-1.0.9    | 13 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnbclient-0.5.13      | 96 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnbclient-0.5.13      | 96 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ####################3                 |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexecuting-0.8.3      | 18 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | #####################7                |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | #######################1              |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ########################5             |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlcms2-2.12           | 443 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlcms2-2.12           | 443 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nld_impl_linux-64-2.3 | 667 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nld_impl_linux-64-2.3 | 667 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | #########################9            |  70% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ###########################5          |  74% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\npyzmq-22.3.0         | 509 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\npyzmq-22.3.0         | 509 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njpeg-9e              | 268 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njpeg-9e              | 268 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ############################9         |  78% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ##############################4       |  82% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ###############################8      |  86% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | #################################3    |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmistune-0.8.4        | 57 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmistune-0.8.4        | 57 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ###################################1  |  95% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ####################################7 |  99% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\nglib-2.56.2          | 5.0 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy-base-1.22.3    | 6.8 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\npillow-9.1.1         | 44.5 MB   | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \n                                                                                \u001b[A\n\n                                                                                \u001b[A\u001b[A\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: | By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n\ndone\nInstalling pip dependencies: / Ran pip subprocess with arguments:\n['/opt/conda/envs/nlp/bin/python', '-m', 'pip', 'install', '-U', '-r', '/kaggle/working/ProgressivePrompts/condaenv.80gubv5j.requirements.txt', '--exists-action=b']\nPip subprocess output:\nCollecting adapter-transformers==3.0.1\n  Downloading adapter_transformers-3.0.1-py3-none-any.whl (3.9 MB)\n      3.9/3.9 MB 46.4 MB/s eta 0:00:00\nCollecting aiohttp==3.8.1\n  Downloading aiohttp-3.8.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n      1.2/1.2 MB 60.2 MB/s eta 0:00:00\nCollecting aiosignal==1.2.0\n  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\nCollecting async-timeout==4.0.2\n  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\nCollecting certifi==2022.6.15\n  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n      160.2/160.2 kB 16.0 MB/s eta 0:00:00\nCollecting charset-normalizer==2.0.12\n  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\nCollecting click==8.1.3\n  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n      96.6/96.6 kB 9.9 MB/s eta 0:00:00\nCollecting cycler==0.11.0\n  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\nCollecting data==0.4\n  Downloading data-0.4.tar.gz (7.0 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting datasets==2.3.2\n  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n      362.3/362.3 kB 30.6 MB/s eta 0:00:00\nCollecting dill==0.3.5.1\n  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n      95.8/95.8 kB 9.4 MB/s eta 0:00:00\nCollecting filelock==3.7.1\n  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\nCollecting fonttools==4.33.3\n  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n      930.9/930.9 kB 55.3 MB/s eta 0:00:00\nCollecting frozenlist==1.3.0\n  Downloading frozenlist-1.3.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n      156.2/156.2 kB 15.5 MB/s eta 0:00:00\nCollecting fsspec==2022.5.0\n  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n      140.6/140.6 kB 13.4 MB/s eta 0:00:00\nCollecting funcsigs==1.0.2\n  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\nCollecting future==0.18.2\n  Downloading future-0.18.2.tar.gz (829 kB)\n      829.2/829.2 kB 52.3 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting huggingface-hub==0.7.0\n  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n      86.2/86.2 kB 7.5 MB/s eta 0:00:00\nCollecting idna==3.3\n  Downloading idna-3.3-py3-none-any.whl (61 kB)\n      61.2/61.2 kB 5.5 MB/s eta 0:00:00\nCollecting joblib==1.1.0\n  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n      307.0/307.0 kB 26.3 MB/s eta 0:00:00\nCollecting kiwisolver==1.4.3\n  Downloading kiwisolver-1.4.3-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n      1.6/1.6 MB 64.8 MB/s eta 0:00:00\nCollecting latex==0.7.0\n  Downloading latex-0.7.0.tar.gz (6.5 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting matplotlib==3.5.2\n  Downloading matplotlib-3.5.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n      11.2/11.2 MB 80.9 MB/s eta 0:00:00\nCollecting multidict==6.0.2\n  Downloading multidict-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n      114.2/114.2 kB 12.5 MB/s eta 0:00:00\nCollecting multiprocess==0.70.13\n  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n      132.3/132.3 kB 13.3 MB/s eta 0:00:00\nCollecting nltk==3.7\n  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n      1.5/1.5 MB 72.1 MB/s eta 0:00:00\nCollecting pandas==1.4.2\n  Downloading pandas-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n      11.7/11.7 MB 81.8 MB/s eta 0:00:00\nCollecting pyarrow==8.0.0\n  Downloading pyarrow-8.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n      29.4/29.4 MB 46.3 MB/s eta 0:00:00\nRequirement already satisfied: pyparsing==3.0.9 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from -r /kaggle/working/ProgressivePrompts/condaenv.80gubv5j.requirements.txt (line 29)) (3.0.9)\nCollecting pytorch-ranger==0.1.1\n  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\nCollecting pytz==2022.1\n  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n      503.5/503.5 kB 39.8 MB/s eta 0:00:00\nCollecting pyyaml==6.0\n  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n      661.8/661.8 kB 46.8 MB/s eta 0:00:00\nCollecting regex==2022.6.2\n  Downloading regex-2022.6.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)\n      763.2/763.2 kB 1.7 MB/s eta 0:00:00\nCollecting requests==2.28.0\n  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n      62.8/62.8 kB 6.7 MB/s eta 0:00:00\nCollecting responses==0.18.0\n  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\nCollecting sacremoses==0.0.53\n  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n      880.6/880.6 kB 56.8 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting scikit-learn==1.1.1\n  Downloading scikit_learn-1.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.8 MB)\n      30.8/30.8 MB 42.7 MB/s eta 0:00:00\nCollecting scipy==1.8.1\n  Downloading scipy-1.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n      42.2/42.2 MB 31.7 MB/s eta 0:00:00\nCollecting seaborn==0.12.0\n  Downloading seaborn-0.12.0-py3-none-any.whl (285 kB)\n      285.1/285.1 kB 25.6 MB/s eta 0:00:00\nCollecting sentencepiece==0.1.96\n  Downloading sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n      1.2/1.2 MB 59.8 MB/s eta 0:00:00\nCollecting shutilwhich==1.1.0\n  Downloading shutilwhich-1.1.0.tar.gz (2.3 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting sklearn==0.0\n  Downloading sklearn-0.0.tar.gz (1.1 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting tempdir==0.7.1\n  Downloading tempdir-0.7.1.tar.gz (5.9 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting threadpoolctl==3.1.0\n  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\nCollecting tokenizers==0.12.1\n  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n      6.6/6.6 MB 68.8 MB/s eta 0:00:00\nCollecting torch-optimizer==0.3.0\n  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n      61.9/61.9 kB 6.4 MB/s eta 0:00:00\nCollecting tqdm==4.64.0\n  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n      78.4/78.4 kB 8.8 MB/s eta 0:00:00\nCollecting transformers==4.20.0\n  Downloading transformers-4.20.0-py3-none-any.whl (4.4 MB)\n      4.4/4.4 MB 89.3 MB/s eta 0:00:00\nCollecting urllib3==1.26.9\n  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n      139.0/139.0 kB 14.1 MB/s eta 0:00:00\nCollecting xxhash==3.0.0\n  Downloading xxhash-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n      211.2/211.2 kB 21.2 MB/s eta 0:00:00\nCollecting yarl==1.7.2\n  Downloading yarl-1.7.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (304 kB)\n      304.5/304.5 kB 27.2 MB/s eta 0:00:00\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from adapter-transformers==3.0.1->-r /kaggle/working/ProgressivePrompts/condaenv.80gubv5j.requirements.txt (line 1)) (21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from adapter-transformers==3.0.1->-r /kaggle/working/ProgressivePrompts/condaenv.80gubv5j.requirements.txt (line 1)) (1.22.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from aiohttp==3.8.1->-r /kaggle/working/ProgressivePrompts/condaenv.80gubv5j.requirements.txt (line 2)) (21.4.0)\nRequirement already satisfied: six in /opt/conda/envs/nlp/lib/python3.9/site-packages (from data==0.4->-r /kaggle/working/ProgressivePrompts/condaenv.80gubv5j.requirements.txt (line 9)) (1.16.0)\nRequirement already satisfied: decorator in /opt/conda/envs/nlp/lib/python3.9/site-packages (from data==0.4->-r /kaggle/working/ProgressivePrompts/condaenv.80gubv5j.requirements.txt (line 9)) (5.1.1)\nCollecting fsspec[http]>=2021.05.0\n  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n      166.4/166.4 kB 16.8 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from huggingface-hub==0.7.0->-r /kaggle/working/ProgressivePrompts/condaenv.80gubv5j.requirements.txt (line 18)) (4.2.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from matplotlib==3.5.2->-r /kaggle/working/ProgressivePrompts/condaenv.80gubv5j.requirements.txt (line 23)) (9.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/nlp/lib/python3.9/site-packages (from matplotlib==3.5.2->-r /kaggle/working/ProgressivePrompts/condaenv.80gubv5j.requirements.txt (line 23)) (2.8.2)\nRequirement already satisfied: torch in /opt/conda/envs/nlp/lib/python3.9/site-packages (from pytorch-ranger==0.1.1->-r /kaggle/working/ProgressivePrompts/condaenv.80gubv5j.requirements.txt (line 30)) (1.10.1)\n  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n      173.4/173.4 kB 15.4 MB/s eta 0:00:00\n  Downloading fsspec-2023.9.1-py3-none-any.whl (173 kB)\n      173.4/173.4 kB 15.9 MB/s eta 0:00:00\n  Downloading fsspec-2023.9.0-py3-none-any.whl (173 kB)\n      173.2/173.2 kB 16.4 MB/s eta 0:00:00\n  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n      163.8/163.8 kB 16.5 MB/s eta 0:00:00\n  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n      160.1/160.1 kB 17.4 MB/s eta 0:00:00\n  Downloading fsspec-2023.4.0-py3-none-any.whl (153 kB)\n      154.0/154.0 kB 15.4 MB/s eta 0:00:00\n  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n      145.4/145.4 kB 14.5 MB/s eta 0:00:00\n  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n      143.0/143.0 kB 14.6 MB/s eta 0:00:00\n  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n      139.5/139.5 kB 14.4 MB/s eta 0:00:00\n  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n      138.8/138.8 kB 13.5 MB/s eta 0:00:00\n  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n      140.8/140.8 kB 5.1 MB/s eta 0:00:00\n  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n      141.2/141.2 kB 14.5 MB/s eta 0:00:00\n  Downloading fsspec-2022.7.0-py3-none-any.whl (141 kB)\n      141.2/141.2 kB 11.3 MB/s eta 0:00:00\nBuilding wheels for collected packages: data, future, latex, sacremoses, shutilwhich, sklearn, tempdir\n  Building wheel for data (setup.py): started\n  Building wheel for data (setup.py): finished with status 'done'\n  Created wheel for data: filename=data-0.4-py3-none-any.whl size=7227 sha256=f68722cb2264e5688e3ba8f9b073768c0b983f7739b7920c11d33509efa1fb60\n  Stored in directory: /root/.cache/pip/wheels/8a/0b/a3/37ca07d5a2838bba2e475e8090455e40b94631bd57a99a35f4\n  Building wheel for future (setup.py): started\n  Building wheel for future (setup.py): finished with status 'done'\n  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=b9ed7e07e3c02dda9ebd925d90933f8e3cdbad99b630439aedc1b9b2293e085c\n  Stored in directory: /root/.cache/pip/wheels/2f/a0/d3/4030d9f80e6b3be787f19fc911b8e7aa462986a40ab1e4bb94\n  Building wheel for latex (setup.py): started\n  Building wheel for latex (setup.py): finished with status 'done'\n  Created wheel for latex: filename=latex-0.7.0-py3-none-any.whl size=7588 sha256=5d9ccffd3cf2ad372e006a7500749b1c673d7533710346dee2fc2322f38481bb\n  Stored in directory: /root/.cache/pip/wheels/94/84/e5/5ce582523fd479d00356867953085a67c47fbbc86506aa92f8\n  Building wheel for sacremoses (setup.py): started\n  Building wheel for sacremoses (setup.py): finished with status 'done'\n  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=9dcaa5626f7925c962139a3e7e163000a4ba383facd0db2d031c7665570f8d0a\n  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n  Building wheel for shutilwhich (setup.py): started\n  Building wheel for shutilwhich (setup.py): finished with status 'done'\n  Created wheel for shutilwhich: filename=shutilwhich-1.1.0-py3-none-any.whl size=2767 sha256=8346e34e9432a9d70a14c5f5d8890d1d11dee466b1c4ba19f84f08ac0bc0878a\n  Stored in directory: /root/.cache/pip/wheels/84/c7/f5/fed66dce1ed897b44e0da776b6a592dfad0a70f7dd61f73a9d\n  Building wheel for sklearn (setup.py): started\n  Building wheel for sklearn (setup.py): finished with status 'done'\n  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1304 sha256=5b2fbf4a1c028222d214b4c3a5236647fe306f5446fa2fa7abaa093408201d9f\n  Stored in directory: /root/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n  Building wheel for tempdir (setup.py): started\n  Building wheel for tempdir (setup.py): finished with status 'done'\n  Created wheel for tempdir: filename=tempdir-0.7.1-py3-none-any.whl size=2197 sha256=d998dd072681345ed635a72b9030ff9b3f17414b23f9704ca046bbdf65f3f11b\n  Stored in directory: /root/.cache/pip/wheels/31/7b/e3/af441c2f71a48c30809aada978c1433b163a0747e73b5805ca\nSuccessfully built data future latex sacremoses shutilwhich sklearn tempdir\nInstalling collected packages: tokenizers, tempdir, shutilwhich, sentencepiece, pytz, funcsigs, xxhash, urllib3, tqdm, threadpoolctl, scipy, regex, pyyaml, pyarrow, multidict, kiwisolver, joblib, idna, future, fsspec, frozenlist, fonttools, filelock, dill, data, cycler, click, charset-normalizer, certifi, async-timeout, yarl, scikit-learn, sacremoses, requests, pytorch-ranger, pandas, nltk, multiprocess, matplotlib, latex, aiosignal, torch-optimizer, sklearn, seaborn, responses, huggingface-hub, aiohttp, transformers, adapter-transformers, datasets\nSuccessfully installed adapter-transformers-3.0.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 certifi-2022.6.15 charset-normalizer-2.0.12 click-8.1.3 cycler-0.11.0 data-0.4 datasets-2.3.2 dill-0.3.5.1 filelock-3.7.1 fonttools-4.33.3 frozenlist-1.3.0 fsspec-2022.5.0 funcsigs-1.0.2 future-0.18.2 huggingface-hub-0.7.0 idna-3.3 joblib-1.1.0 kiwisolver-1.4.3 latex-0.7.0 matplotlib-3.5.2 multidict-6.0.2 multiprocess-0.70.13 nltk-3.7 pandas-1.4.2 pyarrow-8.0.0 pytorch-ranger-0.1.1 pytz-2022.1 pyyaml-6.0 regex-2022.6.2 requests-2.28.0 responses-0.18.0 sacremoses-0.0.53 scikit-learn-1.1.1 scipy-1.8.1 seaborn-0.12.0 sentencepiece-0.1.96 shutilwhich-1.1.0 sklearn-0.0 tempdir-0.7.1 threadpoolctl-3.1.0 tokenizers-0.12.1 torch-optimizer-0.3.0 tqdm-4.64.0 transformers-4.20.0 urllib3-1.26.9 xxhash-3.0.0 yarl-1.7.2\n\ndone\n#\n# To activate this environment, use\n#\n#     $ conda activate nlp\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd T5_codebase\nimport os\nos.mkdir(\"results\")\nos.mkdir(\"save_checkpoint\")","metadata":{"execution":{"iopub.status.busy":"2023-11-28T07:37:15.506615Z","iopub.execute_input":"2023-11-28T07:37:15.506947Z","iopub.status.idle":"2023-11-28T07:37:15.517776Z","shell.execute_reply.started":"2023-11-28T07:37:15.506912Z","shell.execute_reply":"2023-11-28T07:37:15.516762Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working/ProgressivePrompts/T5_codebase\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list multirc boolq wic mnli cb copa qqp rte imdb sst2 dbpedia_14 ag_news yelp_review_full amazon yahoo_answers_topics \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 10 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 --select_k_per_class 1000 \\\n--save_name large_1000_T5_order_2 --save_dir results","metadata":{"execution":{"iopub.status.busy":"2023-11-26T15:40:45.790659Z","iopub.execute_input":"2023-11-26T15:40:45.791526Z","iopub.status.idle":"2023-11-26T19:48:17.350851Z","shell.execute_reply.started":"2023-11-26T15:40:45.791484Z","shell.execute_reply":"2023-11-26T19:48:17.349565Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading: 100%|| 1.18k/1.18k [00:00<00:00, 1.47MB/s]\nDownloading: 100%|| 2.75G/2.75G [01:56<00:00, 25.3MB/s]\nDownloading: 100%|| 773k/773k [00:00<00:00, 3.18MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nmultirc\nDownloading builder script: 29.9kB [00:00, 19.1MB/s]                            \nDownloading metadata: 38.2kB [00:00, 23.5MB/s]                                  \nDownloading and preparing dataset super_glue/multirc (download: 1.06 MiB, generated: 65.77 MiB, post-processed: Unknown size, total: 66.84 MiB) to /root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 1.12M/1.12M [00:00<00:00, 36.5MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7c26324600d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                  | 0/2000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 2000/2000 [00:06<00:00, 306.23ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 500/500 [00:01<00:00, 330.02ex/s]\n100%|| 500/500 [00:01<00:00, 339.75ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 329.35ex/s]\nboolq\nDownloading and preparing dataset super_glue/boolq (download: 3.93 MiB, generated: 9.92 MiB, post-processed: Unknown size, total: 13.85 MiB) to /root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 4.12M/4.12M [00:00<00:00, 44.2MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:03<00:00, 612.92ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 500/500 [00:00<00:00, 629.92ex/s]\n100%|| 500/500 [00:00<00:00, 603.23ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 672.89ex/s]\nwic\nDownloading and preparing dataset super_glue/wic (download: 386.93 KiB, generated: 906.64 KiB, post-processed: Unknown size, total: 1.26 MiB) to /root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 396k/396k [00:00<00:00, 707kB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1047.84ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 319/319 [00:00<00:00, 1223.59ex/s]\n100%|| 319/319 [00:00<00:00, 1222.94ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 1199.82ex/s]\nmnli\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nDownloading and preparing dataset parquet/LysandreJik--glue-mnli-train to /root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8...\nDownloading data files:   0%|                             | 0/3 [00:00<?, ?it/s]\nDownloading data:   0%|                             | 0.00/51.3M [00:00<?, ?B/s]\u001b[A\nDownloading data:   6%|                  | 2.93M/51.3M [00:00<00:01, 29.3MB/s]\u001b[A\nDownloading data:  17%|                | 8.84M/51.3M [00:00<00:00, 46.8MB/s]\u001b[A\nDownloading data:  29%|              | 14.9M/51.3M [00:00<00:00, 52.9MB/s]\u001b[A\nDownloading data:  41%|           | 20.9M/51.3M [00:00<00:00, 55.9MB/s]\u001b[A\nDownloading data:  53%|         | 27.0M/51.3M [00:00<00:00, 57.8MB/s]\u001b[A\nDownloading data:  65%|       | 33.1M/51.3M [00:00<00:00, 58.9MB/s]\u001b[A\nDownloading data:  76%|    | 39.1M/51.3M [00:00<00:00, 59.3MB/s]\u001b[A\nDownloading data: 100%|| 51.3M/51.3M [00:00<00:00, 57.0MB/s]\u001b[A\nDownloading data files:  33%|              | 1/3 [00:01<00:03,  1.68s/it]\nDownloading data: 100%|| 760k/760k [00:00<00:00, 24.4MB/s]\u001b[A\n\nDownloading data: 100%|| 793k/793k [00:00<00:00, 27.1MB/s]\u001b[A\nDownloading data files:  67%|       | 2/3 [00:03<00:01,  1.62s/it]\nDownloading data: 100%|| 758k/758k [00:00<00:00, 17.9MB/s]\u001b[A\n\nDownloading data: 100%|| 793k/793k [00:00<00:00, 25.2MB/s]\u001b[A\nDownloading data files: 100%|| 3/3 [00:04<00:00,  1.62s/it]\nExtracting data files: 100%|| 3/3 [00:00<00:00, 940.92it/s]\nDataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8. Subsequent calls will reuse this data.\n100%|| 3000/3000 [00:02<00:00, 1057.94ex/s]\nk =  1000   k-val =  500\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nReusing dataset parquet (/root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n100%|| 750/750 [00:00<00:00, 1225.14ex/s]\n100%|| 750/750 [00:00<00:00, 1234.39ex/s]\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nReusing dataset parquet (/root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n100%|| 90/90 [00:00<00:00, 1224.43ex/s]\ncb\nDownloading and preparing dataset super_glue/cb (download: 73.71 KiB, generated: 198.02 KiB, post-processed: Unknown size, total: 271.73 KiB) to /root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 75.5k/75.5k [00:00<00:00, 9.39MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 250/250 [00:00<00:00, 931.36ex/s]\nk =  -1   k-val =  -1\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 28/28 [00:00<00:00, 808.42ex/s]\n100%|| 28/28 [00:00<00:00, 940.10ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 6/6 [00:00<00:00, 862.55ex/s]\ncopa\nDownloading and preparing dataset super_glue/copa (download: 42.96 KiB, generated: 119.62 KiB, post-processed: Unknown size, total: 162.57 KiB) to /root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 44.0k/44.0k [00:00<00:00, 305kB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 400/400 [00:00<00:00, 1259.67ex/s]\nk =  -1   k-val =  -1\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 50/50 [00:00<00:00, 1321.33ex/s]\n100%|| 50/50 [00:00<00:00, 1349.70ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 8/8 [00:00<00:00, 1240.60ex/s]\nqqp\nDownloading builder script: 28.8kB [00:00, 18.7MB/s]                            \nDownloading metadata: 28.7kB [00:00, 17.2MB/s]                                  \nDownloading and preparing dataset glue/qqp (download: 39.76 MiB, generated: 106.55 MiB, post-processed: Unknown size, total: 146.32 MiB) to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 41.7M/41.7M [00:00<00:00, 59.8MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1146.06ex/s]\nk =  1000   k-val =  500\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 500/500 [00:00<00:00, 1393.62ex/s]\n100%|| 500/500 [00:00<00:00, 1357.69ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 40/40 [00:00<00:00, 1339.01ex/s]\nrte\nDownloading and preparing dataset glue/rte (download: 680.81 KiB, generated: 1.83 MiB, post-processed: Unknown size, total: 2.49 MiB) to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 697k/697k [00:00<00:00, 17.5MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2490/2490 [00:02<00:00, 840.67ex/s]\nk =  -1   k-val =  -1\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 138/138 [00:00<00:00, 992.52ex/s]\n100%|| 139/139 [00:00<00:00, 959.11ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 48/48 [00:00<00:00, 916.58ex/s]\nimdb\nDownloading builder script: 4.31kB [00:00, 3.45MB/s]                            \nDownloading metadata: 2.17kB [00:00, 1.95MB/s]                                  \nDownloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\nDownloading data: 100%|| 84.1M/84.1M [00:16<00:00, 5.14MB/s]\nDataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:05<00:00, 364.71ex/s]\nk =  1000   k-val =  500\nReusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n100%|| 500/500 [00:01<00:00, 390.41ex/s]\n100%|| 500/500 [00:01<00:00, 369.46ex/s]\nReusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n100%|| 40/40 [00:00<00:00, 410.79ex/s]\nsst2\nDownloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 7.44M/7.44M [00:00<00:00, 52.8MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1435.19ex/s]\nk =  1000   k-val =  400\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 400/400 [00:00<00:00, 1535.17ex/s]\n100%|| 400/400 [00:00<00:00, 1561.12ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 40/40 [00:00<00:00, 1671.02ex/s]\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 4.44MB/s]                            \nDownloading metadata: 2.47kB [00:00, 2.30MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:04<00:00, 15.9MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n100%|| 14000/14000 [00:16<00:00, 860.84ex/s]\nk =  1000   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 3500/3500 [00:03<00:00, 888.28ex/s]\n100%|| 3500/3500 [00:04<00:00, 865.50ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 1960/1960 [00:02<00:00, 909.87ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 3.23MB/s]                            \nDownloading metadata: 2.65kB [00:00, 2.16MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 62.3MB/s]                                      \nDownloading data: 1.86MB [00:00, 49.0MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|| 4000/4000 [00:03<00:00, 1006.55ex/s]\nk =  1000   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 1000/1000 [00:00<00:00, 1023.17ex/s]\n100%|| 1000/1000 [00:01<00:00, 987.55ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 160/160 [00:00<00:00, 1139.20ex/s]\nyelp_review_full\nDownloading builder script: 4.41kB [00:00, 2.67MB/s]                            \nDownloading metadata: 2.04kB [00:00, 1.38MB/s]                                  \nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\nDownloading data: 100%|| 196M/196M [00:06<00:00, 30.9MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\n100%|| 5000/5000 [00:08<00:00, 561.84ex/s]\nk =  1000   k-val =  500\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 1250/1250 [00:02<00:00, 555.32ex/s]\n100%|| 1250/1250 [00:02<00:00, 564.82ex/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 250/250 [00:00<00:00, 610.49ex/s]\namazon\n  0%|                                                  | 0/5000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 5000/5000 [00:06<00:00, 748.28ex/s]\nk =  1000   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 1250/1250 [00:01<00:00, 760.72ex/s]\n100%|| 1250/1250 [00:01<00:00, 776.14ex/s]\n  0%|                                                   | 0/250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 250/250 [00:00<00:00, 800.29ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.07MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.62MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:11<00:00, 28.5MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 10000/10000 [00:19<00:00, 518.45ex/s]\nk =  1000   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 2500/2500 [00:04<00:00, 525.57ex/s]\n100%|| 2500/2500 [00:04<00:00, 519.14ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 1000/1000 [00:01<00:00, 534.10ex/s]\ntask =  multirc\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n0\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:55<00:00,  2.27it/s]\n0 multirc -> 0.806\n1\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:54<00:00,  2.27it/s]\n1 multirc -> 0.838\n2\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:54<00:00,  2.28it/s]\n2 multirc -> 0.83\n3\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:54<00:00,  2.27it/s]\n3 multirc -> 0.828\n4\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:55<00:00,  2.27it/s]\n4 multirc -> 0.834\n5\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:54<00:00,  2.28it/s]\n5 multirc -> 0.838\n6\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:54<00:00,  2.27it/s]\n6 multirc -> 0.83\n7\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:54<00:00,  2.28it/s]\n7 multirc -> 0.836\n8\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:55<00:00,  2.27it/s]\n8 multirc -> 0.842\n9\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:55<00:00,  2.27it/s]\n9 multirc -> 0.844\nUpdated progressive prompts  torch.Size([10, 1024])\nmultirc [0.806, 0.838, 0.83, 0.828, 0.834, 0.838, 0.83, 0.836, 0.842, 0.844]\nCalculating test acc ...\n/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py:887: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  curr_prompt = torch.tensor(self.previous_prompts, requires_grad=False).to(self.device)\n100%|| 125/125 [00:54<00:00,  2.28it/s]\ntask =  boolq\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n0\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.45it/s]\n0 boolq -> 0.838\n1\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.45it/s]\n1 boolq -> 0.844\n2\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.45it/s]\n2 boolq -> 0.842\n3\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.44it/s]\n3 boolq -> 0.836\n4\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.45it/s]\n4 boolq -> 0.834\n5\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.45it/s]\n5 boolq -> 0.838\n6\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.44it/s]\n6 boolq -> 0.838\n7\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.45it/s]\n7 boolq -> 0.842\n8\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.45it/s]\n8 boolq -> 0.842\n9\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.45it/s]\n9 boolq -> 0.838\nUpdated progressive prompts  torch.Size([20, 1024])\nboolq [0.838, 0.844, 0.842, 0.836, 0.834, 0.838, 0.838, 0.842, 0.842, 0.838]\nCalculating test acc ...\n/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py:887: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  curr_prompt = torch.tensor(self.previous_prompts, requires_grad=False).to(self.device)\n100%|| 125/125 [00:51<00:00,  2.44it/s]\ntask =  wic\nprogressive prompts\n0\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 80/80 [00:32<00:00,  2.45it/s]\n0 wic -> 0.6394984326018809\n1\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 80/80 [00:32<00:00,  2.45it/s]\n1 wic -> 0.670846394984326\n2\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 80/80 [00:32<00:00,  2.45it/s]\n2 wic -> 0.664576802507837\n3\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 80/80 [00:32<00:00,  2.44it/s]\n3 wic -> 0.6269592476489029\n4\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 80/80 [00:32<00:00,  2.45it/s]\n4 wic -> 0.658307210031348\n5\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 80/80 [00:32<00:00,  2.45it/s]\n5 wic -> 0.6677115987460815\n6\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 80/80 [00:32<00:00,  2.45it/s]\n6 wic -> 0.6489028213166145\n7\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 80/80 [00:32<00:00,  2.45it/s]\n7 wic -> 0.6520376175548589\n8\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 80/80 [00:32<00:00,  2.44it/s]\n8 wic -> 0.670846394984326\n9\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 80/80 [00:32<00:00,  2.45it/s]\n9 wic -> 0.6363636363636364\nUpdated progressive prompts  torch.Size([30, 1024])\nwic [0.6394984326018809, 0.670846394984326, 0.664576802507837, 0.6269592476489029, 0.658307210031348, 0.6677115987460815, 0.6489028213166145, 0.6520376175548589, 0.670846394984326, 0.6363636363636364]\nCalculating test acc ...\n100%|| 80/80 [00:32<00:00,  2.46it/s]\ntask =  mnli\nprogressive prompts\n0\n100%|| 750/750 [08:41<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 188/188 [01:30<00:00,  2.08it/s]\n0 mnli -> 0.8613333333333333\n1\n100%|| 750/750 [08:41<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 188/188 [01:30<00:00,  2.09it/s]\n1 mnli -> 0.8626666666666667\n2\n100%|| 750/750 [08:40<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 188/188 [01:30<00:00,  2.08it/s]\n2 mnli -> 0.8626666666666667\n3\n  7%|                                       | 52/750 [00:35<08:04,  1.44it/s]^C\n  7%|                                       | 52/750 [00:36<08:09,  1.43it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 775, in train_one_task\n    loss = self.train_step_lester(batch,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 456, in train_step_lester\n    outputs = model(\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/adapters/context.py\", line 103, in wrapper_func\n    results = f(self, *args, **kwargs)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 1678, in forward\n    decoder_input_ids = self._shift_right(labels)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 855, in _shift_right\n    assert torch.all(shifted_input_ids >= 0).item(), \"Verify that `shifted_input_ids` has only positive values\"\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list multirc boolq wic mnli cb copa qqp rte imdb sst2 dbpedia_14 ag_news yelp_review_full amazon yahoo_answers_topics \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 10 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 --select_k_per_class 1000 \\\n--save_name large_1000_T5_order_2 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-26T21:06:34.907402Z","iopub.execute_input":"2023-11-26T21:06:34.908072Z","iopub.status.idle":"2023-11-27T05:23:07.139987Z","shell.execute_reply.started":"2023-11-26T21:06:34.908038Z","shell.execute_reply":"2023-11-27T05:23:07.138747Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading: 100%|| 1.18k/1.18k [00:00<00:00, 1.33MB/s]\nDownloading: 100%|| 2.75G/2.75G [00:50<00:00, 58.7MB/s]\nDownloading: 100%|| 773k/773k [00:00<00:00, 15.6MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nmultirc\nDownloading builder script: 29.9kB [00:00, 18.5MB/s]                            \nDownloading metadata: 38.2kB [00:00, 25.8MB/s]                                  \nDownloading and preparing dataset super_glue/multirc (download: 1.06 MiB, generated: 65.77 MiB, post-processed: Unknown size, total: 66.84 MiB) to /root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 1.12M/1.12M [00:00<00:00, 21.5MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7a306eb470d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                  | 0/2000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 2000/2000 [00:06<00:00, 306.44ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 500/500 [00:01<00:00, 337.02ex/s]\n100%|| 500/500 [00:01<00:00, 348.08ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 336.71ex/s]\nboolq\nDownloading and preparing dataset super_glue/boolq (download: 3.93 MiB, generated: 9.92 MiB, post-processed: Unknown size, total: 13.85 MiB) to /root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 4.12M/4.12M [00:00<00:00, 36.6MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:03<00:00, 613.82ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 500/500 [00:00<00:00, 693.14ex/s]\n100%|| 500/500 [00:00<00:00, 614.76ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 677.72ex/s]\nwic\nDownloading and preparing dataset super_glue/wic (download: 386.93 KiB, generated: 906.64 KiB, post-processed: Unknown size, total: 1.26 MiB) to /root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 396k/396k [00:00<00:00, 12.3MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1078.25ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 319/319 [00:00<00:00, 1260.97ex/s]\n100%|| 319/319 [00:00<00:00, 1249.56ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 1246.39ex/s]\nmnli\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nDownloading and preparing dataset parquet/LysandreJik--glue-mnli-train to /root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8...\nDownloading data files:   0%|                             | 0/3 [00:00<?, ?it/s]\nDownloading data:   0%|                             | 0.00/51.3M [00:00<?, ?B/s]\u001b[A\nDownloading data:   8%|                  | 4.04M/51.3M [00:00<00:01, 40.4MB/s]\u001b[A\nDownloading data:  19%|                | 9.83M/51.3M [00:00<00:00, 50.7MB/s]\u001b[A\nDownloading data:  31%|              | 15.7M/51.3M [00:00<00:00, 54.3MB/s]\u001b[A\nDownloading data:  42%|           | 21.6M/51.3M [00:00<00:00, 56.1MB/s]\u001b[A\nDownloading data:  53%|         | 27.4M/51.3M [00:00<00:00, 57.0MB/s]\u001b[A\nDownloading data:  65%|       | 33.6M/51.3M [00:00<00:00, 58.5MB/s]\u001b[A\nDownloading data:  78%|    | 39.8M/51.3M [00:00<00:00, 59.6MB/s]\u001b[A\nDownloading data: 100%|| 51.3M/51.3M [00:00<00:00, 57.8MB/s]\u001b[A\nDownloading data files:  33%|              | 1/3 [00:01<00:02,  1.33s/it]\nDownloading data: 100%|| 760k/760k [00:00<00:00, 15.0MB/s]\u001b[A\n\nDownloading data: 100%|| 793k/793k [00:00<00:00, 18.3MB/s]\u001b[A\nDownloading data files:  67%|       | 2/3 [00:02<00:01,  1.08s/it]\nDownloading data: 100%|| 758k/758k [00:00<00:00, 20.5MB/s]\u001b[A\n\nDownloading data: 100%|| 793k/793k [00:00<00:00, 17.9MB/s]\u001b[A\nDownloading data files: 100%|| 3/3 [00:03<00:00,  1.01s/it]\nExtracting data files: 100%|| 3/3 [00:00<00:00, 859.55it/s]\nDataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8. Subsequent calls will reuse this data.\n100%|| 3000/3000 [00:02<00:00, 1047.13ex/s]\nk =  1000   k-val =  500\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nReusing dataset parquet (/root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n100%|| 750/750 [00:00<00:00, 1265.22ex/s]\n100%|| 750/750 [00:00<00:00, 1270.92ex/s]\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nReusing dataset parquet (/root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n100%|| 90/90 [00:00<00:00, 1261.07ex/s]\ncb\nDownloading and preparing dataset super_glue/cb (download: 73.71 KiB, generated: 198.02 KiB, post-processed: Unknown size, total: 271.73 KiB) to /root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 75.5k/75.5k [00:00<00:00, 4.83MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 250/250 [00:00<00:00, 939.75ex/s]\nk =  -1   k-val =  -1\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 28/28 [00:00<00:00, 822.45ex/s]\n100%|| 28/28 [00:00<00:00, 932.78ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 6/6 [00:00<00:00, 892.41ex/s]\ncopa\nDownloading and preparing dataset super_glue/copa (download: 42.96 KiB, generated: 119.62 KiB, post-processed: Unknown size, total: 162.57 KiB) to /root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 44.0k/44.0k [00:00<00:00, 5.76MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 400/400 [00:00<00:00, 1413.59ex/s]\nk =  -1   k-val =  -1\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 50/50 [00:00<00:00, 1367.04ex/s]\n100%|| 50/50 [00:00<00:00, 1393.20ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 8/8 [00:00<00:00, 1310.11ex/s]\nqqp\nDownloading builder script: 28.8kB [00:00, 17.4MB/s]                            \nDownloading metadata: 28.7kB [00:00, 19.9MB/s]                                  \nDownloading and preparing dataset glue/qqp (download: 39.76 MiB, generated: 106.55 MiB, post-processed: Unknown size, total: 146.32 MiB) to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 41.7M/41.7M [00:00<00:00, 57.2MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1176.10ex/s]\nk =  1000   k-val =  500\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 500/500 [00:00<00:00, 1355.81ex/s]\n100%|| 500/500 [00:00<00:00, 1285.22ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 40/40 [00:00<00:00, 1409.64ex/s]\nrte\nDownloading and preparing dataset glue/rte (download: 680.81 KiB, generated: 1.83 MiB, post-processed: Unknown size, total: 2.49 MiB) to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 697k/697k [00:00<00:00, 16.9MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2490/2490 [00:02<00:00, 867.71ex/s]\nk =  -1   k-val =  -1\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 138/138 [00:00<00:00, 1031.10ex/s]\n100%|| 139/139 [00:00<00:00, 993.60ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 48/48 [00:00<00:00, 910.53ex/s]\nimdb\nDownloading builder script: 4.31kB [00:00, 3.67MB/s]                            \nDownloading metadata: 2.17kB [00:00, 1.84MB/s]                                  \nDownloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\nDownloading data: 100%|| 84.1M/84.1M [00:05<00:00, 14.4MB/s]\nDataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:05<00:00, 365.08ex/s]\nk =  1000   k-val =  500\nReusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n100%|| 500/500 [00:01<00:00, 405.26ex/s]\n100%|| 500/500 [00:01<00:00, 382.13ex/s]\nReusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n100%|| 40/40 [00:00<00:00, 412.55ex/s]\nsst2\nDownloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 7.44M/7.44M [00:00<00:00, 50.2MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1386.73ex/s]\nk =  1000   k-val =  400\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 400/400 [00:00<00:00, 1552.14ex/s]\n100%|| 400/400 [00:00<00:00, 1587.52ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 40/40 [00:00<00:00, 1797.68ex/s]\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 4.57MB/s]                            \nDownloading metadata: 2.47kB [00:00, 2.43MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:02<00:00, 32.2MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\n100%|| 5000/5000 [00:08<00:00, 565.23ex/s]\nk =  1000   k-val =  500\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 1250/1250 [00:02<00:00, 571.55ex/s]\n100%|| 1250/1250 [00:02<00:00, 586.67ex/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 250/250 [00:00<00:00, 598.77ex/s]\namazon\n  0%|                                                  | 0/5000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 5000/5000 [00:06<00:00, 767.96ex/s]\nk =  1000   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 1250/1250 [00:01<00:00, 768.57ex/s]\n100%|| 1250/1250 [00:01<00:00, 791.52ex/s]\n  0%|                                                   | 0/250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 250/250 [00:00<00:00, 864.08ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.81MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.65MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:08<00:00, 37.1MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 10000/10000 [00:18<00:00, 535.25ex/s]\nk =  1000   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 2500/2500 [00:04<00:00, 535.21ex/s]\n100%|| 2500/2500 [00:04<00:00, 540.46ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 1000/1000 [00:01<00:00, 541.46ex/s]\ntask =  mnli\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n3\n100%|| 750/750 [08:42<00:00,  1.43it/s]\ntorch.Size([10, 1024])\n100%|| 188/188 [01:29<00:00,  2.09it/s]\n3 mnli -> 0.8826666666666667\n4\n100%|| 750/750 [08:41<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 188/188 [01:30<00:00,  2.09it/s]\n4 mnli -> 0.876\n5\n100%|| 750/750 [08:41<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 188/188 [01:29<00:00,  2.10it/s]\n5 mnli -> 0.868\n6\n100%|| 750/750 [08:41<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 188/188 [01:29<00:00,  2.10it/s]\n6 mnli -> 0.8706666666666667\n7\n100%|| 750/750 [08:41<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 188/188 [01:29<00:00,  2.10it/s]\n7 mnli -> 0.876\n8\n100%|| 750/750 [08:41<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 188/188 [01:29<00:00,  2.11it/s]\n8 mnli -> 0.8746666666666667\n9\n100%|| 750/750 [08:41<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 188/188 [01:29<00:00,  2.10it/s]\n9 mnli -> 0.8746666666666667\nUpdated progressive prompts  torch.Size([40, 1024])\nmnli [0.8613333333333333, 0.8626666666666667, 0.8626666666666667, 0.8826666666666667, 0.876, 0.868, 0.8706666666666667, 0.876, 0.8746666666666667, 0.8746666666666667]\nCalculating test acc ...\n/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py:887: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  curr_prompt = torch.tensor(self.previous_prompts, requires_grad=False).to(self.device)\n100%|| 188/188 [01:29<00:00,  2.11it/s]\ntask =  cb\nprogressive prompts\n0\n100%|| 63/63 [00:43<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 7/7 [00:03<00:00,  2.04it/s]\n0 cb -> 0.9113445378151261\n1\n100%|| 63/63 [00:43<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 7/7 [00:03<00:00,  2.05it/s]\n1 cb -> 0.9113445378151261\n2\n100%|| 63/63 [00:43<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 7/7 [00:03<00:00,  2.04it/s]\n2 cb -> 0.8671299460773145\n3\n100%|| 63/63 [00:43<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 7/7 [00:03<00:00,  2.05it/s]\n3 cb -> 0.8347727980080921\n4\n100%|| 63/63 [00:43<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 7/7 [00:03<00:00,  2.04it/s]\n4 cb -> 0.8347727980080921\n5\n100%|| 63/63 [00:43<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 7/7 [00:03<00:00,  2.06it/s]\n5 cb -> 0.8854497354497355\n6\n100%|| 63/63 [00:43<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 7/7 [00:03<00:00,  2.06it/s]\n6 cb -> 0.8671299460773145\n7\n100%|| 63/63 [00:43<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 7/7 [00:03<00:00,  2.06it/s]\n7 cb -> 0.9437590187590188\n8\n100%|| 63/63 [00:43<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 7/7 [00:03<00:00,  2.05it/s]\n8 cb -> 1.0\n9\n100%|| 63/63 [00:43<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 7/7 [00:03<00:00,  2.05it/s]\n9 cb -> 1.0\nUpdated progressive prompts  torch.Size([50, 1024])\ncb [0.9113445378151261, 0.9113445378151261, 0.8671299460773145, 0.8347727980080921, 0.8347727980080921, 0.8854497354497355, 0.8671299460773145, 0.9437590187590188, 1.0, 1.0]\nCalculating test acc ...\n/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py:887: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  curr_prompt = torch.tensor(self.previous_prompts, requires_grad=False).to(self.device)\n100%|| 7/7 [00:03<00:00,  2.04it/s]\ntask =  copa\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n0\n100%|| 100/100 [01:09<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 13/13 [00:05<00:00,  2.46it/s]\n0 copa -> 0.62\n1\n100%|| 100/100 [01:09<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 13/13 [00:05<00:00,  2.46it/s]\n1 copa -> 0.62\n2\n100%|| 100/100 [01:09<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 13/13 [00:05<00:00,  2.45it/s]\n2 copa -> 0.62\n3\n100%|| 100/100 [01:09<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 13/13 [00:05<00:00,  2.44it/s]\n3 copa -> 0.62\n4\n100%|| 100/100 [01:09<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 13/13 [00:05<00:00,  2.46it/s]\n4 copa -> 0.62\n5\n100%|| 100/100 [01:09<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 13/13 [00:05<00:00,  2.46it/s]\n5 copa -> 0.62\n6\n100%|| 100/100 [01:09<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 13/13 [00:05<00:00,  2.46it/s]\n6 copa -> 0.62\n7\n100%|| 100/100 [01:09<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 13/13 [00:05<00:00,  2.46it/s]\n7 copa -> 0.62\n8\n100%|| 100/100 [01:09<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 13/13 [00:05<00:00,  2.44it/s]\n8 copa -> 0.62\n9\n100%|| 100/100 [01:09<00:00,  1.45it/s]\ntorch.Size([10, 1024])\n100%|| 13/13 [00:05<00:00,  2.45it/s]\n9 copa -> 0.62\nUpdated progressive prompts  torch.Size([60, 1024])\ncopa [0.62, 0.62, 0.62, 0.62, 0.62, 0.62, 0.62, 0.62, 0.62, 0.62]\nCalculating test acc ...\n/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py:887: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  curr_prompt = torch.tensor(self.previous_prompts, requires_grad=False).to(self.device)\n100%|| 13/13 [00:05<00:00,  2.55it/s]\ntask =  qqp\nprogressive prompts\n0\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [01:00<00:00,  2.06it/s]\n0 qqp -> 0.894\n1\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:59<00:00,  2.09it/s]\n1 qqp -> 0.83\n2\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:59<00:00,  2.09it/s]\n2 qqp -> 0.834\n3\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:59<00:00,  2.08it/s]\n3 qqp -> 0.858\n4\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [01:00<00:00,  2.07it/s]\n4 qqp -> 0.872\n5\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:59<00:00,  2.09it/s]\n5 qqp -> 0.866\n6\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [01:00<00:00,  2.06it/s]\n6 qqp -> 0.876\n7\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [01:00<00:00,  2.07it/s]\n7 qqp -> 0.864\n8\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [01:00<00:00,  2.08it/s]\n8 qqp -> 0.876\n9\n100%|| 500/500 [05:47<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [01:00<00:00,  2.06it/s]\n9 qqp -> 0.884\nUpdated progressive prompts  torch.Size([70, 1024])\nqqp [0.894, 0.83, 0.834, 0.858, 0.872, 0.866, 0.876, 0.864, 0.876, 0.884]\nCalculating test acc ...\n100%|| 125/125 [01:01<00:00,  2.05it/s]\ntask =  rte\nprogressive prompts\n0\n100%|| 623/623 [07:12<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 35/35 [00:17<00:00,  2.04it/s]\n0 rte -> 0.5144927536231884\n1\n100%|| 623/623 [07:12<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 35/35 [00:17<00:00,  2.05it/s]\n1 rte -> 0.7608695652173914\n2\n100%|| 623/623 [07:12<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 35/35 [00:17<00:00,  2.04it/s]\n2 rte -> 0.7898550724637681\n3\n100%|| 623/623 [07:13<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 35/35 [00:17<00:00,  2.04it/s]\n3 rte -> 0.8115942028985508\n4\n100%|| 623/623 [07:12<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 35/35 [00:17<00:00,  2.04it/s]\n4 rte -> 0.8333333333333334\n5\n100%|| 623/623 [07:12<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 35/35 [00:17<00:00,  2.05it/s]\n5 rte -> 0.8405797101449275\n6\n100%|| 623/623 [07:12<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 35/35 [00:17<00:00,  2.05it/s]\n6 rte -> 0.8405797101449275\n7\n100%|| 623/623 [07:12<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 35/35 [00:17<00:00,  2.05it/s]\n7 rte -> 0.8478260869565217\n8\n100%|| 623/623 [07:12<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 35/35 [00:17<00:00,  2.05it/s]\n8 rte -> 0.8478260869565217\n9\n100%|| 623/623 [07:12<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 35/35 [00:17<00:00,  2.05it/s]\n9 rte -> 0.8478260869565217\nUpdated progressive prompts  torch.Size([80, 1024])\nrte [0.5144927536231884, 0.7608695652173914, 0.7898550724637681, 0.8115942028985508, 0.8333333333333334, 0.8405797101449275, 0.8405797101449275, 0.8478260869565217, 0.8478260869565217, 0.8478260869565217]\nCalculating test acc ...\n100%|| 35/35 [00:17<00:00,  2.04it/s]\ntask =  imdb\nprogressive prompts\n0\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:50<00:00,  2.45it/s]\n0 imdb -> 0.738\n1\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.45it/s]\n1 imdb -> 0.938\n2\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:50<00:00,  2.45it/s]\n2 imdb -> 0.932\n3\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.44it/s]\n3 imdb -> 0.938\n4\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.44it/s]\n4 imdb -> 0.952\n5\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.44it/s]\n5 imdb -> 0.948\n6\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.44it/s]\n6 imdb -> 0.956\n7\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.45it/s]\n7 imdb -> 0.956\n8\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.44it/s]\n8 imdb -> 0.952\n9\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 125/125 [00:51<00:00,  2.44it/s]\n9 imdb -> 0.952\nUpdated progressive prompts  torch.Size([90, 1024])\nimdb [0.738, 0.938, 0.932, 0.938, 0.952, 0.948, 0.956, 0.956, 0.952, 0.952]\nCalculating test acc ...\n100%|| 125/125 [00:51<00:00,  2.45it/s]\ntask =  sst2\nprogressive prompts\n0\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 100/100 [00:40<00:00,  2.44it/s]\n0 sst2 -> 0.935\n1\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 100/100 [00:40<00:00,  2.45it/s]\n1 sst2 -> 0.94\n2\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 100/100 [00:40<00:00,  2.46it/s]\n2 sst2 -> 0.94\n3\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 100/100 [00:40<00:00,  2.45it/s]\n3 sst2 -> 0.9475\n4\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 100/100 [00:40<00:00,  2.45it/s]\n4 sst2 -> 0.9425\n5\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 100/100 [00:40<00:00,  2.45it/s]\n5 sst2 -> 0.95\n6\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 100/100 [00:40<00:00,  2.45it/s]\n6 sst2 -> 0.95\n7\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 100/100 [00:40<00:00,  2.45it/s]\n7 sst2 -> 0.95\n8\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 100/100 [00:40<00:00,  2.46it/s]\n8 sst2 -> 0.955\n9\n100%|| 500/500 [05:46<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 100/100 [00:40<00:00,  2.45it/s]\n9 sst2 -> 0.945\nUpdated progressive prompts  torch.Size([100, 1024])\nsst2 [0.935, 0.94, 0.94, 0.9475, 0.9425, 0.95, 0.95, 0.95, 0.955, 0.945]\nCalculating test acc ...\n100%|| 100/100 [00:40<00:00,  2.46it/s]\ntask =  dbpedia_14\nprogressive prompts\n0\n100%|| 3500/3500 [40:32<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 875/875 [06:45<00:00,  2.16it/s]\n0 dbpedia_14 -> 0.8014285714285714\n1\n100%|| 3500/3500 [40:32<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 875/875 [06:51<00:00,  2.13it/s]\n1 dbpedia_14 -> 0.914\n2\n 23%|                              | 809/3500 [09:22<31:12,  1.44it/s]^C\n 23%|                              | 809/3500 [09:22<31:10,  1.44it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in train_one_task\n    batch = {k:batch[k].to('cuda') for k in batch}\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in <dictcomp>\n    batch = {k:batch[k].to('cuda') for k in batch}\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list multirc boolq wic mnli cb copa qqp rte imdb sst2 dbpedia_14 ag_news yelp_review_full amazon yahoo_answers_topics \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 10 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 --select_k_per_class 1000 \\\n--save_name large_1000_T5_order_2 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-27T06:32:11.636583Z","iopub.execute_input":"2023-11-27T06:32:11.636911Z","iopub.status.idle":"2023-11-27T17:28:57.319842Z","shell.execute_reply.started":"2023-11-27T06:32:11.636886Z","shell.execute_reply":"2023-11-27T17:28:57.318638Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading: 100%|| 1.18k/1.18k [00:00<00:00, 1.00MB/s]\nDownloading: 100%|| 2.75G/2.75G [02:42<00:00, 18.2MB/s]\nDownloading: 100%|| 773k/773k [00:00<00:00, 1.01MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nmultirc\nDownloading builder script: 29.9kB [00:00, 14.7MB/s]                            \nDownloading metadata: 38.2kB [00:00, 16.6MB/s]                                  \nDownloading and preparing dataset super_glue/multirc (download: 1.06 MiB, generated: 65.77 MiB, post-processed: Unknown size, total: 66.84 MiB) to /root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 1.12M/1.12M [00:00<00:00, 2.25MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7ae4d0bc50d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                  | 0/2000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 2000/2000 [00:06<00:00, 295.89ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 500/500 [00:01<00:00, 314.47ex/s]\n100%|| 500/500 [00:01<00:00, 329.84ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 308.00ex/s]\nboolq\nDownloading and preparing dataset super_glue/boolq (download: 3.93 MiB, generated: 9.92 MiB, post-processed: Unknown size, total: 13.85 MiB) to /root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 4.12M/4.12M [00:00<00:00, 38.7MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:03<00:00, 589.08ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 500/500 [00:00<00:00, 673.82ex/s]\n100%|| 500/500 [00:00<00:00, 581.12ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 590.63ex/s]\nwic\nDownloading and preparing dataset super_glue/wic (download: 386.93 KiB, generated: 906.64 KiB, post-processed: Unknown size, total: 1.26 MiB) to /root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 396k/396k [00:00<00:00, 22.8MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1023.13ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 319/319 [00:00<00:00, 1194.03ex/s]\n100%|| 319/319 [00:00<00:00, 1199.81ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 1196.13ex/s]\nmnli\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nDownloading and preparing dataset parquet/LysandreJik--glue-mnli-train to /root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8...\nDownloading data files:   0%|                             | 0/3 [00:00<?, ?it/s]\nDownloading data:   0%|                             | 0.00/51.3M [00:00<?, ?B/s]\u001b[A\nDownloading data:   9%|                  | 4.76M/51.3M [00:00<00:00, 47.6MB/s]\u001b[A\nDownloading data:  20%|                | 10.5M/51.3M [00:00<00:00, 53.3MB/s]\u001b[A\nDownloading data:  32%|             | 16.3M/51.3M [00:00<00:00, 55.3MB/s]\u001b[A\nDownloading data:  43%|           | 22.0M/51.3M [00:00<00:00, 56.1MB/s]\u001b[A\nDownloading data:  54%|         | 27.6M/51.3M [00:00<00:00, 55.2MB/s]\u001b[A\nDownloading data:  65%|       | 33.1M/51.3M [00:00<00:00, 52.6MB/s]\u001b[A\nDownloading data:  76%|    | 38.9M/51.3M [00:00<00:00, 54.2MB/s]\u001b[A\nDownloading data:  86%|  | 44.3M/51.3M [00:00<00:00, 54.3MB/s]\u001b[A\nDownloading data: 100%|| 51.3M/51.3M [00:00<00:00, 54.1MB/s]\u001b[A\nDownloading data files:  33%|              | 1/3 [00:01<00:02,  1.49s/it]\nDownloading data: 100%|| 760k/760k [00:00<00:00, 34.5MB/s]\u001b[A\n\nDownloading data: 100%|| 793k/793k [00:00<00:00, 18.8MB/s]\u001b[A\nDownloading data files:  67%|       | 2/3 [00:02<00:01,  1.31s/it]\nDownloading data: 100%|| 758k/758k [00:00<00:00, 10.0MB/s]\u001b[A\n\nDownloading data: 100%|| 793k/793k [00:00<00:00, 35.0MB/s]\u001b[A\nDownloading data files: 100%|| 3/3 [00:03<00:00,  1.29s/it]\nExtracting data files: 100%|| 3/3 [00:00<00:00, 888.50it/s]\nDataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8. Subsequent calls will reuse this data.\n100%|| 3000/3000 [00:02<00:00, 1031.65ex/s]\nk =  1000   k-val =  500\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nReusing dataset parquet (/root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n100%|| 750/750 [00:00<00:00, 1220.70ex/s]\n100%|| 750/750 [00:00<00:00, 1199.54ex/s]\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nReusing dataset parquet (/root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n100%|| 90/90 [00:00<00:00, 1193.95ex/s]\ncb\nDownloading and preparing dataset super_glue/cb (download: 73.71 KiB, generated: 198.02 KiB, post-processed: Unknown size, total: 271.73 KiB) to /root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 75.5k/75.5k [00:00<00:00, 10.2MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 250/250 [00:00<00:00, 931.28ex/s]\nk =  -1   k-val =  -1\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 28/28 [00:00<00:00, 698.56ex/s]\n100%|| 28/28 [00:00<00:00, 788.09ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 6/6 [00:00<00:00, 842.15ex/s]\ncopa\nDownloading and preparing dataset super_glue/copa (download: 42.96 KiB, generated: 119.62 KiB, post-processed: Unknown size, total: 162.57 KiB) to /root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 44.0k/44.0k [00:00<00:00, 8.70MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 400/400 [00:00<00:00, 1372.18ex/s]\nk =  -1   k-val =  -1\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 50/50 [00:00<00:00, 1276.88ex/s]\n100%|| 50/50 [00:00<00:00, 1364.53ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 8/8 [00:00<00:00, 1211.53ex/s]\nqqp\nDownloading builder script: 28.8kB [00:00, 14.5MB/s]                            \nDownloading metadata: 28.7kB [00:00, 14.8MB/s]                                  \nDownloading and preparing dataset glue/qqp (download: 39.76 MiB, generated: 106.55 MiB, post-processed: Unknown size, total: 146.32 MiB) to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 41.7M/41.7M [00:00<00:00, 44.9MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1102.34ex/s]\nk =  1000   k-val =  500\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 500/500 [00:00<00:00, 1273.09ex/s]\n100%|| 500/500 [00:00<00:00, 1356.29ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 40/40 [00:00<00:00, 1322.21ex/s]\nrte\nDownloading and preparing dataset glue/rte (download: 680.81 KiB, generated: 1.83 MiB, post-processed: Unknown size, total: 2.49 MiB) to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 697k/697k [00:00<00:00, 16.7MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2490/2490 [00:02<00:00, 841.56ex/s]\nk =  -1   k-val =  -1\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 138/138 [00:00<00:00, 977.08ex/s]\n100%|| 139/139 [00:00<00:00, 944.60ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 48/48 [00:00<00:00, 909.78ex/s]\nimdb\nDownloading builder script: 4.31kB [00:00, 2.56MB/s]                            \nDownloading metadata: 2.17kB [00:00, 1.14MB/s]                                  \nDownloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\nDownloading data: 100%|| 84.1M/84.1M [00:17<00:00, 4.84MB/s]\nDataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:05<00:00, 357.66ex/s]\nk =  1000   k-val =  500\nReusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n100%|| 500/500 [00:01<00:00, 385.77ex/s]\n100%|| 500/500 [00:01<00:00, 364.27ex/s]\nReusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n100%|| 40/40 [00:00<00:00, 400.77ex/s]\nsst2\nDownloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 7.44M/7.44M [00:00<00:00, 49.8MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1415.91ex/s]\nk =  1000   k-val =  400\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 400/400 [00:00<00:00, 1523.01ex/s]\n100%|| 400/400 [00:00<00:00, 1549.15ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 40/40 [00:00<00:00, 1564.47ex/s]\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 2.97MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.54MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:05<00:00, 13.0MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n100%|| 14000/14000 [00:16<00:00, 843.60ex/s]\nk =  1000   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 3500/3500 [00:04<00:00, 859.52ex/s]\n100%|| 3500/3500 [00:04<00:00, 843.72ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 1960/1960 [00:02<00:00, 885.80ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 2.36MB/s]                            \nDownloading metadata: 2.65kB [00:00, 1.44MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 48.5MB/s]                                      \nDownloading data: 1.86MB [00:00, 10.2MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|| 4000/4000 [00:03<00:00, 1001.40ex/s]\nk =  1000   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 1000/1000 [00:01<00:00, 998.44ex/s]\n100%|| 1000/1000 [00:01<00:00, 998.60ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 160/160 [00:00<00:00, 1003.48ex/s]\nyelp_review_full\nDownloading builder script: 4.41kB [00:00, 2.11MB/s]                            \nDownloading metadata: 2.04kB [00:00, 1.16MB/s]                                  \nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\nDownloading data: 100%|| 196M/196M [00:12<00:00, 15.4MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\n100%|| 5000/5000 [00:09<00:00, 549.09ex/s]\nk =  1000   k-val =  500\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 1250/1250 [00:02<00:00, 556.50ex/s]\n100%|| 1250/1250 [00:02<00:00, 560.76ex/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 250/250 [00:00<00:00, 614.41ex/s]\namazon\n  0%|                                                  | 0/5000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 5000/5000 [00:06<00:00, 720.24ex/s]\nk =  1000   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 1250/1250 [00:01<00:00, 746.10ex/s]\n100%|| 1250/1250 [00:01<00:00, 759.86ex/s]\n  0%|                                                   | 0/250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 250/250 [00:00<00:00, 775.80ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 1.97MB/s]                            \nDownloading metadata: 1.88kB [00:00, 950kB/s]                                   \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:25<00:00, 12.6MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 10000/10000 [00:19<00:00, 514.33ex/s]\nk =  1000   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 2500/2500 [00:05<00:00, 496.28ex/s]\n100%|| 2500/2500 [00:04<00:00, 516.11ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 1000/1000 [00:01<00:00, 528.16ex/s]\ntask =  dbpedia_14\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n2\n100%|| 3500/3500 [40:36<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 875/875 [06:57<00:00,  2.10it/s]\n2 dbpedia_14 -> 0.9708571428571429\n3\n100%|| 3500/3500 [40:36<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 875/875 [06:59<00:00,  2.09it/s]\n3 dbpedia_14 -> 0.9797142857142858\n4\n100%|| 3500/3500 [40:35<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 875/875 [06:59<00:00,  2.08it/s]\n4 dbpedia_14 -> 0.9802857142857143\n5\n100%|| 3500/3500 [40:35<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 875/875 [07:00<00:00,  2.08it/s]\n5 dbpedia_14 -> 0.9797142857142858\n6\n100%|| 3500/3500 [40:35<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 875/875 [07:00<00:00,  2.08it/s]\n6 dbpedia_14 -> 0.982\n7\n100%|| 3500/3500 [40:34<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 875/875 [06:56<00:00,  2.10it/s]\n7 dbpedia_14 -> 0.9837142857142858\n8\n100%|| 3500/3500 [40:34<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 875/875 [06:55<00:00,  2.11it/s]\n8 dbpedia_14 -> 0.984\n9\n100%|| 3500/3500 [40:34<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 875/875 [06:55<00:00,  2.11it/s]\n9 dbpedia_14 -> 0.9842857142857143\nUpdated progressive prompts  torch.Size([110, 1024])\ndbpedia_14 [0.8014285714285714, 0.914, 0.9708571428571429, 0.9797142857142858, 0.9802857142857143, 0.9797142857142858, 0.982, 0.9837142857142858, 0.984, 0.9842857142857143]\nCalculating test acc ...\n/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py:887: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  curr_prompt = torch.tensor(self.previous_prompts, requires_grad=False).to(self.device)\n100%|| 875/875 [06:51<00:00,  2.13it/s]\ntask =  ag_news\nprogressive prompts\n0\n100%|| 1000/1000 [11:33<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 250/250 [01:42<00:00,  2.44it/s]\n0 ag_news -> 0.811\n1\n100%|| 1000/1000 [11:33<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 250/250 [01:41<00:00,  2.45it/s]\n1 ag_news -> 0.867\n2\n100%|| 1000/1000 [11:33<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 250/250 [01:42<00:00,  2.44it/s]\n2 ag_news -> 0.883\n3\n100%|| 1000/1000 [11:33<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 250/250 [01:42<00:00,  2.45it/s]\n3 ag_news -> 0.886\n4\n100%|| 1000/1000 [11:33<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 250/250 [01:41<00:00,  2.46it/s]\n4 ag_news -> 0.887\n5\n100%|| 1000/1000 [11:33<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 250/250 [01:42<00:00,  2.45it/s]\n5 ag_news -> 0.896\n6\n100%|| 1000/1000 [11:33<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 250/250 [01:42<00:00,  2.45it/s]\n6 ag_news -> 0.891\n7\n100%|| 1000/1000 [11:33<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 250/250 [01:42<00:00,  2.43it/s]\n7 ag_news -> 0.893\n8\n100%|| 1000/1000 [11:33<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 250/250 [01:42<00:00,  2.44it/s]\n8 ag_news -> 0.893\n9\n100%|| 1000/1000 [11:33<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 250/250 [01:42<00:00,  2.43it/s]\n9 ag_news -> 0.897\nUpdated progressive prompts  torch.Size([120, 1024])\nag_news [0.811, 0.867, 0.883, 0.886, 0.887, 0.896, 0.891, 0.893, 0.893, 0.897]\nCalculating test acc ...\n100%|| 250/250 [01:42<00:00,  2.44it/s]\ntask =  yelp_review_full\nprogressive prompts\n0\n100%|| 1250/1250 [14:27<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:08<00:00,  2.43it/s]\n0 yelp_review_full -> 0.4864\n1\n100%|| 1250/1250 [14:27<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:08<00:00,  2.43it/s]\n1 yelp_review_full -> 0.5208\n2\n100%|| 1250/1250 [14:27<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:09<00:00,  2.42it/s]\n2 yelp_review_full -> 0.5312\n3\n100%|| 1250/1250 [14:26<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:08<00:00,  2.44it/s]\n3 yelp_review_full -> 0.54\n4\n100%|| 1250/1250 [14:26<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:08<00:00,  2.44it/s]\n4 yelp_review_full -> 0.5688\n5\n100%|| 1250/1250 [14:26<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:08<00:00,  2.43it/s]\n5 yelp_review_full -> 0.544\n6\n100%|| 1250/1250 [14:26<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:08<00:00,  2.43it/s]\n6 yelp_review_full -> 0.5688\n7\n  3%|                                        | 32/1250 [00:21<14:05,  1.44it/s]^C\n  3%|                                        | 32/1250 [00:22<14:06,  1.44it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 775, in train_one_task\n    loss = self.train_step_lester(batch,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 447, in train_step_lester\n    encoder_outputs = model.encoder(\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 1066, in forward\n    layer_outputs = layer_module(\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 695, in forward\n    self_attention_outputs = self.layer[0](\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 595, in forward\n    attention_output = self.SelfAttention(\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 519, in forward\n    value_states = project(\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 496, in project\n    hidden_states = shape(proj_layer(hidden_states))\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 103, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/functional.py\", line 1848, in linear\n    return torch._C._nn.linear(input, weight, bias)\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list multirc boolq wic mnli cb copa qqp rte imdb sst2 dbpedia_14 ag_news yelp_review_full amazon yahoo_answers_topics \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 10 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 --select_k_per_class 1000 \\\n--save_name large_1000_T5_order_2 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-27T18:38:22.678417Z","iopub.execute_input":"2023-11-27T18:38:22.678675Z","iopub.status.idle":"2023-11-28T03:09:15.128724Z","shell.execute_reply.started":"2023-11-27T18:38:22.678652Z","shell.execute_reply":"2023-11-28T03:09:15.127555Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading: 100%|| 1.18k/1.18k [00:00<00:00, 937kB/s]\nDownloading: 100%|| 2.75G/2.75G [01:11<00:00, 41.5MB/s]\nDownloading: 100%|| 773k/773k [00:00<00:00, 1.01MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nmultirc\nDownloading builder script: 29.9kB [00:00, 19.3MB/s]                            \nDownloading metadata: 38.2kB [00:00, 21.7MB/s]                                  \nDownloading and preparing dataset super_glue/multirc (download: 1.06 MiB, generated: 65.77 MiB, post-processed: Unknown size, total: 66.84 MiB) to /root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 1.12M/1.12M [00:00<00:00, 49.0MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7c021ac1a0d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                  | 0/2000 [00:00<?, ?ex/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n100%|| 2000/2000 [00:06<00:00, 288.20ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 500/500 [00:01<00:00, 323.13ex/s]\n100%|| 500/500 [00:01<00:00, 329.10ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 321.59ex/s]\nboolq\nDownloading and preparing dataset super_glue/boolq (download: 3.93 MiB, generated: 9.92 MiB, post-processed: Unknown size, total: 13.85 MiB) to /root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 4.12M/4.12M [00:00<00:00, 53.4MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:03<00:00, 580.25ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 500/500 [00:00<00:00, 632.76ex/s]\n100%|| 500/500 [00:00<00:00, 571.59ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 640.53ex/s]\nwic\nDownloading and preparing dataset super_glue/wic (download: 386.93 KiB, generated: 906.64 KiB, post-processed: Unknown size, total: 1.26 MiB) to /root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 396k/396k [00:00<00:00, 34.4MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1029.87ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 319/319 [00:00<00:00, 1195.48ex/s]\n100%|| 319/319 [00:00<00:00, 1171.64ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 1203.22ex/s]\nmnli\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nDownloading and preparing dataset parquet/LysandreJik--glue-mnli-train to /root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8...\nDownloading data files:   0%|                             | 0/3 [00:00<?, ?it/s]\nDownloading data:   0%|                             | 0.00/51.3M [00:00<?, ?B/s]\u001b[A\nDownloading data:   0%|                     | 34.8k/51.3M [00:00<02:36, 327kB/s]\u001b[A\nDownloading data:   0%|                     | 80.9k/51.3M [00:00<02:10, 391kB/s]\u001b[A\nDownloading data:   0%|                      | 198k/51.3M [00:00<01:10, 720kB/s]\u001b[A\nDownloading data:   1%|                    | 427k/51.3M [00:00<00:38, 1.31MB/s]\u001b[A\nDownloading data:   2%|                    | 867k/51.3M [00:00<00:21, 2.39MB/s]\u001b[A\nDownloading data:   4%|                   | 1.83M/51.3M [00:00<00:10, 4.82MB/s]\u001b[A\nDownloading data:   8%|                  | 3.88M/51.3M [00:00<00:04, 9.86MB/s]\u001b[A\nDownloading data:  16%|                | 8.08M/51.3M [00:00<00:02, 20.0MB/s]\u001b[A\nDownloading data:  24%|               | 12.3M/51.3M [00:00<00:01, 26.8MB/s]\u001b[A\nDownloading data:  32%|             | 16.5M/51.3M [00:01<00:01, 31.5MB/s]\u001b[A\nDownloading data:  40%|            | 20.7M/51.3M [00:01<00:00, 34.8MB/s]\u001b[A\nDownloading data:  49%|          | 24.9M/51.3M [00:01<00:00, 37.0MB/s]\u001b[A\nDownloading data:  57%|        | 29.1M/51.3M [00:01<00:00, 38.6MB/s]\u001b[A\nDownloading data:  65%|       | 33.4M/51.3M [00:01<00:00, 39.7MB/s]\u001b[A\nDownloading data:  73%|     | 37.6M/51.3M [00:01<00:00, 40.6MB/s]\u001b[A\nDownloading data:  82%|   | 41.9M/51.3M [00:01<00:00, 41.2MB/s]\u001b[A\nDownloading data:  90%|  | 46.2M/51.3M [00:01<00:00, 41.7MB/s]\u001b[A\nDownloading data: 100%|| 51.3M/51.3M [00:01<00:00, 27.9MB/s]\u001b[A\nDownloading data files:  33%|              | 1/3 [00:04<00:08,  4.35s/it]\nDownloading data:   0%|                              | 0.00/760k [00:00<?, ?B/s]\u001b[A\nDownloading data:   6%|                    | 49.2k/760k [00:00<00:01, 466kB/s]\u001b[A\nDownloading data:  13%|                   | 96.3k/760k [00:00<00:01, 455kB/s]\u001b[A\nDownloading data:  26%|                 | 199k/760k [00:00<00:00, 679kB/s]\u001b[A\nDownloading data: 100%|| 760k/760k [00:00<00:00, 1.52MB/s]\u001b[A\n\nDownloading data:   0%|                              | 0.00/793k [00:00<?, ?B/s]\u001b[A\nDownloading data:   6%|                    | 49.2k/793k [00:00<00:02, 276kB/s]\u001b[A\nDownloading data:  18%|                   | 139k/793k [00:00<00:01, 537kB/s]\u001b[A\nDownloading data:  38%|              | 303k/793k [00:00<00:00, 945kB/s]\u001b[A\nDownloading data: 100%|| 793k/793k [00:00<00:00, 1.52MB/s]\u001b[A\nDownloading data files:  67%|       | 2/3 [00:09<00:04,  4.88s/it]\nDownloading data:   0%|                              | 0.00/758k [00:00<?, ?B/s]\u001b[A\nDownloading data:   4%|                     | 33.8k/758k [00:00<00:02, 320kB/s]\u001b[A\nDownloading data:  11%|                   | 80.9k/758k [00:00<00:01, 398kB/s]\u001b[A\nDownloading data:  27%|                | 206k/758k [00:00<00:00, 745kB/s]\u001b[A\nDownloading data: 100%|| 758k/758k [00:00<00:00, 1.54MB/s]\u001b[A\n\nDownloading data:   0%|                              | 0.00/793k [00:00<?, ?B/s]\u001b[A\nDownloading data:   6%|                    | 49.2k/793k [00:00<00:01, 446kB/s]\u001b[A\nDownloading data:  12%|                   | 98.3k/793k [00:00<00:01, 456kB/s]\u001b[A\nDownloading data:  28%|                | 221k/793k [00:00<00:00, 782kB/s]\u001b[A\nDownloading data: 100%|| 793k/793k [00:00<00:00, 1.62MB/s]\u001b[A\nDownloading data files: 100%|| 3/3 [00:14<00:00,  4.96s/it]\nExtracting data files: 100%|| 3/3 [00:00<00:00, 921.62it/s]\nDataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8. Subsequent calls will reuse this data.\n100%|| 3000/3000 [00:02<00:00, 1036.18ex/s]\nk =  1000   k-val =  500\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nReusing dataset parquet (/root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n100%|| 750/750 [00:00<00:00, 1230.93ex/s]\n100%|| 750/750 [00:00<00:00, 1190.69ex/s]\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nReusing dataset parquet (/root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n100%|| 90/90 [00:00<00:00, 1196.28ex/s]\ncb\nDownloading and preparing dataset super_glue/cb (download: 73.71 KiB, generated: 198.02 KiB, post-processed: Unknown size, total: 271.73 KiB) to /root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 75.5k/75.5k [00:00<00:00, 14.4MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 250/250 [00:00<00:00, 906.14ex/s]\nk =  -1   k-val =  -1\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 28/28 [00:00<00:00, 774.57ex/s]\n100%|| 28/28 [00:00<00:00, 875.43ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 6/6 [00:00<00:00, 832.78ex/s]\ncopa\nDownloading and preparing dataset super_glue/copa (download: 42.96 KiB, generated: 119.62 KiB, post-processed: Unknown size, total: 162.57 KiB) to /root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 44.0k/44.0k [00:00<00:00, 11.7MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 400/400 [00:00<00:00, 1311.31ex/s]\nk =  -1   k-val =  -1\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 50/50 [00:00<00:00, 1254.11ex/s]\n100%|| 50/50 [00:00<00:00, 1318.78ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 8/8 [00:00<00:00, 1204.14ex/s]\nqqp\nDownloading builder script: 28.8kB [00:00, 17.2MB/s]                            \nDownloading metadata: 28.7kB [00:00, 18.5MB/s]                                  \nDownloading and preparing dataset glue/qqp (download: 39.76 MiB, generated: 106.55 MiB, post-processed: Unknown size, total: 146.32 MiB) to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 41.7M/41.7M [00:00<00:00, 55.8MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1112.94ex/s]\nk =  1000   k-val =  500\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 500/500 [00:00<00:00, 1331.17ex/s]\n100%|| 500/500 [00:00<00:00, 1327.14ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 40/40 [00:00<00:00, 1287.83ex/s]\nrte\nDownloading and preparing dataset glue/rte (download: 680.81 KiB, generated: 1.83 MiB, post-processed: Unknown size, total: 2.49 MiB) to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 697k/697k [00:00<00:00, 43.4MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2490/2490 [00:02<00:00, 832.87ex/s]\nk =  -1   k-val =  -1\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 138/138 [00:00<00:00, 922.25ex/s]\n100%|| 139/139 [00:00<00:00, 908.28ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 48/48 [00:00<00:00, 906.88ex/s]\nimdb\nDownloading builder script: 4.31kB [00:00, 2.98MB/s]                            \nDownloading metadata: 2.17kB [00:00, 1.86MB/s]                                  \nDownloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\nDownloading data: 100%|| 84.1M/84.1M [00:13<00:00, 6.03MB/s]\nDataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:05<00:00, 365.02ex/s]\nk =  1000   k-val =  500\nReusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n100%|| 500/500 [00:01<00:00, 383.02ex/s]\n100%|| 500/500 [00:01<00:00, 356.91ex/s]\nReusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n100%|| 40/40 [00:00<00:00, 364.50ex/s]\nsst2\nDownloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 7.44M/7.44M [00:00<00:00, 55.0MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1401.27ex/s]\nk =  1000   k-val =  400\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 400/400 [00:00<00:00, 1521.14ex/s]\n100%|| 400/400 [00:00<00:00, 1548.60ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 40/40 [00:00<00:00, 1634.55ex/s]\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 4.09MB/s]                            \nDownloading metadata: 2.47kB [00:00, 2.14MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:06<00:00, 9.93MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n100%|| 14000/14000 [00:16<00:00, 862.43ex/s]\nk =  1000   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 3500/3500 [00:04<00:00, 867.84ex/s]\n100%|| 3500/3500 [00:04<00:00, 815.83ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 1960/1960 [00:02<00:00, 902.59ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 3.39MB/s]                            \nDownloading metadata: 2.65kB [00:00, 2.34MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 64.1MB/s]                                      \nDownloading data: 1.86MB [00:00, 50.9MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|| 4000/4000 [00:04<00:00, 996.37ex/s]\nk =  1000   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 1000/1000 [00:01<00:00, 955.03ex/s]\n100%|| 1000/1000 [00:01<00:00, 963.59ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 160/160 [00:00<00:00, 1149.16ex/s]\nyelp_review_full\nDownloading builder script: 4.41kB [00:00, 3.19MB/s]                            \nDownloading metadata: 2.04kB [00:00, 1.69MB/s]                                  \nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\nDownloading data: 100%|| 196M/196M [00:25<00:00, 7.75MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\n100%|| 5000/5000 [00:08<00:00, 555.89ex/s]\nk =  1000   k-val =  500\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 1250/1250 [00:02<00:00, 553.82ex/s]\n100%|| 1250/1250 [00:02<00:00, 553.93ex/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 250/250 [00:00<00:00, 610.95ex/s]\namazon\n  0%|                                                  | 0/5000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 5000/5000 [00:06<00:00, 746.24ex/s]\nk =  1000   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 1250/1250 [00:01<00:00, 774.65ex/s]\n100%|| 1250/1250 [00:01<00:00, 767.22ex/s]\n  0%|                                                   | 0/250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 250/250 [00:00<00:00, 845.83ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.90MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.47MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:22<00:00, 14.3MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 10000/10000 [00:19<00:00, 503.01ex/s]\nk =  1000   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 2500/2500 [00:04<00:00, 512.76ex/s]\n100%|| 2500/2500 [00:04<00:00, 500.64ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 1000/1000 [00:01<00:00, 524.85ex/s]\ntask =  yelp_review_full\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n7\n100%|| 1250/1250 [14:27<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:07<00:00,  2.45it/s]\n7 yelp_review_full -> 0.5976\n8\n100%|| 1250/1250 [14:26<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:07<00:00,  2.45it/s]\n8 yelp_review_full -> 0.604\n9\n100%|| 1250/1250 [14:26<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:07<00:00,  2.45it/s]\n9 yelp_review_full -> 0.6008\nUpdated progressive prompts  torch.Size([130, 1024])\nyelp_review_full [0.4864, 0.5208, 0.5312, 0.54, 0.5688, 0.544, 0.5688, 0.5976, 0.604, 0.6008]\nCalculating test acc ...\n/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py:887: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  curr_prompt = torch.tensor(self.previous_prompts, requires_grad=False).to(self.device)\n100%|| 313/313 [02:07<00:00,  2.46it/s]\ntask =  amazon\nprogressive prompts\n0\n100%|| 1250/1250 [14:26<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:07<00:00,  2.45it/s]\n0 amazon -> 0.4344\n1\n100%|| 1250/1250 [14:26<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:07<00:00,  2.46it/s]\n1 amazon -> 0.4408\n2\n100%|| 1250/1250 [14:26<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:07<00:00,  2.45it/s]\n2 amazon -> 0.4088\n3\n100%|| 1250/1250 [14:26<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:07<00:00,  2.46it/s]\n3 amazon -> 0.4168\n4\n100%|| 1250/1250 [14:26<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:07<00:00,  2.45it/s]\n4 amazon -> 0.4144\n5\n100%|| 1250/1250 [14:27<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:08<00:00,  2.44it/s]\n5 amazon -> 0.484\n6\n100%|| 1250/1250 [14:27<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:08<00:00,  2.45it/s]\n6 amazon -> 0.5184\n7\n100%|| 1250/1250 [14:27<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:07<00:00,  2.45it/s]\n7 amazon -> 0.528\n8\n100%|| 1250/1250 [14:27<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:08<00:00,  2.44it/s]\n8 amazon -> 0.5536\n9\n100%|| 1250/1250 [14:26<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 313/313 [02:07<00:00,  2.46it/s]\n9 amazon -> 0.5424\nUpdated progressive prompts  torch.Size([140, 1024])\namazon [0.4344, 0.4408, 0.4088, 0.4168, 0.4144, 0.484, 0.5184, 0.528, 0.5536, 0.5424]\nCalculating test acc ...\n100%|| 313/313 [02:07<00:00,  2.46it/s]\ntask =  yahoo_answers_topics\nprogressive prompts\n0\n100%|| 2500/2500 [28:58<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 625/625 [05:07<00:00,  2.04it/s]\n0 yahoo_answers_topics -> 0.6288\n1\n100%|| 2500/2500 [28:57<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 625/625 [05:07<00:00,  2.03it/s]\n1 yahoo_answers_topics -> 0.6288\n2\n100%|| 2500/2500 [28:58<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 625/625 [05:06<00:00,  2.04it/s]\n2 yahoo_answers_topics -> 0.668\n3\n100%|| 2500/2500 [28:57<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 625/625 [05:07<00:00,  2.04it/s]\n3 yahoo_answers_topics -> 0.6876\n4\n100%|| 2500/2500 [28:57<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 625/625 [05:07<00:00,  2.03it/s]\n4 yahoo_answers_topics -> 0.726\n5\n100%|| 2500/2500 [28:57<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 625/625 [05:07<00:00,  2.03it/s]\n5 yahoo_answers_topics -> 0.7216\n6\n100%|| 2500/2500 [28:57<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 625/625 [05:07<00:00,  2.03it/s]\n6 yahoo_answers_topics -> 0.7236\n7\n100%|| 2500/2500 [28:57<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 625/625 [05:08<00:00,  2.02it/s]\n7 yahoo_answers_topics -> 0.7392\n8\n  4%|                                       | 92/2500 [01:03<27:54,  1.44it/s]^C\n  4%|                                       | 92/2500 [01:03<27:54,  1.44it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 240, in <module>\n    main(parser.parse_args())\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/train_t5_cl.py\", line 51, in main\n    results_dict = continual_learner.train_continual(continual_learner.task_list,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 873, in train_continual\n    val_acc = self.train_one_task(num, task, epochs,\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in train_one_task\n    batch = {k:batch[k].to('cuda') for k in batch}\n  File \"/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py\", line 772, in <dictcomp>\n    batch = {k:batch[k].to('cuda') for k in batch}\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!source activate nlp && python train_t5_cl.py --task_list multirc boolq wic mnli cb copa qqp rte imdb sst2 dbpedia_14 ag_news yelp_review_full amazon yahoo_answers_topics \\\n--lr 0.3 --num_epochs 10 --freeze_weights 1 --prefix_len 10 --batch_size 4 \\\n--model_name t5-large --early_stopping 1 --select_k_per_class 1000 \\\n--save_name large_1000_T5_order_2 --save_dir results --continue_train 1","metadata":{"execution":{"iopub.status.busy":"2023-11-28T07:37:15.518930Z","iopub.execute_input":"2023-11-28T07:37:15.519532Z","iopub.status.idle":"2023-11-28T09:02:41.952128Z","shell.execute_reply.started":"2023-11-28T07:37:15.519498Z","shell.execute_reply":"2023-11-28T09:02:41.951102Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading: 100%|| 1.18k/1.18k [00:00<00:00, 937kB/s]\nDownloading: 100%|| 2.75G/2.75G [00:51<00:00, 57.2MB/s]\nDownloading: 100%|| 773k/773k [00:00<00:00, 15.5MB/s]\nFreezing weights\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nmultirc\nDownloading builder script: 29.9kB [00:00, 13.3MB/s]                            \nDownloading metadata: 38.2kB [00:00, 22.1MB/s]                                  \nDownloading and preparing dataset super_glue/multirc (download: 1.06 MiB, generated: 65.77 MiB, post-processed: Unknown size, total: 66.84 MiB) to /root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 1.12M/1.12M [00:00<00:00, 17.6MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\nParameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7efb31adadc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n  0%|                                                  | 0/2000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 2000/2000 [00:06<00:00, 302.37ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 500/500 [00:01<00:00, 319.31ex/s]\n100%|| 500/500 [00:01<00:00, 333.57ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 323.81ex/s]\nboolq\nDownloading and preparing dataset super_glue/boolq (download: 3.93 MiB, generated: 9.92 MiB, post-processed: Unknown size, total: 13.85 MiB) to /root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 4.12M/4.12M [00:00<00:00, 40.2MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:03<00:00, 590.98ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 500/500 [00:00<00:00, 653.96ex/s]\n100%|| 500/500 [00:00<00:00, 587.40ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 656.57ex/s]\nwic\nDownloading and preparing dataset super_glue/wic (download: 386.93 KiB, generated: 906.64 KiB, post-processed: Unknown size, total: 1.26 MiB) to /root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 396k/396k [00:00<00:00, 11.9MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1037.61ex/s]\nk =  1000   k-val =  500\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 319/319 [00:00<00:00, 1224.85ex/s]\n100%|| 319/319 [00:00<00:00, 1218.93ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 40/40 [00:00<00:00, 1185.36ex/s]\nmnli\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nDownloading and preparing dataset parquet/LysandreJik--glue-mnli-train to /root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8...\nDownloading data files:   0%|                             | 0/3 [00:00<?, ?it/s]\nDownloading data:   0%|                             | 0.00/51.3M [00:00<?, ?B/s]\u001b[A\nDownloading data:   8%|                  | 4.31M/51.3M [00:00<00:01, 43.1MB/s]\u001b[A\nDownloading data:  20%|                | 10.4M/51.3M [00:00<00:00, 53.4MB/s]\u001b[A\nDownloading data:  32%|             | 16.3M/51.3M [00:00<00:00, 56.2MB/s]\u001b[A\nDownloading data:  44%|           | 22.3M/51.3M [00:00<00:00, 57.7MB/s]\u001b[A\nDownloading data:  55%|         | 28.3M/51.3M [00:00<00:00, 58.6MB/s]\u001b[A\nDownloading data:  67%|      | 34.4M/51.3M [00:00<00:00, 59.2MB/s]\u001b[A\nDownloading data:  79%|    | 40.3M/51.3M [00:00<00:00, 59.1MB/s]\u001b[A\nDownloading data: 100%|| 51.3M/51.3M [00:00<00:00, 56.5MB/s]\u001b[A\nDownloading data files:  33%|              | 1/3 [00:04<00:08,  4.39s/it]\nDownloading data: 100%|| 760k/760k [00:00<00:00, 15.1MB/s]\u001b[A\n\nDownloading data: 100%|| 793k/793k [00:00<00:00, 18.1MB/s]\u001b[A\nDownloading data files:  67%|       | 2/3 [00:09<00:04,  4.83s/it]\nDownloading data: 100%|| 758k/758k [00:00<00:00, 16.6MB/s]\u001b[A\n\nDownloading data: 100%|| 793k/793k [00:00<00:00, 16.0MB/s]\u001b[A\nDownloading data files: 100%|| 3/3 [00:18<00:00,  6.29s/it]\nExtracting data files: 100%|| 3/3 [00:00<00:00, 966.28it/s]\nDataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8. Subsequent calls will reuse this data.\n100%|| 3000/3000 [00:03<00:00, 993.09ex/s]\nk =  1000   k-val =  500\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nReusing dataset parquet (/root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n100%|| 750/750 [00:00<00:00, 1219.97ex/s]\n100%|| 750/750 [00:00<00:00, 1212.65ex/s]\nUsing custom data configuration LysandreJik--glue-mnli-train-aef4727e80ba5593\nReusing dataset parquet (/root/.cache/huggingface/datasets/LysandreJik___parquet/LysandreJik--glue-mnli-train-aef4727e80ba5593/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n100%|| 90/90 [00:00<00:00, 1205.49ex/s]\ncb\nDownloading and preparing dataset super_glue/cb (download: 73.71 KiB, generated: 198.02 KiB, post-processed: Unknown size, total: 271.73 KiB) to /root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 75.5k/75.5k [00:00<00:00, 4.28MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 250/250 [00:00<00:00, 915.82ex/s]\nk =  -1   k-val =  -1\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 28/28 [00:00<00:00, 807.76ex/s]\n100%|| 28/28 [00:00<00:00, 925.98ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 6/6 [00:00<00:00, 844.60ex/s]\ncopa\nDownloading and preparing dataset super_glue/copa (download: 42.96 KiB, generated: 119.62 KiB, post-processed: Unknown size, total: 162.57 KiB) to /root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7...\nDownloading data: 100%|| 44.0k/44.0k [00:00<00:00, 4.46MB/s]\nDataset super_glue downloaded and prepared to /root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7. Subsequent calls will reuse this data.\n100%|| 400/400 [00:00<00:00, 1335.30ex/s]\nk =  -1   k-val =  -1\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 50/50 [00:00<00:00, 1309.91ex/s]\n100%|| 50/50 [00:00<00:00, 1335.77ex/s]\nReusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/copa/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n100%|| 8/8 [00:00<00:00, 1258.65ex/s]\nqqp\nDownloading builder script: 28.8kB [00:00, 15.0MB/s]                            \nDownloading metadata: 28.7kB [00:00, 16.2MB/s]                                  \nDownloading and preparing dataset glue/qqp (download: 39.76 MiB, generated: 106.55 MiB, post-processed: Unknown size, total: 146.32 MiB) to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 41.7M/41.7M [00:00<00:00, 55.2MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1087.75ex/s]\nk =  1000   k-val =  500\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 500/500 [00:00<00:00, 1372.06ex/s]\n100%|| 500/500 [00:00<00:00, 1362.73ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 40/40 [00:00<00:00, 1323.01ex/s]\nrte\nDownloading and preparing dataset glue/rte (download: 680.81 KiB, generated: 1.83 MiB, post-processed: Unknown size, total: 2.49 MiB) to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 697k/697k [00:00<00:00, 15.4MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2490/2490 [00:02<00:00, 837.80ex/s]\nk =  -1   k-val =  -1\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 138/138 [00:00<00:00, 985.37ex/s]\n100%|| 139/139 [00:00<00:00, 951.17ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 48/48 [00:00<00:00, 922.69ex/s]\nimdb\nDownloading builder script: 4.31kB [00:00, 2.74MB/s]                            \nDownloading metadata: 2.17kB [00:00, 1.55MB/s]                                  \nDownloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\nDownloading data: 100%|| 84.1M/84.1M [00:02<00:00, 34.2MB/s]\nDataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:05<00:00, 355.73ex/s]\nk =  1000   k-val =  500\nReusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n100%|| 500/500 [00:01<00:00, 378.02ex/s]\n100%|| 500/500 [00:01<00:00, 368.45ex/s]\nReusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n100%|| 40/40 [00:00<00:00, 392.31ex/s]\nsst2\nDownloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\nDownloading data: 100%|| 7.44M/7.44M [00:00<00:00, 48.2MB/s]\nDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n100%|| 2000/2000 [00:01<00:00, 1295.43ex/s]\nk =  1000   k-val =  400\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 400/400 [00:00<00:00, 1498.11ex/s]\n100%|| 400/400 [00:00<00:00, 1504.61ex/s]\nReusing dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n100%|| 40/40 [00:00<00:00, 1759.72ex/s]\ndbpedia_14\nDownloading builder script: 5.22kB [00:00, 2.91MB/s]                            \nDownloading metadata: 2.47kB [00:00, 1.67MB/s]                                  \nDownloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\nDownloading data: 100%|| 68.3M/68.3M [00:01<00:00, 34.4MB/s]\nDataset dbpedia_14 downloaded and prepared to /root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n100%|| 14000/14000 [00:16<00:00, 839.89ex/s]\nk =  1000   k-val =  500\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 3500/3500 [00:04<00:00, 855.78ex/s]\n100%|| 3500/3500 [00:04<00:00, 847.75ex/s]\nReusing dataset dbpedia_14 (/root/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n100%|| 1960/1960 [00:02<00:00, 899.86ex/s]\nag_news\nDownloading builder script: 4.06kB [00:00, 2.79MB/s]                            \nDownloading metadata: 2.65kB [00:00, 1.84MB/s]                                  \nUsing custom data configuration default\nDownloading and preparing dataset ag_news/default (download: 29.88 MiB, generated: 30.23 MiB, post-processed: Unknown size, total: 60.10 MiB) to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\nDownloading data: 29.5MB [00:00, 62.1MB/s]                                      \nDownloading data: 1.86MB [00:00, 40.1MB/s]                                      \nDataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n100%|| 4000/4000 [00:04<00:00, 981.75ex/s]\nk =  1000   k-val =  500\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 1000/1000 [00:00<00:00, 1008.73ex/s]\n100%|| 1000/1000 [00:00<00:00, 1008.57ex/s]\nUsing custom data configuration default\nReusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n100%|| 160/160 [00:00<00:00, 1152.21ex/s]\nyelp_review_full\nDownloading builder script: 4.41kB [00:00, 3.29MB/s]                            \nDownloading metadata: 2.04kB [00:00, 1.43MB/s]                                  \nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\nDownloading data: 100%|| 196M/196M [00:05<00:00, 34.6MB/s]\nDataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\n100%|| 5000/5000 [00:09<00:00, 554.45ex/s]\nk =  1000   k-val =  500\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 1250/1250 [00:02<00:00, 551.63ex/s]\n100%|| 1250/1250 [00:02<00:00, 561.97ex/s]\nReusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n100%|| 250/250 [00:00<00:00, 611.47ex/s]\namazon\n  0%|                                                  | 0/5000 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 5000/5000 [00:06<00:00, 727.62ex/s]\nk =  1000   k-val =  500\n  0%|                                                  | 0/1250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 1250/1250 [00:01<00:00, 763.49ex/s]\n100%|| 1250/1250 [00:01<00:00, 736.96ex/s]\n  0%|                                                   | 0/250 [00:00<?, ?ex/s]/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|| 250/250 [00:00<00:00, 825.65ex/s]\nyahoo_answers_topics\nDownloading builder script: 3.60kB [00:00, 2.64MB/s]                            \nDownloading metadata: 1.88kB [00:00, 1.34MB/s]                                  \nDownloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics (download: 304.68 MiB, generated: 756.38 MiB, post-processed: Unknown size, total: 1.04 GiB) to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\nDownloading data: 100%|| 319M/319M [00:09<00:00, 32.3MB/s]\nDataset yahoo_answers_topics downloaded and prepared to /root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n100%|| 10000/10000 [00:19<00:00, 512.10ex/s]\nk =  1000   k-val =  500\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 2500/2500 [00:04<00:00, 508.81ex/s]\n100%|| 2500/2500 [00:04<00:00, 513.67ex/s]\nReusing dataset yahoo_answers_topics (/root/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439)\n100%|| 1000/1000 [00:01<00:00, 528.05ex/s]\ntask =  yahoo_answers_topics\nprogressive prompts\n/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n8\n100%|| 2500/2500 [28:59<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 625/625 [05:08<00:00,  2.03it/s]\n8 yahoo_answers_topics -> 0.7364\n9\n100%|| 2500/2500 [28:58<00:00,  1.44it/s]\ntorch.Size([10, 1024])\n100%|| 625/625 [05:07<00:00,  2.03it/s]\n9 yahoo_answers_topics -> 0.736\nUpdated progressive prompts  torch.Size([150, 1024])\nyahoo_answers_topics [0.6288, 0.6288, 0.668, 0.6876, 0.726, 0.7216, 0.7236, 0.7392, 0.7364, 0.736]\nCalculating test acc ...\n/kaggle/working/ProgressivePrompts/T5_codebase/t5_continual.py:887: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  curr_prompt = torch.tensor(self.previous_prompts, requires_grad=False).to(self.device)\n100%|| 625/625 [05:08<00:00,  2.03it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"%mv /kaggle/working/ProgressivePrompts/T5_codebase/save_checkpoint/current_checkpoint.pt /kaggle/working/\n%cd /kaggle/working\nfrom IPython.display import FileLink\nFileLink(r'current_checkpoint.pt')","metadata":{"execution":{"iopub.status.busy":"2023-11-28T03:13:11.394604Z","iopub.execute_input":"2023-11-28T03:13:11.395607Z","iopub.status.idle":"2023-11-28T03:13:12.341653Z","shell.execute_reply.started":"2023-11-28T03:13:11.395561Z","shell.execute_reply":"2023-11-28T03:13:12.340652Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/current_checkpoint.pt","text/html":"<a href='current_checkpoint.pt' target='_blank'>current_checkpoint.pt</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nres = np.load('/kaggle/working/ProgressivePrompts/T5_codebase/results/large_1000_T5_order_2/results_dict.npy', allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T09:32:09.004862Z","iopub.execute_input":"2023-11-28T09:32:09.005315Z","iopub.status.idle":"2023-11-28T09:32:09.011815Z","shell.execute_reply.started":"2023-11-28T09:32:09.005269Z","shell.execute_reply":"2023-11-28T09:32:09.010959Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"res","metadata":{"execution":{"iopub.status.busy":"2023-11-28T09:32:13.417975Z","iopub.execute_input":"2023-11-28T09:32:13.418823Z","iopub.status.idle":"2023-11-28T09:32:13.426043Z","shell.execute_reply.started":"2023-11-28T09:32:13.418789Z","shell.execute_reply":"2023-11-28T09:32:13.425151Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array({'test': {'multirc': 0.83, 'boolq': 0.826, 'wic': 0.6677115987460815, 'mnli': 0.868, 'cb': (0.8928571428571429, 0.7843578210894554), 'copa': 0.48, 'qqp': 0.894, 'rte': 0.841726618705036, 'imdb': 0.944, 'sst2': 0.95, 'dbpedia_14': 0.9882857142857143, 'ag_news': 0.887, 'yelp_review_full': 0.6152, 'amazon': 0.5736, 'yahoo_answers_topics': 0.7272}, 'multirc': [0.806, 0.838, 0.83, 0.828, 0.834, 0.838, 0.83, 0.836, 0.842, 0.844], 'boolq': [0.838, 0.844, 0.842, 0.836, 0.834, 0.838, 0.838, 0.842, 0.842, 0.838], 'wic': [0.6394984326018809, 0.670846394984326, 0.664576802507837, 0.6269592476489029, 0.658307210031348, 0.6677115987460815, 0.6489028213166145, 0.6520376175548589, 0.670846394984326, 0.6363636363636364], 'mnli': [0.8613333333333333, 0.8626666666666667, 0.8626666666666667, 0.8826666666666667, 0.876, 0.868, 0.8706666666666667, 0.876, 0.8746666666666667, 0.8746666666666667], 'cb': [0.9113445378151261, 0.9113445378151261, 0.8671299460773145, 0.8347727980080921, 0.8347727980080921, 0.8854497354497355, 0.8671299460773145, 0.9437590187590188, 1.0, 1.0], 'copa': [0.62, 0.62, 0.62, 0.62, 0.62, 0.62, 0.62, 0.62, 0.62, 0.62], 'qqp': [0.894, 0.83, 0.834, 0.858, 0.872, 0.866, 0.876, 0.864, 0.876, 0.884], 'rte': [0.5144927536231884, 0.7608695652173914, 0.7898550724637681, 0.8115942028985508, 0.8333333333333334, 0.8405797101449275, 0.8405797101449275, 0.8478260869565217, 0.8478260869565217, 0.8478260869565217], 'imdb': [0.738, 0.938, 0.932, 0.938, 0.952, 0.948, 0.956, 0.956, 0.952, 0.952], 'sst2': [0.935, 0.94, 0.94, 0.9475, 0.9425, 0.95, 0.95, 0.95, 0.955, 0.945], 'dbpedia_14': [0.8014285714285714, 0.914, 0.9708571428571429, 0.9797142857142858, 0.9802857142857143, 0.9797142857142858, 0.982, 0.9837142857142858, 0.984, 0.9842857142857143], 'ag_news': [0.811, 0.867, 0.883, 0.886, 0.887, 0.896, 0.891, 0.893, 0.893, 0.897], 'yelp_review_full': [0.4864, 0.5208, 0.5312, 0.54, 0.5688, 0.544, 0.5688, 0.5976, 0.604, 0.6008], 'amazon': [0.4344, 0.4408, 0.4088, 0.4168, 0.4144, 0.484, 0.5184, 0.528, 0.5536, 0.5424], 'yahoo_answers_topics': [0.6288, 0.6288, 0.668, 0.6876, 0.726, 0.7216, 0.7236, 0.7392, 0.7364, 0.736]},\n      dtype=object)"},"metadata":{}}]}]}